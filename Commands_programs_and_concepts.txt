-------
Android
-------

# Starting out with an Android project [from a DevOps point of view]
# Source: Engineer Man - https://www.youtube.com/watch?v=R3eAMMBh2ng

1) Download Android Studio from the developer.android.com site and follow the instructions to install on your computer or server

2) Open Android Studio and in "Select A Project Template", choose whatever you like [e.g. Empty Activity]

3) In "Configure Your Project", youll need to configure the following:

	- Name: My Application
	- Package Name: com.hq.myapplication       # can leave as default
	- Save location: <whereever_you_like>
	- Language: Java or Kotlin
	- Minimum SDK: recommendation is Android 5.0 [Lollipop] or 6.0 [Marshmallow]       # going too low might make for problems and extra work down the road

- Side note: - designing layouts in Android [you will see the options when in Android studio]
	- purely with code
	- split between code and visual preview
	- purely drag and drop 


4) Explantations on different files and folders created:

- AndroidManifest.xml [under app/manifests]
	- metadata about the application
		- in the application block, see things like the text label, icons, activity block [screen that shows up on Android or a screen that might be switched to]


- java folder
	- contains all the java code for your application
	- other folders: one for main code, one for tests
	- example code: MainActivity.java under com.hq.myapplication


- java [generated] folder
	- never need to directly be concerned with it, though code gets generated here because of content or code you create elsewhere [such as a content xml in the layout folder]


- res folder [resources]
	- contains everything except java code itself

	- sub-folders
		- drawable folder
			- contains vector assets and images
		- layout folder
			- for layouts [e.g. menus, list items, etc.]
			- eg. activity_main.xml - eg. content xml file referenced by the MainActivity.java file


- mipmap folder
	- contains several variations of the same image [for different phone sizes]
	- in code, reference by the same name, and the phone figures out which image to use


- values folder
	- stores various constant values of different things such as colours and strings [e.g. color name="purple_200" matched to a hex value]
	- file: strings.xml - used in case app is gonna be in different languages


- themes folder
	- application wide theme [eg. light and dark themes to apply, phone can toggle depending on user choice]


- Gradle scripts
	- for building the application
		- build.gradle - has the build instructions


5) To test run your app, plug your phone into your computer and click the Play button in Android Studio



-------
Ansible
-------

#### How to automate your Mac setup ####

# Source: https://github.com/geerlingguy/mac-dev-playbook




---
AWS
---

#### Top 50 List of Services ####
# Source: Fireship - https://www.youtube.com/watch?v=JIbIYCM48to&pp=sAQA

# Robots and Large Scale
- Robomaker - simulate and test your robots at scale (e.g. robot vacuums)
- IOT core - collect data from those robots, update their software, manage them remotely
- Ground Station - control satellite communications, process data and scale operations from satellites orbiting Earth
- Bracket - software used to interact with a quantum computer (i.e. to learn about the future of computing)

# Compute
- EC2 (Elastic Compute Cloud) - create a virtual computer in the cloud (OS, memory, computing power), for rent
- elastic load balancing - distribute traffic to multiple instances automatically
- Cloud Watch - collect logs and metrics from each individual instance
- Auto Scaling - create or scale down instances based on the metrics collected from Cloud Watch
- Elastic Beanstalk (PaaS) - interface for easily deploying code to AWS (including scaling, load balancing, etc.)
- Lightsail - even easier deployment tool for AWS, don[t have to worry at all about the underlying infrastructure
- Lambda (FaaS) - serverless, upload your code, decide when it should run (traffic, scaling, networking all work in background). Pay only when the app is used.
- Serverless Repo - if you dont like writing your own code, find pre-built functions to deploy with a click
- Outposts - Run AWS APIs on your own infrastructure (without tossing out your old servers)
- Snow - mini data centers that work without internet in hostile envs (e.g. the Arctic)

# Containers and Container Orchestration
- Container Registry - upload/store container images
- Elastic Container Service - run a container stored in the registry (Elastic Container Service (ECS) is the service for stopping, starting and allocating virtual machines for your containers
															and connect them to other products such as load balancers)
- EKS - Amazons Kubernetes service
- Fargate - make your containers behave like serverless functions (automatic resources)
- App Runner (2021) - deployment tool for AWS (choose your images to run, it takes care of running in AWS, including resource allocation)

# File Storage
- S3 (Simple Storage Service) - store any type of file or object
- Glacier - higher latency, lower cost storage (more for archiving, when access rate is low)
- Elastic Block Storage - fass, handle bigger throughput (more configuration required)
- Elastic File System - high performance, fully manager, much higher cost

# Database
- Simple DB - general purpose no SQL database
- Dynamo DB - document database easy to scale horizontally (cheap, scales easy, fast, but no joins and limited queries, not good at modelling relational data)
- Document DB - controversial: not mongoDB, but exactly like it to get around licensing
- Elastic Search - good for search
- Amazon RDS (Relational Database Service) - supports a variety of SQL flavours, fully manage backups, scale and caching
- Aurora - Amazons own proprietary version of SQL (compatible with Postgres/mySQL, better performance at a lower cost, easy to scale with new serverless option, only pay when in use)
- Neptune - graph database, high performance on highly connected data sets (e.g. social graph, recommendation engine)
- Elastic Cache - ultra fast database, fully managed version of Redis (in=memory database)
- Timestream - time series database
- Quantum Ledger - cryptographically signed transactions

# Analyitcs
- Redshift - data warehouse (shift away from Oracle)
- Lake Formation - tool for creating data lakes or repos that store a large amount of unstructured data (can be used in addition to DWs to query a larger variety of data sources)
- Kinesis - capture real time streams, view the captured data in a business intelligence tool
- Apache Spark (Elastic Map Reduce) - stream data from multiple platforms to a business intelligence tool for analysis
- MSK - AWS fully managed service version of Apache Kafka
- Glue - Auto ETL (Extract, Transform, Load), can connect to different kinds of databases

# Machine Learning
- Data Exchange - purchase/exchange data from third party sources for analysis
- Sagemaker - connect to data exchange, build machine learning models
- Rekognition - identify/classify images
- Lex - chatbot
- Deep Racer - actual miniature race car driven remotely with machine learning code

# Developer Essentials
- IAM - identity and access management, roles
- Cognito - enable to login with different auth methods
- Simple Notification Service (SNS)
- Simple Email Services (SES)
- Cloud Formation - templates based on infrastructure in yaml/json format (I assume like Terraform/Ansible)
- Amplify - provide SDKs to connect to infrastructure from front end apps

# Bonus
- AWS cost explorer - budgeting

# Another bonus 
- App Engine Standard - for hosting NodeJS applications, easily scalable


-------
backups
-------

Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=o-83E6levzM&list=WL&index=3

- fault tolerance - the prevention of data loss if a component fails
- disaster recovery - the process of rebuilding an organizations data after a disaster


Comparison             Data thats backed up           Restore Procedure

- full                 All data (longest)							Full backup only (fastest)

- incremental          Data thats been changed				Full and incremetals (in the correct order) (longest)
											 since last full or incr.
											 backup (fastest)

- differential				 Data thats been changed				Full and last differential (medium)
											 since the last full backup
											 (medium)



------
base64
------

# Encrypt a plaintext password

echo -n "password123" | base64 -i -     # Output: cFSzf3skacQrIrR=


------
cables
------


# Ethernet Cables
Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=_NX99ad2FUA

- unshielded twisted pair
	- most common type of ethernet cable (used more in homes)
	- 4 pairs of colour-coded wires twisted around each other to prevent electromagnetic interference (crosstalk)

- shielded twisted pair
	- same as UTP except has a foil shield that covers the wires (more protection against interference)
	- both use RJ-45 connector

- types of twisted pairs
	- straight (patch) cable
		- if both ends of a cable are using the same standard
		- allows signals to pass through from end-to-end
		- to connect computers to dissimilar devices together (e.g. from a PC to a modem)
		- most commonly used
	- crossover cable
		- if both ends of a cable are using 2 different standards
		- to connect to similar devices together

- wiring standards
	- 568A
		- order of wires: white-green, green, white-orange, blue, white-blue, orange, white-brown, brown
	- 568B (more commonly used)
		- order of wires: white-orange, orange, white-green, blue, white-blue, green, white-brown, brown
	

- categories
	- the difference being the max speed they can handle without interference/crosstalk
	- the numbers represent the tightness of the twist that are applied to the wires
		- CAT3 - 10 Mbps - obsolete
		- CAT5 - 100 Mbps - obsolete
		- CAT5e - 1 Gbps - Enhanced
		- CAT6 - 1 Gbps  - 10 Gbps (cable length under 100 meters)
		- CAT6a - 10 Gbps - Augmented
		- CAT7 - 10 Gbps - Added shielding to the wires (shielded twisted pair version of CAT6a)
		- CAT8 - 40 Gbps - ultimate copper cable, shielded (4 times faster than CA6a/CAT7)


------------
certificates
------------

# Download and import website certs into JAVA store (eg. google's certs for dl.google.com/android repo):

openssl s_client -connect google.com:443 -showcerts | openssl x509 -out certfile.txt
keytool -importcert -alias globalproxy -file certfile.txt -trustcacerts -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storetype JKS

OR 

keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts
keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security/cacerts

# check cacerts store

keytool -list -v -keystore /path/to/cacerts



=======
ciphers
=======

Cipher groups
-------------

Group 1: These cipher suites have perfect forward secrecy (ECDHE) and authenticated encryption (GCM):
	E.g.: ECDHE-RSA-AES128-GCM-SHA256, ECDHE-RSA-AES256-GCM-SHA384

Group 2:  Perfect forward secrecy, but do not have authenticated encryption.
	E.g.: ECDHE-RSA-AES128-SHA256, ECDHE-RSA-AES256-SHA384

Group 3: Authenticated encryption but no perfect forward secrecy:
	E.g. : AES128-GCM-SHA256, AES256-GCM-SHA384

Group 4: Standard algorithms but no perfect forward secrecy or authenticated encryption:
	E.g.: AES128-SHA256, AES256-SHA256
	
Group 5: Perfect forward secrecy, but use SHA-1: (Do Not Include if possible)
	E.g.: ECDHE-RSA-AES128-SHA, ECDHE-RSA-AES256-SHA

Group 6: Standard algorithms but use SHA-1: (Do Not Include if possible)
	E.g.: AES128-SHA, AES256-SHA, DES-CBC3-SHA

Group 7: RC4 gets its own special category: (Do Not Include. This is deviation)
	E.g.: RC4-SHA

Group 8: For the love of God, do not use these: (Do Not Include. This is deviation)
	E.g.: DES-CBC-SHA, RC4-MD5, IDEA-CBC-SHA



-------------------------------
CompTIA A+ Certifcation 220-801
-------------------------------

- see "CompTIA_Aplus_Certification_220-801_notes.txt"



-------
crontab
-------

# Sources:
- https://crontab.guru - translate any cron entry and tells you what its timing interval is
- https://www.youtube.com/watch?v=QEdHAwHfGPc - Engineer Mans cron video

# Of the 5 stars:
- 1st - minute (0 - 59)
- 2nd - hour (0 - 23)
- 3rd - day of the month (1 - 31)
- 4th - month (1 - 12)
- 5th - day of the week (0 - 6) (Sunday to Saturday; 7 is also Sunday on some systems)

# Format
* * * * * [user] [command]

# Quick examples

# every minute
* * * * * /path/to/script

# every minute as user tim
* * * * * tim /path/to/script

# every hour at the top of the hour
0 * * * * /path/to/script

# nightly at 11pm
0 23 * * * /path/to/script

# first of the month at 3pm, every month
0 15 1 * * /path/to/script

# every day at 8am, 10am, 12pm and 2pm
0 8,10,12,14 * * * /path/to/script

# every half hour
*/30 * * * * /path/to/script

# every tuesday at 8am
0 8 * * 2 /path/to/scropt

# first wednesday of each month
0 23 1-7 * 3 /path/to/script


# Edit crontab
crontab -e

# List crontab
crontab -l

# Run crontab (Debian/Ubuntu)
service cron start
ps -ef | grep cron



---------
curl/wget
---------

# Download a file (eg. Artifactory, using credentials)
curl -u <user>:<password> http://artifactory_url:8081/artifactory/path/to/package/something.rpm -o ./something.rpm

# Upload a file (eg. to Artifactory w/ creds)
curl -X PUT -u <user>:<password> -T something.rpm "http://artifactory_url:8081/artifactory/path/to/package/something.rpm"

# recursive download with wget
wget --no-parent -r http://WEBSITE.com/DIRECTORY

# set insecure
echo insecure >> $HOME/.curlrc


---------------
default gateway
---------------

Source: https://www.youtube.com/watch?v=pCcJFdYNamc

- definition: a device that forwards data from one network to another [usually a router]
	- lets devices from one network communicate with devices on another network
	- "default" means the designated device is the first option thats looked upon when data needs to exit the network
	- to connect out to the internet from a local network, it has to go through a gateway, usually the default
	- not required for internetwork communication

	- an IP address consists of two parts: network address and host address
	- subnet mask: reveals how many bits in the IP address are used for the network by masking the network portion of the address

		- eg.
			- IP address  - 192.168.0.2   -   11000000.10101000.00000000.00000010
			- Subnet mask - 255.255.255.0 -   11111111.11111111.11111111.00000000
																				network  network network     host

			  - for each of the 4 digits, the sections with all 1s indicate the network portion, and these "mask" the corresponding parts of the IP address of the network, telling it
			  	that it does NOT have to go through the default gateway to communicate with the device on that subnet
			  	- 192.168.0 is the subnet in this case
			  	- see 3:39 in source vid for examples of communication


----
deno
----

- What is it? A secure runtime for javascript and typescript

***to be continued***




----
DHCP
----

# Additional notes not covered in detail in CompTIA notes

- DHCP is a service that runs on a server, such as a Microsoft server or a Linux server
	- also a service that runs on routers


- scope - the range of IP addresses a DHCP server can hand out [from a start IP address to an end IP address]
				- customizable


- lease - DHCP assigns the IP address to a computer for a certain amount of time
				- to make sure DHCP server does not run out of IP addresses in its scope
				- renewal - lease expires if a computer does not automatically renew its lease [e.g. if a computer is not on the network, in which case lease expires]


- reservation - ensures that a specific computer or device [identified by its MAC address] will always be given the same IP address
						  - can configure this in DHCP settings on a DHCP server
						  - typically given to special devices such as network printers, servers, routers, etc.




---
DMZ
---

- Demilitarized Zone
	- a.k.a a perimeter network
	- used to improve the security of an organizations network by segregating devices such as computers and servers on opposite sides of a firewall

	- the servers are behind a companys firewall and are inside the companys private network
	 	- the company is letting in people from an untrusted network [the internet] and are given access behind the companys firewall

	- divides a network into two parts by taking devices from inside the firewall and putting them outside
		- a more secure network would use 2 [or more] firewalls for extra layers of protection

	- in the real world, a DMZ is an area where the military is forbidden
	- in the computing world, a DMZ is where firewall protection is forbidden


------
docker
------

# sharing namespaces: https://www.guidodiepen.nl/2017/04/accessing-container-contents-from-another-container/

# exposing docker port with bobrik/socat container:
docker run -d -v /var/run/docker.sock:/var/run/docker.sock -p 2376:2375 bobrik/socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock

# bobrik/socat container as a service:
docker service create --mode=global --name socat \
--publish 2376:2375 \
--mount "type=bind,source=/var/run/docker.sock,destination=/var/run/docker.sock" \
--entrypoint "socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock" \
bobrik/socat

# docker login troubleshooting
	- docker login not working (user interaction not allowed msg)
   		- remove credsStore from config.json
   		- if above doesn't work, uncheck 'Securely store Docker logins in macOS keychain'
   		- if above doesn't work, rm /usr/local/bin/docker-credential-osxkeychain

# checking container logs

docker run -it -v /var/lib/docker:/var/lib/docker <image_ID> bash - check /var/lib/docker/containers/<ID>/<container-id>-json.log

# for a persistent container, ensure auto restarts if machine goes down
docker update --restart=always <container_id>

# sidecar pattern - https://www.magalix.com/blog/the-sidecar-pattern

# spin up Jenkins in docker
docker run \
    --name dcct-mobile-callisto-jenkins \
    --detach \
    --network jenkins \
    --env DOCKER_HOST=tcp://docker:2376 \
    --env DOCKER_CERT_PATH=/certs/client \
    --env DOCKER_TLS_VERIFY=1 \
    --publish 80:8080 \
    --publish 50000:50000 \
    --volume jenkins-data:/var/jenkins_home \
    --volume jenkins-docker-certs:/certs/client:ro \
    <ARTIFACTORY_URL>/local-docker-dcct/mobile/dev-jenkins-lts-2.263.1:1.0

# spin up cntlm on docker
docker run --restart always --name cntlm \
  -e "USERNAME=username" \
  -e "DOMAIN=mydomain" \
  -e "PASSNTLMV2=640937B847F8C6439D87155508FA8479" \
  -e "PROXY=123.123.123.123:8080" \
  -p 3128:3128 \
  robertdebock/docker-cntlm



# Multi-stage docker image build - Building an image with multi-stage (cut down the size) - example Dockerfile  (2021/04/08)

#### First stage

FROM ubuntu:18.04 as builder					 # notice the 'as builder' in the first stage
RUN apt-get update
RUN apt-get install -y make yasm nasm as31 binutils
COPY . .
RUN make release


#### Second stage

FROM scratch            					     # can use alpine here if scratch is too empty
COPY --from=builder /asttpd /asmttpd             # this is where the executable produced from stage 1 gets copied over to the second stage
COPY /web_root/index.html /web_root/index.html

CMD ["/asmttpd", "/web_root", "8080"]



# Multi-stage docker image build - 2nd example of a python Dockerfile where you can build either a debugger or regular image, depending on the scenario
# Source: DevOps Directive - https://www.youtube.com/watch?v=qCCj7qy72Bg

###############
# base
###############

FROM python:3.8.4-slim AS base

RUN pip install pytz

WORKDIR /src
COPY . ./


######################
# debugger
######################

FROM base AS debugger

RUN pip install debuggy

ENTRYPOINT ["python","-m", "debugger", "--listen", "0.0.0.0:5678", "--wait-for-client", "-m"]


######################
# primary
######################

FROM base as primary

ENTRYPOINT [ "python", "-m"]


# END DOCKERFILE

- to build the debugger:
	docker build -t debugger:<version> --target=debugger .

- to run debugger:
	docker run -p 5678:5678 debugger:<version> unittest         # where unittest is the py module used in the video example

- to build the primary image:
  docker build -t primary:<version> --target=primary .




--------------
docker compose
--------------

# Source: Marcel Dempers (That DevOps Guy) - https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/messaging/kafka/docker-compose.yaml

- side notes
	- docker compose is a separate installer package on Linux, but is included in Docker Desktop for Mac and Windows and does not need to be installed separately in this case.
	- docker compose allows you to describe the building and runninig of multiple container images all in one file [in YAML format]


# Example of docker-compose.yaml from Marcel's Kafka with docker compose video
# Note: you would still need to create the network like in the docker example for building/running the containers individually

version: "3.8"
services:
  zookeeper-1:
    container_name: zookeeper-1
    image: aimvector/zookeeper:2.7.0
    build:
      context: ./zookeeper
    volumes:
    - ./config/zookeeper-1/zookeeper.properties:/kafka/config/zookeeper.properties
    - ./data/zookeeper-1/:/tmp/zookeeper/
    networks:
    - kafka
  kafka-1:
    container_name: kafka-1
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-1/server.properties:/kafka/config/server.properties
    - ./data/kafka-1/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-2:
    container_name: kafka-2
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-2/server.properties:/kafka/config/server.properties
    - ./data/kafka-2/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-3:
    container_name: kafka-3
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-3/server.properties:/kafka/config/server.properties
    - ./data/kafka-3/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-producer:
    container_name: kafka-producer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer:
    container_name: kafka-consumer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer-go:
    container_name: kafka-consumer-go
    image: aimvector/kafka-consumer-go:1.0.0
    build: 
      context: ./applications/consumer
    environment:
    - "KAFKA_PEERS=kafka-1:9092,kafka-2:9092,kafka-3:9092"
    - "KAFKA_TOPIC=Orders"
    - "KAFKA_VERSION=2.7.0"
    - "KAFKA_GROUP=orders"
    networks:
    - kafka
networks: 
  kafka:
    name: kafka


# How to Use the above file

docker network create kafka   # pre-requisite to running the containers
docker compose build       # will build all of the container images described in the file. Notice the different context or folder paths for each image shown in the build context section
docker compose up          # will run all of the container images that were built from this docker compose file



------------------------------
Fail2ban - for blocking access
------------------------------

- Source article: https://www.digitalocean.com/community/tutorials/how-to-protect-ssh-with-fail2ban-on-centos-7

To install on CentOS [for Rocky Linux, you would use dnf, but need to test it]:

yum install epel-release
yum install fail2ban
systemctl enable fail2ban

- Edit /etc/fail2ban/jail.local (see jail.local.txt in this repo for details)

systemctl restart fail2ban
fail2ban-client status
fail2ban-client status sshd



--------------
File transfers
--------------

# Linux large tar transfer w/ untar at the same time, nohup the log into tmp:
nohup bash -c "cat somearchive.tar | ssh -t linuxuser@x.x.x.x 'tar -C /path/to/dest -xvf - /path/to/tarup'" > /tmp/filexfer.out &

# using netcat

tar -cvf - ~/var | nc -vv -l 127.0.0.1 -p 1234    # better to use the localhost IP for security

# then on client
cd /tmp
nc 127.0.0.1 1234 | tar -xvf -


-----------
Filesystems
-----------

# General

- file systems divide the storage space on a drive into virtual compartments known as "clusters"
- maintain an index of where individual files are located



# Linux file system directory assignment meanings

/bin - stores common executables available for everyone, egs. cp rm ls

/boot - kernel and boot configuration

/dev - files which point to both physical and pseudo devices

/etc - system and program configuration files

/home - non-root user hoem directories

/lib
/lib32
/lib64 - library files used by the system, includes .so files and others

/lost+found - saved files due to failure

/media - auto-mounting place for certain external devices on some distros

/mnt - place to mount various file systems

/opt - various software

/proc - virtual filesystem for resources, processes and more

/root - root user home directory




# Types

auto - this is a special one. It will try to guess the fs type when you use this.

exFAT - Extended file allocation table
	- file system optimized for high-capacity USB flash drives and memory cards
	- maximum file size = 16EB
	- default FS for SDXC mem cards
	- broader non-Windows OS support than NTFS [including read/write on MacOS]


ext - Extended File System
	- launched in 1992 for Linux

ext2 - launched in 1993
	- default FS in many Linux systems for years

ext3
	- launched in 2001
	- this is the most common Linux fs type from a couple years back
	- introduced journaling to protect against corruption in the event of crashes or power failures

ext4
	- launched 2008
	- this is probably the most common Linux fs type of the last few years
	- maximum file size of 16TB, max volume size of 1EB
	- no native Windows or MacOS support


FAT - File Allocation Table
	- major variants: FAT12, FAT16, FAT32
		- each has increasing number of clusters, maximum file and volume sizes
		- FAT32 still widely used for removeable media and popular due to wide OS compatibility

		- max file size
			- FAT12 - 32MB [8KB clusters] or 16MB w/ 4KB clusters
			- FAT16 - 2GB/4GB
			- FAT32 - 4GB

		- max volume size
			- FAT12 - 32MB [8KB clusters]
			- FAT16 - 16GB [256KB clusters]
			- FAT32 - 32GB [Windows format]
						  - 2TB [other OS]
						  - 16TB [theoretical]


glusterfs - for Gluster


HFS - Hierarchical File System [for MacOS]
	- aka MacOS Standard
	- was introduced w/ journaling as HSF+ [HSF Extended]
	- files and volumes up to 8EB [as of MacOS 10.4]
	- 2017 - APFS launched [Apple file system]
	- no native Windows or Linux support



NTFS - New Technology File System

	- this is the most common Windows fs type or larger external hard drives
	- file size limit of 16 exabytes
	- journaling file system [maintains a record of changes in case of failures]
	- supports file permissions and encryption
	- all Windows must be installed on NTFS
		- downside: limited non-Windows OS compatibility - eg. read-only in MacOS and older Linux distros


vfat - this is the most common fs type used for smaller external hard drives


ZFS - zed file system
- created by Sun Microsystems, now developed by OpenZFS project
- an advanced fs fype that pools disk storage
- integrated volume manager to control storage hardware
	- provides increased data protection
- available for Linux, FreeBSD and TrueOS





---------------------------
firewalld (on CentOS/RHEL7)
---------------------------

# configuration file location
/etc/firewalld/firewalld.conf


# check firewalld status
systemctl status firewalld
	# or
firewall-cmd --state


# check default configuration
firewall-cmd --list-all


# check zones
firewall-cmd --get-zones   		# output: block dmz drop external home internal public trusted work

# or get default zone
firewall-cmd --get-default-zone


# check firewalld services, see all the apps which firewalld can be configured for
firewall-cmd --get-services


# allow a specific port
firewall-cmd --add-port=3306/tcp


# change default zone to dmz and open up incoming connections (this is supposed to make it internally accessible only, but you can also limit the source address entries instead of putting 0.0.0.0 as shown below)
firewall-cmd --set-default-zone=dmz
firewall-cmd --zone=dmz --add-rich-rule='rule family="ipv4" source address="0.0.0.0/0" accept'


# restart firewalld service
systemctl restart firewalld
   # or
firewall-cmd --reload


# add/remove a specific service(s)
firewall-cmd --add-service=mysql --permanent    # the permanent flag will make this setting survive a reload or restart
  # or
firewall-cmd --add-service={mysql,http,https,ldap} --permanent
  # or remove
firewall-cmd --remove-service={mysql,http,https,ldap} --permanent
firewall-cmd --reload


# port forwarding
firewall-cmd --add-forward-port=port=8080:proto=tcp:toport=80   # can add toaddr= if you want to redirect to another host


# add traffic "rich rule"
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.102" accept' # will accept all traffic from this IP 
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.103" drop'   # will not accept traffic from this IP


# set all temporary or runtime firewalld configurations permanently ***
firewall-cmd --runtime-to-permanent



-------
Fortify
-------

# upload command:
./fortifyclient -url https://fortify_url.com/ssc -authtoken xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx uploadFPR -file ../something.fpr -project "project_name" -version "latest"


---
GIT
---


### Commands and configs ###
# Sources: Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o
#					 git-scm - https://git-scm.com
#          General experience :D

# .gitattributes - use .gitattributes within a repo to define file behaviours/attributes. Eg below of contents within a .gitattributes file:

*               text=auto
*.txt		text
*.vcproj	text eol=crlf
*.sh		text eol=lf
*.jpg		-text


# Alias - creating command alias and storing within git configuration

git config --global alias.ac "commit -am"    # where ac is the new alias command
git ac "Your commit message"								 # usage for above alias



# Bisect - do a binary search through commits to figure out where a bug was introduced
# Source: git-scm - https://git-scm.com/docs/git-bisect

1] As an example, suppose you are trying to find the commit that broke a feature that was known to work in version v2.6.13-rc2 of your project. You start a bisect session as follows:

git bisect start
git bisect bad                 # Current version is bad
git bisect good v2.6.13-rc2    # v2.6.13-rc2 is known to be good


2] Once you have specified at least one bad and one good commit, git bisect selects a commit in the middle of that range of history, checks it out, and outputs something similar to the following:

# Output
      Bisecting: 675 revisions left to test after this (roughly 10 steps)
  

3] You should now compile the checked-out version and test it. If that version works correctly, type:

git bisect good


4] If that version is broken, type

git bisect bad

- then git bisect will respond with something like

# Output
	Bisecting: 337 revisions left to test after this (roughly 9 steps)


5] Keep repeating the process: compile the tree, test it, and depending on whether it is good or bad run git bisect good or git bisect bad to ask for the next commit that needs testing. Eventually
there will be no more revisions left to inspect, and the command will print out a description of the first bad commit. The reference refs/bisect/bad will be left pointing at that commit.



# Clean - cleans up hanging or loose file references

git clean -df


# Combine add and commit in one command (add applies to working directory like using "git add .")

git commit -am "Your commit message"



# Commit message update (of an existing commit)

git commit --amend -m "Your new commit message"



# Commit additional files to your last commit (without updating the commit message) - only works locally, if you haven't pushed your commit to the remote

git commit --amend --no-edit



# Credentials storing - set up storing creds in OSX
git config --global credential.helper store



# Hooks - tell GIT to do stuff upon certain CLI actions

- in local repo, look under .git/hooks folder, you can create shell scripts for different things like pre-commit actions, post-commit actions, etc.
- husky - an npm package that makes implementing GIT hooks easier [go to Github repo and "npm install husky -D" locally to find out more]


# Log - an advanced command for an easier look at git commit history locally

git log --graph --oneline --decorate


# Migrate repos from one remote to another
git clone --bare <repo_url>
cd <cloned_repo>.git
git push --mirror  https://github.com/<new-repo>



# Squash - take two or more commits in a branch and "squash" them together into a single commit
# See medium.com for more details - https://medium.com/@slamflipstrom/a-beginners-guide-to-squashing-commits-with-git-rebase-8185cf6e62ec

git rebase <branch_name> --interactive

	# the above opens a file that shows all of the commits for that branch
	# then you would "pick" the commit you want to use, and "squash" the commits you want to go into the picked commit.


# Squash automatically or auto-squash - tell GIT in advance that you will be squashing commits

git commit --fixup fb2f677       # fixup is like squash except it ignores the commit message when squashing occurs
git commit --squash fc2f55       # preps commit for squashing upon commit
git rebase -i --autosquash       



# Stash - used to set local changes aside for later

git stash        
git pop          # you can use stash and pop if you only plan to use stash for one set of changes to put away


# Stash - commands below are for saving multiple stashes

git stash save <name_of_stash>
git stash list                                  # list all stashes
git stash apply <index_of_stash_name>           # pop the selected stash from the index list


# yum install - install newer version on CentOS7
yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm
yum install git



------
Github
------

# Commands

- sudo reboot - reboot Linux server [which will ultimately reboot the Github service - note needs updating]
- sudo systemctl start elasticsearch - restart search service
- sudo systemctl status elasticsearch - check search services status


# Cool Stuff
# Source: Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o

- to use VSCode in a browser to work on a specific repo in Github, go to the repo you want to work on, and hit the "." key
	- you can use a terminal if you set up Github codespace, option can be found within the VSCode browser editor



------
groovy
------

# General
# Source: Derek Banas - https://www.youtube.com/watch?v=B98jc8hdu9g


# Example class

class GroovyTut {
	static void main(String[] args) {

		println("Hello World")

		def age = "Dog";
		age = 40;
		def name = "Derek";
		def multString = '''I am
		a String that goes on
		for many lines''';

		println("5 + 4 = " + (5 + 4));
		println("5 - 4 = " + (5 - 4));
		println("5 / 4 = " + (5.intdiv(4)));

		println("5.2 + 4.4 = " + (5.2.plus(4.4)));
		println("(3 + 2) * 5 = " + ((3 + 2) * 5));

		println("age++ = " + (age++));              				# prints 40, then increments
		println("++age = " + (++age));              				# prints 42 - increment the value of age then print the value [age being 41 at this line]

		println("Biggest Int " + Integer.MAX_VALUE);        # Biggest Int 2147483647
		println("Smallest Int " + Integer.MIN_VALUE);				# Smallest Int -2147483647

		println("Biggest Float " + Float.MAX_VALUE);        # Biggest Float 3.4028235E38

		println ('I am ${name}\n');													# Single quotes: string is taken literally, newline is not ignored. Output: I am name
		println ("I am ${name}\n");													# Double quotes: prints content of variable, newline is not ignored. Output: I am Derek
		println (multString);																# Triple quotes: allows to print on multiple lines

		println("3rd index of name " + name[3]);						# name[3] would resolve to 'r'
		println("Index of r " + name.indexOf('r'))					# name.indexOf('r') would resolve to '2'

		println("Derek == Derek " + ('Derek'.equalsIgnoreCase(Derek)));

	}

}


# Analysis of example 

- static - method or function that belongs to the class
- void - doesnt return anything when function runs
- main - main executable function that runs everytime you call the file this code is in
- String[] args - a string array called 'args' - pass in a bunch of numbers or strings, all of them would be stored in this array
- def - for declaring variables
- println - print whatever is in parenthesis to the screen/console

- arithmetic operations - see examples above [and more in video]
	- (Math.xxx(y)) - use Math class xxx function on a number y
		- eg. Math.abs() - absolute value of a number

- var.substring(x) - get substring of the contents of var starting with the x index
- var.split('x') - split everything in var with the occurrence of x [separated by commas]
- var.toList() - split var into a list of every character [separated by commas]




# Notes

- everything in groovy is an object


# Command-line

groovy groovytut.groovy       # execute groovytut.groovy script





----
helm
----

Definition: a package manager for kubernetes, defines package specifications in the form of custom YAMLs called 'charts'

Directory structure:

folder/
	Chart.yaml     			# contains info about the chart
	LICENSE 			 			# OPTIONAL: license for the chart
	README.md 		 			# OPTIONAL: human readable readme
	values.yaml    			# Default config values for the chart
	values.schema.json  # OPTIONAL: impose a structure on the values file with a JSON schema
	charts/           	# A directory containing any charts upon which this chart depends
	crds/								# Custom Resource Definitions
	templates/					# A dir of templates when combined with values generate valid K8s manifest files
	templates/NOTES.txt # OPTIONAL: plan text file containing short usage notes  


------
httpie
------

# Sources
- https://pypi.org/project/httpie/ - homepage, lots of examples

# on Ubuntu, the httpie CLI utility for interacting with websites via CLI
apt-get install python3-pip
pip3 install --upgrade httpie
http <website_URL>


### Examples ###

# Custom HTTP method, HTTP headers and JSON data:

http PUT pie.dev/put X-API-Token:123 name=John


# Submitting forms:

http -f POST pie.dev/post hello=World


# See the request that is being sent using one of the output options:

http -v pie.dev/get


# Build and print a request without sending it using offline mode:

http --offline pie.dev/post hello=offline


# Use GitHub API to post a comment on an issue with authentication:

http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body='HTTPie is awesome! :heart:'


# Upload a file using redirected input:

http pie.dev/post < files/data.json


# Download a file and save it via redirected output:

http pie.dev/image/png > image.png


# Download a file wget style:

http --download pie.dev/image/png


-------
ingress
-------

# Responsible for
	- accept/deny incoming requests
	- SSL termination
	- routing
	- load balancing



--------
ipconfig
--------

Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=ZKhorleA5aA

- ipconfig command line tool displays TCP/IP network configuration of the network adapters on a Windows computer


# command variations/switches

ipconfig /all                # displays full TCP/IP configuration of your network adapters (eg. additional info such as hostname, DHCP enabled, physical address, DHCP server address, etc.)
ipconfig /flushdns           # flushes the DNS resolver cache on the computer (side note: computers only understand numbers, not names, hence DNS translation)
	- DNS resolver cache contains a list of DNS names matched up with IPs that youve visited, so its faster on subsequent visits to those websites
		- this means less load on DNS server for lookups
	- reasons to flush DNS
		- if a website changes its IP and youre not able to resolve
		- if your cache has been hacked, redirecting you to malicious websites
			- helps hide website search behaviour

ipconfig /displaydns         # displays the contents of the DNS resolver cache
	- normally shows a bunch of records
	- Time To Live - amount of time before entry is erased from the cached





--------
IPTables
--------

# To block 116.10.191.* addresses:
sudo iptables -A INPUT -s 116.10.191.0/24 -j DROP

# To block 116.10.*.* addresses:
sudo iptables -A INPUT -s 116.10.0.0/16 -j DROP

# To block 116.*.*.* addresses:
sudo iptables -A INPUT -s 116.0.0.0/8 -j DROP

# Open a port (RHEL6)
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 5667 -j ACCEPT
service iptables save



-----
istio
-----

Video source: Istio Service Mesh Explained - https://www.youtube.com/watch?v=KUHzxTCe5Uc

- istiod - control plane of the Istio service mesh, responsible for injecting sidecar proxies into pods (which happen when apps "opt-in" to the service mesh)
 - components
    - pilot - traffic management, injecting/managing lifecycle of sidecar proxies
    - citadel - certificate authority, helps achieve mutual TLS between services within the mesh
    - galley - translates kubernetes YAML into format for Istio to process


# install Istio

curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.12 TARGET_ARCH=x86_64 bash -
mv istio-1.6.12/bin/istioctl /usr/local/bin
chmod +x /usr/local/bin/istioctl
mv istio-1.6.12 /tmp/


# Pre-flight check for compatibility with a cluster (k8s api, version, if istio already installed, can setup within cluster, auto sidecar injector)

istioctl x precheck


# Istio profile check and install with default profile

istioctl profile list
istioctl install --set profile=default
kubectl -n istio-system get pods
istioctl proxy-status


- two ways to opt-in
 	- label a namespace (all pods within the namespace will join the service mesh)
		kubectl label namespace/default istio-injection=enabled    # takes effect for new pods, you'll see an additional container within each pod
 
	- use istioctl to grab the deployment YAML and inject the sidecar proxy using istioctl
		kubectl -n ingress-nginx get deploy nginx-ingress-controller -o yaml | istioctl kube-inject -f - | kubectl apply -f -    # takes effect for new pods
  

- add-ons
	- comes pre-shipped with a Grafana and Kiali dashboard add-ons for rich metrics/telemetry
	- to install Grafana (to the Istio namespace):
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/prometheus.yaml
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/grafana.yaml
		kubectl get pods --namespace istio-system

	- to expose Grafana dashboard
		kubectl -b istio-system port-forward svc/grafana 3000

    - see something wrong in Grafana? Command below to check logs within the cluster of pods throwing errors:
    	kubectl logs <pod_name> -c <container_name> --tail 50

    - need to buy dev some time to fix something? Implement virtual service in Istio (i.e. automated retries, canary deploys, traffic splitting)
    	- see https://www.youtube.com/watch?v=KUHzxTCe5Uc @ 26:10


----
jaxb
----

# find jax2b cached artifacts in maven local repo
find . | grep jaxb2-plugin | grep 0.8.1 | grep jar


-------
Jenkins
-------

# when login is broken
	- Stop Jenkins (the easiest way to do this is to kill the servlet container. Best way is to run a stop script, either a custom shell or stopping the service if its running that way)
	- Go to $JENKINS_HOME in the file system and find config.xml file.
	- Open this file in the editor.
	- Look for the false element in this file.
	- Replace true with false
	- Remove the elements authorizationStrategy and securityRealm
	- Start Jenkins




-----
kafka
-----

# Introduction
# Source: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=heR3I3Wxgro
#																					 - https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/messaging/kafka  [source code + README with instructions]

- Apache Kafka - an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration,
								 and mission-critical applications.

- zookeeper - centralized service for maintaining configuration information [naming and providing distributed synchronization for apache services such as kafka]
						- keeps track of status of kafka cluster nodes
						- also keeps track of partitions and topics


- logistics
	- client [producer] ----> kafka broker	----> application server[s]

		- clients are referred to as "producers"
		- messages go to "brokers" which are kafka instances

		- servers that consume messages are called "consumers"
			- consumer can subscribe to a topic and receive messages, in order, indexing what it has received so far
				- if a consumer dies/shuts down, once its back online it can use the index number to pick up consuming messages from the index it left off on

		- messages are stored on the brokers in "topics"
			- topics can be divided into partitions, messages/copies of messages go into the partitions
				- if a broker dies, messages are not lost as a result of this replication

		- in source vid and repo, the example for testing will make use of pre-packed scripts that come with the Kafka installation to mock producers and consumers for testing purposes


 - testing

	- the Github repo above has a Dockerfile to build a Debian/Java11 container with Kafka installed [see dockerfile under messaging/kafka folder]
 		- can spin up multiple instances of this image for having more than one broker to replicate topics/partitions/messages
 			- minor configuration updates requires for server.properties, such as broker ID, which has to be unique for each container
 		- for zookeeper, its the same dockerfile except you use the start-zookeeper.sh script as the entrypoint. For testing, one zookeeper container is sufficient
 			- see zookeeper dockerfile and start script under messaging/kafka/zookeeper



------------------------------------------------------------------------
kind a software for running kubernetes clusters within docker containers
------------------------------------------------------------------------

# Sources
- https://www.youtube.com/watch?v=m-IlbCgSzkc&list=WL&index=1

# Prerequisites
- docker
- go (1.11+)
- kubectl

# Install kind
- can download binary from Github, place in any PATH (e.g. /usr/local/bin/kind)

# Create kind kubernetescluster
kind create cluster --name <cluster_name>
	- this will pull the kind docker image to spin up for creating a single master node cluster

kind get clusters
	- show running clusters

kind delete cluster
	- to remove the created cluster

# Check running nodes
kubectl get nodes

# set kubeconfig to point to your new cluster (can run more than once to add clusters)
export KUBECONFIG="$(kind get kubeconfig-path --name="<cluster_name>"


# Multi-node cluster - its possible to create a kind cluster with worker nodes added

- create a yaml with the following def
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker


- create cluster with this yaml definition

kind create cluster --config /path/to/kind.yaml


- verify

kind get clusters
docker ps 					                   # will show running kind containers, one which is your master/control plane as in the first example, and one or more as your worker nodes
kubectl cluster-info                   # show info about cluster, including the URL to hit it
kubectl get nodes -o wide  					   # see the nodes not as docker containers but as kind nodes

docker exec -ti <container_ID> bash    # log into any of the kind containers
which ctr 														 # container runtime binary similar to docker command line
ctr namespaces list  									 # can see namespaces within the cluster
ctr namespace k8s.io containers list   # can see all the containers that make up the cluster



-------
kubectl
-------

# Random useful kubectl commands

kubectl expose deploy <pod> --port <port> --dry-run -o yaml     # Expose a deployment as a service, dry run to check yaml definition
kubectl -n <namespace> port-forward <pod_name> <port> 			    # Port forward to expose an app as a service
kubectl scale deploy <deployment_name> --replicas=<number>      # Manually scale a deployment



----------
kubernetes
----------


# General

- control nodes and control plane - the administrative node[s] of a cluster which instruct and manage the worker nodes which run workloads

	- kube-api server - main component which communicates instructions to/from all cluster components, and validates/configures data for api objects such as pods, services, RCs, etc.
	- etcd - key/value store for a cluster
	- scheduler - responsible for telling api server where a pod needs to be scheduled and when
	- kubelet - the agent which sits on the worker node and communicates with the api server
	- kube-proxy - intercommunication mechanism in a cluster between nodes
	- controller-manager - responsible for telling the api server to match the desired state with the actual state of pods within a cluster
	- daemonset - ensures that all or some nodes run a copy of a pod [e.g. kube-proxy daemonset would ensure kube-proxy pod runs on every available node in the cluster]



# Installing a Kubernetes cluster using kubeadm [Centos7]
# Source: Just Me And Opensource - https://www.youtube.com/watch?v=v6Jh2sZClDY
#																 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-centos-7.md
# 															 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-ubuntu-20.md [for install on Ubuntu 20.04]         


For Centos 7 install:

On both master and worker,

1. Disable firewalld

systemctl disable firewalld; systemctl stop firewalld


2. Disable swap

swapoff -a; sed -i '/swap/d' /etc/fstab


3. Disable SELinux

setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux


4. Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


5. Install docker  #### note will have to check how this works for containerd since docker is deprecated in K8s as of v1.21

yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-19.03.12 
systemctl enable --now docker


6. Add yum repo for Kubernetes install

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


7. Install Kubernetes

yum install -y kubeadm-1.18.5-0 kubelet-1.18.5-0 kubectl-1.18.5-0


8. Enable and start kubelet service

systemctl enable --now kubelet



On master node,

9. Initialize Kubernetes cluster

kubeadm init --apiserver-advertise-address=172.16.16.100 --pod-network-cidr=192.168.0.0/16


10. Deploy Calico network

kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml


11. Cluster join command [generating token for worker nodes]

kubeadm token create --print-join-command


Side note: if you want to be able to run kubectl commands as non-root user, then as a non-root user perform these:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


12. On the worker node[s], use the output from the "kubeadm token create" command and run it.


13. Verify cluster

kubectl get nodes
kubectl get cs         # get component status



# Webhooks
# Source - Marcel Dempers (That DevOps Guy) - https://www.youtube.com/watch?v=1mNYSn2KMZk
#																						- https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/kubernetes/admissioncontrollers/introduction


- admission webhooks
	- types
		- mutating - intercepts object/YAML before it hits API server, allows us to make changes first
			- egs. inject CPU limits, labels, ulimits, go to a specific node, etc.

		- validation - accept or reject request
			- egs.
				- policy enforcement - if CPU/mem limits not set, do not create pod
														 - only allow pods if the image is part of a specific image registry
				- notfiications when events occur


- steps to create a webhook from scratch

	1. Create a K8s cluster [can use kind or k3s for ease]

	2. Create a TLS certificate for the webhook [can use Cloudflare SSL utility as shown below, the swiss army knife for dealing with TLS certs]

		cd kubernetes/admissioncontrollers/introduction      # within the Github repo in the source above
		docker run -it --rm -v ${PWD}}:/work -w /work debian bash


		# a. Install Cloudflare SSL and json utilities

		apt-get update && apt-get install -y curl &&
		curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl && \
		curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson && \
		chmod +x /usr/local/bin/cfssl && \
		chmod +x /usr/local/bin/cfssljson


		# b. Generate a CA certificate in /tmp
		cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca


		# c. Generate webhook certificate

    cfssl gencert \
    	-ca=/tmp/ca.pem \
    	-ca-key=/tmp/ca-key.pem \
    	-config=./tls/ca-config.json \
  	  -hostname="example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1" \
  		-profile=default \
  		./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook



		# d. Make a secret

cat <<EOF > ./tls/example-webhook-tls.yaml
apiVersion: v1
kind: Secret
metadata:
	name: example-webhook-tls
type: Opaque
data:
	tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\n')
  tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\n') 
EOF


		# e. Generate CA Bundle + inject into template
		
		ca_pem_b64="$(openssl base64 -A <"/tmp/ca.pem")"         # where -A means no line breaks so the full certificate string can be stored in the variable

		sed -e 's@${CA_PEM_B64}@'"$ca_pem_b64"'@g' <"webhook-template.yaml" \
    > webhook.yaml


  3. Create the webhook with the generated CA

  # a. YAML templated definition - see https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/kubernetes/admissioncontrollers/introduction/webhook-template.yaml

  apiVersion: admissionregistration.k8s.io/v1
  kind: MutatingWebhookConfiguration
  metadata:
    name: example-webhook
  webhooks:
    - name: example-webhook.default.svc.cluster.local
      admissionReviewVersions:
        - "v1beta1"
      sideEffects: "None"
      timeoutSeconds: 30
      objectSelector:
        matchLabels:
          example-webhook-enabled: "true"
      clientConfig:
        service:
          name: example-webhook
          namespace: default
          path: "/mutate"
        caBundle: "${CA_PEM_B64}"
      rules:
        - operations: [ "CREATE" ]
          apiGroups: [""]
          apiVersions: ["v1"]
          resources: ["pods"]

   # End YAML
 
     - note the CA_PEM_B64 variable, referencing step 2e. Running that will replace the variable with the actual value in webhook.yaml

   #### INCOMPLETE ####



-----------------------------------------
kURL - a custom Kubernetes distro creator
-----------------------------------------

# Website - https://kurl.sh/

# Demo for installation: Just Me and Opensource - https://www.youtube.com/watch?v=_4edisHDWzs

- the kURL tool helps you to create a Kubernetes cluster which can be customized off the bat with different tools that are more typical/standard that are not provided out of the box right away
	with a vanilla K8s installation such as monitoring tools [Prometheus], cluster backup and restore mechanism [Velero], certificate management [CertManager], and more. See kurl.sh for more details.



---
ldd
---

# Linux - checking library dependencies
ldd -d 



---------
localtime 
---------

# change time zone

rm -f /etc/localtime
ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime
date




--------
lscolors
--------

# Colourize text in terminal:
export LSCOLORS=gxBxhxDxfxhxhxhxhxcxcx
eval "$(dircolors /etc/DIR_COLORS)"


----
lsof
----

lsof <filename>       # check which processes have this file open
lsof -p <PID>         # check a specific pids open files [could be multiple]
lsof -u <userid>      # check list of open files for specific user
lsof -i <port>        # check which processes are listening on specific port
lsof -i <protocol>    # check which processes are listening on specific protocol [i.e. tcp]




---
lxd
---

# Source: https://linuxcontainers.org/

- a next generation system container and virtual machine manager
- the lxc cli that lxd uses should not be confused with lxc itself [which is OS-level virtualization for running multiple Linux containers on a single host]. lxd is really just
  an alternative to lxc, and makes use of lxc tools via liblxc, such as the cli
	


# Commands - the below are lxc cli commands that can be executed within an lxd environment
# Can also try online without installing locally as well at https://linuxcontainers.org/lxd/try-it/


# Basics for listing and running container images

lxc image list        							    							# list images stored locally (docker equivalent command: docker images)
lxc image list ubuntu: | less  			    							# list of images from the ubuntu: registry
lxc launch images:ubuntu/18.04 [container_name]     	# run a container called container_name based on the ubuntu 18.04 image
lxc list  														  							# check running containers
lxc info [container_name]               							# inspect running container named container_name
lxc config show [container_name]											# show configuration details of container_name
lxc stop [container name] 														# stop running container container_name
lxc delete [container_name]  													# remove running container container_name
lxc start [container_name]														# start running container container_name
lxc delete --force [container_name]                   # force stop and delete container_name


# Container interaction commands

lxc exec [container_name] -- free -m                	      # execute free -m within container_name
lxc config set [container_name] limits.memory 128MB   			# set memory limit on container_name
lxc exec [container_name] -- apt-get update           			# run apt-get update within container_name
lxc exec [container_name] -- apt-get install sl -y    			# install sl package within container_name
lxc exec [container_name] -- /usr/games/sl            			# run sl program within container_name [sl stands for 'Steam Locomotive' - displays a train on-screen when you mistype ls command :D ]
lxc snapshot [container_name] clean  												# take a snapshot called clean of running container_name, which can be used for restoration if something messes up in the container
lxc restore [container_name] clean  												# restore the clean snapshot of container_name
lxc file pull [container_name]/etc/hosts .            			# copy a file from container_name to local
lxc file push hosts [container_name]/etc/hosts        			# copy a file from local to container_name
lxc file pull [container_name]/var/log/syslog - | less      # check log files in the container
lxc exec lxdserver:[container_3] bash  				 							# log into remote container on remote lxdserver
lxc copy lxdserver:[container_3] lxdserver:[container_4]    # create a copy of that container on that same remote
lxc move lxdserver:[container_4] [container_5]              # move the copied remote container to local and rename it to container_5
lxc start [container_5]                                     # and then start container_5


# Image commands

lxc publish [container_name]/clean --alias clean-ubuntu     # publish the snapshot "clean" of container_name to the default registry
lxc launch clean-ubuntu [container_2]												# launch a new container based on the published image
lxc remote list                                             # list all remote registries
lxc list lxdserver:  																			  # list all remote containers on lxdserver
lxc image list lxdserver:	 																	# list all images on remote lxdserver
lxc launch clean-ubuntu lxdserver:[container_3]       			# create new container called container_3 on remote lxdserver
lxc image delete clean-ubuntu                               # delete the published image



---------
Mac OS X
---------

   # check RAM:
   		system_profiler SPHardwareDataType | grep "Memory:"

   # check # of CPU cores:
   		sysctl -a | grep cpu.core_count

   # check GPU:
	 	  system_profiler SPDisplaysDataType

   # stop and start Jenkins
   		sudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist
		  sudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plist

   # update URL
   	    sudo defaults write /Library/Preferences/org.jenkins-ci httpPort 8082
   	    sudo defaults write /Library/Preferences/org.jenkins-ci prefix /jenkins
   	    # for brew:
   	       /usr/local/Cellar/jenkins/2.x.x/homebrew.mxcl.jenkins.plist
   	       		update:
   	       			  <string>--httpPort=8082</string>
  					  <string>--prefix=/jenkins</string>
   	       brew services restart jenkins-lts

   # update heap size/permGen
        sudo defaults write /Library/Preferences/org.jenkins-ci minPermGen 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci permGen 2048m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci minHeapSize 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci heapSize 2048m


   # check installed java versions:
   		  /usr/libexec/java_home -V


-----------------
Mounting a Volume
-----------------

# Source article: https://www.linode.com/docs/platform/block-storage/how-to-use-block-storage-with-your-linode/#add-a-volume-from-the-linode-detail-page

# One-time step for defining the FS type of the volume
mkfs.ext4 $volume_path   # check the file system path of the volume in your cloud console, it will show you the device drive location

# Steps for mounting the volume
mkdir /mnt/my-volume
mount $volume_path /mnt/my-volume
df -h

# Create entry in /etc/fstab:
FILE_SYSTEM_PATH /mnt/my-volume ext4 defaults,noatime,nofail 0 2

* noatime - This will save space and time by preventing writes made to the filesystem for data being read on the volume.
* nofail - If the volume is not attached, this will allow your server to boot/reboot normally without hanging at dependency failures if the volume is not attached.



# if you need to unmount it or reboot your Linode without affecting the volume
umount /mnt/my-volume
Remove /etc/fstab entry



--------
netgroup
--------

# Check netgroup list (Linux):
> ldaplist -l netgroup <netgroup_name> 



---------
netcat/nc
---------

### SOURCE page: https://www.linode.com/docs/guides/netcat/ ###


# make netcat act as the telnet utility (TCP protocol is the default, use -u for UDP procotol)

nc localhost 22


# make netcat act as a server, accept incoming connection on a given port

nc -l -p 1234


# get more info from remote server (eg. for connectivity issues)

nc -v localhost 1234  [or nc -vv]


# port scanning (eg. on localhost, from ports 1-30)

nc -z -vv -n 127.0.0.1 1-30


# transferring files (a client connects to port 4567 below to receive the access.log contents)

cat access.log | nc -vv -l -p 4567

# and then

nc -vv localhost 4567 > fileToGet
^C [to close the connection]


# turn a process into a server (when client connects, nc will execute /bin/bash, to give shell access to machine)

nc -vv -l -p 12345 -e /bin/bash


# executing a command after connecting

nc -vv -c "ls -l" -l 127.0.0.1 -p 1234


# act as simple web server (eg. can use curl or wget to connect to this port to get the index.html contents)

nc -vv -l 127.0.0.1 -p 4567 < index.html
wget -qO- http://localhost:4567/


# get data from web servers

nc www.linode.com 80

# OR

echo -en "GET / HTTP/1.0\n\n\n" | netcat www.linode.com 80



# Create a chat server

nc -vv -l 127.0.0.1 -p 1234

# and from the client

nc -vv 127.0.0.1 1234

# then type in terminal





-------
netstat
-------

netstat -tupln   # display network connections, tcp and udp, show which processes are using which sockets (need root for p option), -l is for listening sockets

-n - show only numbers, not names [i.e. DNS names, which takes time to calculate when not using the -n switch]
-a - show active connections, where TCP and UDP are listening
-b - show which program is creating the connection [e.g. you'll see something like chrome.exe]
-f - shows FQDN in foreign address column
-? - show all options



--------------------
networking - general
--------------------

 - An example LAN [NetworkChucks setup]: PC -> Switch -> Router -> Firewall -> Modem -> Fiber optic -> Internet


# ARP - Address Resolution Protocol
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=cn8Zxh9bPio

- definition - used to resolve IP addresses to MAC addresses [the physical address of a device, eg. 00-04-5A-63-AI-66]

- info
	- devices need the MAC address for communication on a LAN
	- devices use ARP to acquire MAC address for a device
		- IP address is used to locate device on a network
		- MAC address is used to identify the actual device

- process
	1. device checks its ARP cache table, the internal list of all IP/MAC address combinations

		arp -a  				# Windows cmd to check list

	2. device sends a broadcast message to every device on the network for a specific destination IP address it wants to talk to
		- asks for MAC address
	3. receiving device with the destination IP responds with MAC address
	4. stores IP/MAC address info in arp cache [which can be checked with arp -a command]
	5. communication between devices can begin
	6. subsequent communication to that device, it can check the ARP cache instead of broadcasting request to whole network


- two types of ARP entries

	- dynamic - entry is created automatically when a device sends out broadcast message requesting a MAC address
						- entries are not permanent, flushed periodically
	- static - a manual entry of a IP/MAC address into the ARP cache
					 - often used to reduce any unnecessary ARP broadcast traffic on a network

		arp -s x.x.x.x  yy-yy-yy-yy-yy-yy     # command for entering static entry into ARP cache, with IP followed by MAC address




# Bluetooth vs. Wi-Fi
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=mPMGRILsOVk&list=WL&index=1

- both are radio frequency technlogies for wirelessly connecting electronic devices

- Bluetooth
	- a low power wireless technology that uses a short-range radio that provides a way to connect nearby devices to each other
	- has a computer chip that will broadcast a signal for other devices to connect and exchange data - known as "pairing"

	- most common usages:
		- wireless audio streaming [e.g. wireless earbuds to iPhone]
		- headphones to a TV
		- wireless keyboard/mouse to a computer
		- cellphone to car audio for hands free

	- range: 30ft or 10m


- Wi-Fi
	- a wireless technology that uses radio waves that allows devices to be able to connect to the internet
	- most common method: via Wi-Fi router in a network, for devices to connect to
	- range: 100 - 300ft [30 - 91m]


- differences
	- Bluetooth
		- used to connect devices to each other
		- less power
		- slower transfer rate
		- shorter range
		- longer battery life
		- smaller battery
		- simpler to connect to than Wi-Fi

	
	- Wi-Fi
		- used to connect devices to the internet
		- faster than Bluetooth
		- 10x longer ranger
		- often requires a password

	
- both operate at 2.4GHz
	- other devices also operate at this frequency. Bluetooth is less vulernerable/highly resistant to interference because it uses "Frequency Hopping Spread Spectrum" [FHSS]
		- trasmits signals in a specific pattern that only the transmitting and receiving devices know
		- hops between 79 different channels
		- changes channels 1600 times per second




# Hubs vs. Switches vs. Routers
# Source: https://www.youtube.com/watch?v=1z0ULvg_pW8

- hub - connect all network devices together
			- not intelligent, only knows when something is connected to it
			- data is copied/transmitted to all ports which have a connection to it
				- can be a security concern if you dont want every device to see data being transmitted between one device on the network and another

- switch - like a hub except intelligent, only broadcasts between communicating devices, not every device on network
			   - can detect specific devices that are connected to it
			   - keeps track of mac addresses of the devices connected
			   

- both of the above
			- for internal communication within a network
			- cannot read IP addresses
			- used to create networks


- router - route/fwds data based on the IP address
				 - inspects packet for the IP address before it routes it
				 - the gateway of a network
				 	 - accepts only packets fron the internet intended for its own network
				 - used to connect networks


# IPv4 and IPv6 Dual Stack
- IPv4 and IPv6 together is referred to as "dual stack"
	- IPv4 addresses are running out and are being migrated to IPv6
	- dual stack connectivity allows your ISP to process IPv4 and IPv6 data simultaneously




# Misc

- default gateway - IP address of your router or modem/router combo that your computer connects to
- subnet mask - defines which parts of the IP address refers to the network and host
- 



# Port Forwarding
# Source: https://www.youtube.com/watch?v=2G1ueMDgwxw

- definition: allows computers over the internet to connect to a specfic computer or service within a private network

- info
	- port - a logical connection that is used by programs and services to exchange information
				 - is always associated with an IP address
				 - identified by a unique number
				 - IP address purpose is determined by the port [e.g. some_ip:80 usually means a http webpage, 22 for SSH/SFTP, 443 for HTTPS]
				 - "Well Known Ports" - priviledged category of ports ranging from "0 - 1023"

- eg. Remote Desktop Connection from a public computer to a destination computer on a private network behind a router

	- the router will receive the RDP request and needs to know where to forward for port 3389, the default RDP port
	
		- need to configure the destination computers private IP into the routers webpage
			- under single port forwarding -> app name: RDP, external port: 3389, internal port: 3389, protocol: BOTH, To IP Address: <dest_pc_ip>, Enabled: True
			- source computer will try to hit the router IP:port -> router forwards to internal IP:port 

	


-----
nginx
-----

# stop nginx
  nginx -s stop

# start nginx
  ./nginx


# Concept in K8s
- ingress.yaml -> service.yaml -> application-deploy.yaml
	- ingress has the routing rules, service pods act as the load balancers to the application pods/nodes


-----------
nice/renice
-----------

# definition: can run a program with a modified scheduling priority
# range: from -20 to 19. Default is 0, the lower the value, the higher the priority (so -20 is highest priority)


# start process with nice value (10 by default)
nice [command]

# start process with a specific nice value (2 in this case)
nice -n 2 [command]

# change nice value of a running process
renice -n 2 -p [pid]

# change nice value for all running processes for a user
renice -n 2 -u [user] 



------
nodejs
------

# NodeJS Debugging in Docker
# Source Video: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=ktvgr9VZ4dc
# Source Code: https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/nodejs    [for docker stuff and source code]
# 						 https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/.vscode   [for vscode add-on launch.json which runs the debugger]


# Prerequisites
	- VScode
	- Docker


# Components

- dockerfile [in the nodejs folder in the Github repo]
	- the debugger image can be built using docker compose [YAML can be found at the top of the repo]
	- need to expose the server port of the nodejs app and another port for the debugger
	- the source code itself is mounted into the container, technically not needing nodejs installed on the local since its in the container,
		so you just update the code locally and it automatically reflects in the container
		- the only time you need to rebuild the actual image is when package.json gets updated since you may need to npm install any updated dependencies



---
npm
---

# Set auth token in npmrc for a target registry
npm config set '//artifactory_url/artifactory/api/npm/npm/:_authToken' 'zsASDKLNvasdfoloi3w123lk1jhof8jaspodfj'


# Set node-sass binary site for pulling sass bindings during an npm install (if required)
npm config set sass_binary_site=http://artifactory_url:8081/artifactory/node-sass


# Set npm registry for resolving dependencies, publishing
npm config set registry http://artifactory_url:8081/artifactory/api/npm/npm/


# Set root user (in case installs fail with permission denied, only for root or sudo installs)
npm -g config set user root


# Set SSL check to false (use only temporarily as a workaround for certificate errors)
npm config set strict-ssl false


# Remove all node_modules folders on an FS in multiple locations
npx npkill




--------------------
performance analysis
--------------------

# Sources
- https://www.slideshare.net/brendangregg/container-performance-analysis

# To review
- Brendan Greggs ftrace repo - https://github.com/brendangregg/perf-tools
- BGs eBPF blog post - http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html
- BGs eBPF article - http://www.brendangregg.com/ebpf.html
- eBPF [aka BPF] - https://github.com/iovisor/bcc
- Intel snap - a metric collector used by monitoring GUIs: https://github.com/intelsdi-x/snap
- Collectd plugin - https://github.com/bobrik/collectd-docker


# Basic commands

dmesg | tail    	# shows kernel errors
free -m [or -g]		# memory usage
iostat -xz 1		# disk I/O
mpstat -P ALL 1		# CPU balance
netstat -s 			# shows statistics by protocol (network)
pidstat 1			# process usage
sar -n DEV 1		# network I/O
sar -n TCP,ETCP, 1  # TCP stats
top 				# overview of processes (hit 1 to see CPU usage). Note: does not show container ID
uptime     			# shows time passed since last reboot of server
vmstat 1			# overall stats by time


# Advanced tools

iosnoop				# disk I/O events w/ latency
btrfsdist			# latency histogram
zfsslower 1			# file system latency is a better pain indicator than disk latency


# Namespaces - limit what you can see, whereas cgroups limit what you can use

cat /proc/<PID>/cgroup 						 # events for target container within the cgroup
docker stats 								 # a top for containers
dockerpsns.sh [or docker ps --namespaces]    # an initial check before deep dive. More info at https://github.com/docker/docker/issues/32501
grep <PID> /sys/fs/cgroup/cpu,cpuacct/docker/*/tasks | cut -d/ -f7      # will show container associated with PID
htop 										 # can show cgroup (unlike top), but may truncate important info
ls -l /proc/<PID>/ns/*						 # shows ns info about a process
nsenter -t <PID> -u hostname				 # check which host target PID is running on
	-m, -u, -i, -n, -p, -U 					 # mount, uts, ipc, net, pid, user
nsenter -t <PID> -n netstat -i 				 # container netstat
nsenter -t <PID> -m -p df -h 				 # container file system usage
nsenter -t <PID> -p top 					 # container top
nsenter -t <PID> -m -p top 					 # -m for more?
	grep NSpid /proc/<PID>/status 			 # an alternative to above command
systemd-cgtop								 # a top for cgroups



# cgroup metrics cont'd

cd /sys/fs/cgroup/cpu,cpuacct/docker/<container_ID>
ls
cat cpuacct.usage
cat cpu.stat 								 # will show total time throttled



# CPU profiling

perf record -F 49 -a -g -- sleep 30

	# Limitations
	- perf wont be able to find /tmp/perf-PID.map files on the host, PID would be different as well
	- perf cant find container binaries in host paths [i.e. what /usr/bin/java?]

	# Other notes
	- Can copy files to the host, map PIDs, then run perf script/report:
		- https://blog.alicegoldfuss.com/making-flamegraphs-with-containerized-java/
		- http://batey.info/docker-jvm-flamegraphs.html
	- Can nsenter (-m -u -i -n -p) a "power shell, and then run perf -p PID
	- perf should be fixed to be namespace aware


# CPU Flame Graphs

git clone --depth 1 https://github.com/brendangregg/FlameGraph
cd FlameGraph
perf record -F 40 -a -g -- sleep 30
perf script | ./stackcollapse-perf.pl | ./flamegraph.pl > perf.avg

	# Notes
	- see CPU Profiling section for getting perf symbols to work
	- from the host, can study all containers, as well as container overheads

strace -fp <PID>							 # trace/debugging target PID


# ftrace

funccount '*ovl*'							 # show kernel function calls (overlay)
kprobe -s 'p:ovl_fill_merge ctx=%di name=+0(%si):string'   # look into more


# BPF (Berkeley Packet Filter)
runqlat -p <PID> 10 1						 # show queue latency, 10 lines, update every second
runqlat --pidnss -m 						 # show per namespace


#### Summary ####

Identify bottlenecks:
1. in the host vs. container, using system metrics
2. in application code on contaienrs, using CPU flame graphs
3. deeper in the kernel, using tracing tools



------
Piston
------

# Source: Engineer Man - https://www.youtube.com/watch?v=SD4KgwdjmdI
#									     - https://github.com/engineer-man/piston

- This is a code execution engine built by Engineer Man, originally running on Docker but was moved to LXC for better performance [and incidentally, better security]
	- you can take any source code and put it into the engine, and it will execute it for you
	- it even runs untrusted and possibly malicious code without fear of any harmful effects


### To be filled in with more details at a later date if a useful place to apply it is found ###


--
ps
--

# Check full process args on Linux:
ps eww -p <PID>

or

ps -auwwwx | grep <anything>




------
python
------

Threading vs Multiprocessing - both are trying to do the same thing: to run multiple things at the same time.

Threading:
- A new thread is spawned within the existing process
- Starting a thread is faster than starting a process
- Memory is shared between all threads
- Mutexes often necessary to control access to shared data
- One GIL (Global Interpreter Lock) for all threads

Multiprocessing:
- A new process is started independent from the first process
- Starting a process is slower than starting a thread
- Memory is not shared between processes
- Mutexes not necessary (unless threading in the new process)
- One GIL (Global Interpreter Lock) for each process




----
RAID
----

# Sources: Powercert Animated Videos - https://www.youtube.com/watch?v=U-OCdTeZLac [RAID 0, 1, 5, 10]
#																		 - https://www.youtube.com/watch?v=UuUgfCvt9-Q [RAID 5 & 6]


- RAID stands for "Redundant Array of Indepdent Disks"
- used for data loss prevention in the event of hardware failure
	- data is spread across multiple disks for fault tolerance

- types:

	- RAID 0
		- not fault tolerant
		- data is 'striped' across multiple disks
			- eg. disk A would have certain parts of the data, disk B would have the other parts. If one of these disks fail, all of the data would be lost
		- advantage: speed
			- 2 disk controllers instead of one, makes data access much faster

	- RAID 1
		- fault tolerant
		- data is copied on more than 1 disk [duplicate copy of the data across each disk]
			- i.e. each disk has the same data 

	- RAID 5
		- requires 3 or more disks
		- fast and can store a large amount of data
		- can handle single disk failure, cannot handle two disk failures at the same time
		- data is not duplicated but striped across multiple disks along with "parity"
			- parity
				- an entire copy of the data across each disk
				- used to rebuild a failed disk
				- eg. an array of 4 disks totaling 4 TB
					- only 3 TB used for data storage
					- 1 TB used to store parity for rebuilding any of the 4 disks in the event of failure

	- RAID 6
		- requires 4 or more disks
		- 50% capacity used for data storage
		- like RAID 5 in terms of data striping, but parity is spread TWICE on each disk
			- can handle 2 disk failures at the same time [rare occurrence]
				- if 2 drives fail at the same time, double parity from other disks would be used to rebuild the data on the new drives
		- read performance is the same between RAID 5 and 6
		- write performance is slower because it has to write 2 independent parity blocks instead of 1


	- RAID 10
		- combines RAID 0 and 1 together
		- benefits of RAID fault tolerance from RAID 1 with the speed from RAID 0
			- downside: can only use 50% of the capacity for data storage
		- minimum of 4 disks needed
		- RAID 0 pointing to 2 RAID 1 sets of 2 disks 
			- RAID 0 -> RAID 1 -> disk 1 
												 -> disk 2
							 -> RAID 1 -> disk 3
							 					 -> disk 4





------------
Raspberry Pi
------------

# Installing Kubernetes on Raspberry Pis'
# Source - NetowrkChuck - https://www.youtube.com/watch?v=X9fSMGkjtug

- Things youll need:
	- 2 or more Raspberry Pis
	- micro SD USB adapter
	- micro SD card


1) Plug in your micro SD card into your PC and install the Rapberry Pi OS image onto it (can be headless, such as Raspberry Pi OS Lite)
	- using Rapberry Pi imager program

2) Plug the micro SD into your Rapberry Pi and let it boot up

3) Once it is up, remove the SD card, and plug it back into your computer
	- booting the Raspberry Pi for the first time creates a boot folder. Some edits are required, so next step...

4) In the "boot" folder [will show up as a drive once you plug back into your PC], open cmdline.txt and paste the below text:

		cgroup_memory=1 cgroup_enable=memory ip=x.x.x.x::y.y.y.y:255.255.255.0:somehostname:eth0:off

				- where
					- x.x.x.x is an internal IP in your network that isnt used
					- y.y.y.y is the default gateway
					- 255.255.255.0 is the subnet mask
					- somehostname is the hostname for your Raspberry Pi
					- eth0 is the network interface card
					- off means turn off auto configuration


5) Edit config.txt, scroll to the bottom and add: arm_64bit=1
	- means to use 64 bit of Raspbian [the Raspberry Pi OS] 


6) Enabling SSH on the Raspberry Pi - open Powershell and in the boot drives directory, run:

	new-item ssh
		- this will create a new blank file called ssh

	Note: on Mac or Linux, you would run touch ssh


7) Plug the SD card back into the Raspberry Pi, wait to boot

8) On your PC, ping the Raspberry Pi at the internal address you entered in cmdline.txt [and confirm the Rapberry Pi replies]
	
	ping x.x.x.x -t

		- where -t flag means continously


9) SSH into the Raspbeery Pi

	ssh pi@x.x.x.x
		- password would be "raspberry"


10) Become the root user

	sudo su -


11) Enable IPTables

	sudo iptables -F
	sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy


12) Reboot the Raspberry Pi

	sudo reboot


13) Repeat on each Raspberry Pi you want to configure into your cluster


14) SSH back into your first Raspberry Pi and switch to root


15) Install k3s on your first node [which will become your master node]

	curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s          # kubeconfig mode setting will allow to import k3s into Rancher


16) CHeck your cluster

	kubectl get nodes   				# should only show one node at this point


17) Get the master nodes token:

	cat /var/lib/rancher/k3s/server/node-token 				# use this token to register the rest of the nodes to the cluster


18) On another Raspberry Pi, install k3s, registering it as a worker node:

	curl -sfL https://get.k3s.io | K3S_TOKEN="YOURTOKEN" K3S_URL="https://x.x.x.x:6443" K3S_NODE_NAME="somehostname#" sh -s

		- where somehostname# is a unique name for your node

		- note: kubectl can only be used on the master, not the worker nodes 


19) Check your cluster again

	kubectl get nodes



20) Install Rancher on another VM or container [switch to root]

	mkdir -p /etc/rancher/rke2
	cd /etc/rancher/rke2
	vi config.yaml

	# contents of config.yaml

	token: mylittlepony
	tls-san:
		- x.x.x.x             # where x.x.x.x is the VM/container IP

  # end contents

  curl -sfL https://get.rancher.io | sh -


21) Verify Rancher was installed

	 rancherd --help


22) Enable Rancher as a service and start it

	systemctl enable rancherd-server.service
	systemctl start rancherd-server.service

	# this also sets up a K8s cluster within the VM/container with Rancher installed in it


23) Get Rancher login creds

  rancherd reset-admin   # will give you server URL + default username and password to login to the console with


24) Once you login to the Rancher console [and reset your password], add your Raspberry Pi cluster:

	- two options to choose from
		- I want to create or manage multiple clusters                  <----- choose this one
		- Im only going to use the cluster Rancher was installed on

	- agree to Terms and Conditions and continue

	- copy and save server URL thats generated


25) You will see the Rancher local cluster. Next step is to add the Raspberry Pi cluster

	- click Add Cluster
	- click Other Cluster
	- enter a name for the cluster
	- run the insecure curl command in your terminal [or secure if your K3s cluster is secure] to import the Raspberry Pi cluster into Rancher
	- click Done in the consoel once the command executes successfully



----------
References 
----------

# Technical
- https://crontab.guru/          # service to figure out any crontab entry
- https://bundlephobia.com/      # check cost of adding select NPM packages
- https://www.shellcheck.net/    # shell scripting syntax checker
- https://explainshell.com/      # check what individual CLI lines mean [help text]
- https://groovy-lang.org        # official site for learning the Groovy programming language


# Training
- boson.com - highly recommended by NetworkChuck [YT] for CCNA, CCNP, etc. certification
- https://www.youtube.com/watch?v=uEVmD6n8Il0 - NodeJS deployment strategies [Fireship]



-----
Regex
-----

^[0-9]{3}-[0-9]{3}-[0-9]{4}$     # match any phone number
^[0-9]{3}-?[0-9]{3}-?[0-9]{4}$   # match three numbers before first dash, matching second and third sets are optional 
^[a-zA-Z0-9._]+@[a-zA-Z0-9]+\.[a-zA-Z]{2,3}$	 # one or more of the first set, followed by @, followed by one or more letters or numbers
^[\w\d._]+@[\w\d]+\.[\w]{2,3}$   # alternative to above, will match foreign letters and numbers as well because of \w and \d

Legend:

	Assertions/Quantifiers
		$		matches end of line
		^		matches beginning of line
		*		0 or more
		?		0 or 1
		+		1 or more
		{n}		exactly n
		{n,}	n or more
		{n,m}	between n and m


	Characters
		.		any exxcept newline
		[abc]	a, b, or c
		[a-z]	a, b, ...., z
		[^abc]	anything but a, b, c
		\d      digit
		\s      status
		\w      word character


	Special
		\n      newline
		\t      tab



-----
redis
-----

- stands for 'Remote Dictionary Server' - an in-memory multi-model database
- RAM-based processing - data processing happens on RAM, but also stored on disk in case of restructuring/rebuilding needed
- key/value store - storing data in the form of key/value (different types include string, bitmap, hash, list, set, stream)
- secondary database - can be used as cache support for relational DBs
- **primary DB** - can also be the primary database for even a large scale application
- plugins - can install add-on module plugins to help with structuring, searching, etc.
- cli - has its own command line (naturally...)




---
RPM
---

- RPM install
   - set response - RESP_FILE=<app>-environment.response; export RESP_FILE
   - intialize rpmdb - rpm --initdb --dbpath /path/to/rpm_db
   - install - rpm --dbpath /path/to/rpm_db --prefix /install_path/ --nodeps -Uvh /path/to/rpm/ebm-wca-1.0.0-1.x86_64.rpm
   - query package - rpm -qi <rpm_name> --dbpath /path/to/rpm_db
   - check pkgmap - rpm -V <rpm_name> --dbpath /path/to/rpm_db
   - uninstall rpm - rpm -ev pkg_name --dbpath /path/to/rpm_db



-------
secrets
-------

# General levels of keeping secrets
# Source: DevOps Directive [Sid Palas] - https://www.youtube.com/watch?v=7NTFZoDpzbQ

1) Hardcode directly in the code

2) Create a config file, but still stored in plaintext
	- generally ok to check into source if the secrets are for dev envs 

3) Encrypt the config file
	- can use openssl to encrypt the config

		openssl aes-256-cbc -d -a -salt -in secrets.env.enc -out secrets.env -pass pass:Where-am-I-supposed-to-store-this?!
		cat secrets.env
		
		# eg. file contents
		DB_PASS=somethingsomething


4) Use a dedicated secret manager
	- eg. in code, secret manager is called and variable referenced


5) Ephemeral or temporary credentials (eg. using Hashicorp vault)
	- rotates/renews credentials automatically, creds have an enforced expiry



# Apply secret to a K8s namespace from a file

kubectl apply -f secret.yaml


# Apply secret using K8s native CLI
	
	# passing the string directly
	
	kubectl create secret generic my-secret \
		--from-literal=APU_TOKEN=password123

	# pass from a file
	
	kubectl create secret generic my-secret \
		--from-file=cert=/path/to/cert/file`


# secret yaml example
apiVersion: v1
kind: Secret
metadata:
	name: my-secret
data:
	API_TOKEN: c4FDca23zdfA4A=


# Use secret in app (from file) - sub-section of yaml

spec:
  containers:
..
..
..
     env:
       - name: API_TOKEN
         valueFrom:
         	secretKeyRef:
         		name: my-secret
         		key: API_TOKEN


# Use secret from a volume

spec:
  containers:
  ..
  ..
  ..
     volumeMounts:
       - name: secret-volume
       - mountPath: /etc/secret-volume
  volumes:
    - name: secret-volume
      secret:
      	secretName: my-secret



# Creating a generic secret in Kubernetes
# Source: Just Me And Opensource (Venkat) - https://www.youtube.com/watch?v=CK87fP2_tDs

- three types of secrets: docker-registy, generic, or TLS

kubectl create secret generic mysecret --from-literal=username=venkat --from-literal=password=hello

kubectl get secrets                   # see all secrets in cluster

kubectl describe secret mysecret      # see info and contents of mysecret

kubectl edit secret mysecret          # Edit a secret 


# Example of secret yaml in edit mode, where you can edit user or password or both, unless you put the immutable flag as shown below
# You will get error: secrets "mysecret" is invalid, if you try to save an edited secret where the immutable flag is set

apiVersion: v1
data:
	password: aGVsbG8=
	username: dmVua2F0
kind: Secret
immutable: true
metadata:
	creationTimestamp: "2021-05-09T17:17:01Z"
	name: mysecret
	namespace: default
	resourceVersion: "2264"
	uid: c98b0109-c095-4a3d-97f3-dd8d41057752
type: Opaque


kubectl delete secret mysecret     # deletes secret - the only way to "edit" an immutable secret


---
sed
---

# Simple search and replace
sed -i -e 's/few/asd/g' hello.txt


----
sftp
----

# SFTP debug:
sftp -vv account@server




---------------
Shell scripting
---------------

read -ps    # -p is for prompt, -s is for secret text



-----
shipa
-----

Video Source: Making Kubernetes disappear with Shipa - https://www.youtube.com/watch?v=PW44JaAlI_8



------------------------------------------------------
ssh public/private key generation for passwordless SSH
------------------------------------------------------

1) on the client side, create an ssh public/private key pair:
	> ssh-keygen -t rsa -b 4096
	# do not enter a passphrase, just hit enter

2) Copy the contents of id_rsa.pub and ssh to the target server, cd to the users .ssh folder, edit authorized_keys, and paste the pub key entry into the file and save. You should now be able to ssh from the client to the server without a password.

3) You can also try updating authorized_keys from the client like so:
	> ssh-copy-id -i ~/.ssh/id_rsa.pub user@host
	

# SSH tunneling - requires SSH server to be set up on target machine
# source: https://www.youtube.com/watch?v=AtuAdk4MwWw&list=WL&index=9

- tunnel from local machine to a remote machine
ssh -L <port_to_expose>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>


- create SOCKS proxy
ssh -D <any_port> <dest_user>@<destination_SSH_server_IP>

	# after running the above, you can for example set this in your LAN settings in your browser, enter the details in the SOCKS proxy section, and any site you visit will be forwarded through the
	# SSH tunnel to the destination specified


- Set up a SSH tunnel from the target server for a "local" PC or server to access
ssh -R <any_port>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>

	# enter the remote server's IP and any_port in your local browser via remote desktop and you'll be able to remote into the target machine



-------
Storage
-------

# Block vs File Storage: very traditional vs. the newer storage types

Block
	- accessed through SAN (storage area network)
	- lowest possibly latency
	- high performing
	- highly redundant


File
	- accessed via NAS (network area storage - all servers connecting to one place vs. san routing you to the right storage device)
	- highly scalable
	- accessible to multiple runtimes
	- simultaneous read/writes from multiple users


If you need:
	- boot volume - use block storage
	- lowest latency - block storage
	- mix of structured and unstructured data - file storage
	- share data with many users at once - file storage



# NAS vs SAN

NAS - Network Attached Storage
		- store data in a centralized location accessible to all devices
		- just stores data, thats it
		- will have multiple hard drives in a RAID configuration
		- will have NIC to connect to router so its network accessible, accessed as a shared drive
		- small to medium scalability
		- disadvantage:
			- single point of failures


SAN - Storage Area Network
		- special high-speed network, stores and provides access to large amounts of data (dedicated for data storage)
		- multiple disk arrays, switches, and servers
		- advantages:
			- shared, therefore fault tolerant
			- recognized as a single drive from an OS
			- interconnected with fiber channel (from 2gbps to 128gbps - ultra fast)
			- not affected by network traffic
			- highly scalable
			- very redundant
		- disadvantage
			- fiber channel expensive (alternative iSCSI, cheaper but not as fast)
			- very expensive in general


-------
systemd
-------

# Basic setup
# Source: Engineer Man - https://www.youtube.com/watch?v=unIAGt5pB7A&list=WL&index=3

- organizes programs in units, uses unit files to manage their state
- systemd basically auto-manages programs you define for you by auto starting them when they go down, when the system goes down

1) Create a unit file [e.g. somename.service] to define your service:

# File contents

[Unit]
Description=SomeReallyImportantService

[Service]
Type=simple
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]
WantedBy=multi-user.target


# Or can use more complex file contents below

[Unit]
Description=SomeReallyImportantService
Wants=something.service dependedon.service
Requires=dependedon.service
After=something.service

[Service]
Type=simple
User=root
Group=root
Environment=SOMEVAR=someval
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]     ############################################## need to learn more on this section ###################################################
WantedBy=multi-user.target


# Above sections
- Description - describes the service [info]
- Wants - also starts services listed other than this service
- Requires - like Wants, except if a service listed fails to start, this service will also fail to start
- After - starts listed service[s] first before starting this service

- Type - defines how the process starts up     # different types defined here: https://www.freedesktop.org/software/systemd/man/systemd.service.html#
- User and Group - start service as specified user and group
- Environment - add variables to inject into start up
- WorkingDirectory - for temp or generated files
- ExecStart - the path to the script/program to run as part of this service
- TimeoutSec - up to specified number of seconds to start
- Restart - defines restart policy [e.g. always]
- RestartSec - how long to wait before restarting, if service goes down


2) Save the file in /etc/systemd/system/


3) See below the different commands to manage the service

systemctl start somename 
systemctl stop somename
systemctl enable somename 							
systemctl disable somename 				# If service is disabled, service will not start upon system boot/reboot
systemctl list unit-files         # Lists all services and their status'. See following link for possible statuses: https://www.freedesktop.org/software/systemd/man/systemctl.html



# Advanced configuration of systemd
# see also : https://www.freedesktop.org/software/systemd/man/systemd.unit.html

- multi-user.target - if service is enabled, tells systemd at what run level should this service start [above example corresponds to run level "2"]


# Rough Guide on Run Levels - Linux Standard Base [LSB] 4.1.0 [see Wiki on "Runlevel"]
ID			Name																	Description
0				Off																		Turns off the device.
1				Single-user mode											Mode for administrative tasks.
2				Multi-user mode												Does not configure network interfaces and does not export networks services.
3				Multi-user mode with networking				Starts the system normally.
4				Not used/user-definable								For special purposes.
5				Full mode															Same as runlevel 3 + display manager.
6				Reboot																Reboots the device.



-----------------
terraform - azure
-----------------

Source: https://www.youtube.com/watch?v=bHjS4xqwc9A

1) Get an Azure subscription (can be free)

2) Authenticate with Azure (best to use Azure CLI - can download from Docker Hub)
	docker run -it --rm -v ${PWD}:/work -w /work --entrypoint /bin/bash mcr.microsoft.com/azure-cli:2.6.0
	az login
	export TENANT_ID=<your_tenant_id>

3) List subscription details and set
	az account list -o table
	SUBSCRIPTION=<id>
	az account set --subscription $SUBSCRIPTION


4) Create service principal

SERVICE_PRINCIPAL_JSON=$(az ad sp create-for-rbac --skip-assignment --name aks-getting-started-sp -o json)

SERVICE_PRINCIPAL=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.appID')
SERVICE_PRINCIPAL_SECRET=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.password')    # Note: keep appID and password for later use

	# where jq is a cli which is like sed for JSON data


5) Assign a role to the newly created service principal

az role assignment create --assignee $SERVICE_PRINCIPAL \
--scope "/subscriptions/$SUBSCRIPTION" \
--role Contributor


6) Install Terraform CLI

curl -o /tmp/terraform.zip -LO https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip
unzip /tmp/terraform.zip
chmod +x terraform && mv terraform /usr/local/bin/


7) Create terraform files called main.tf and variables.tf

# main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}



8) Initialize terraform

terraform init     	# will create a .terraform folder, will have all the downloaded plugins, metadata, etc.


9) Create terraform plan with the files created in previous step

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION

# terraform will keep an in-memory record of the state of the plan that has been applied


10) Create a terraform module for your AKS cluster, with its own variables.tf file

mkdir -p <dir>/.terraform/modules/cluster


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


11) Create a cluster.tf file and define a resource group and a cluster definition

# cluster.tf file contents

resource "azurerm_resource_group" "aks-getting-started" {
	name 		 = "ask-getting-started"
	location = var.location
}

resource "azurerm_kubernetes_cluster" "aks-getting-started" {
	name 									= "aks-getting-started"
	location 							= azurerm_resource_group.aks-getting-started.location
	resource_group_name		= azurerm_resource_group.aks-getting-started.name
	dns_prefix						= "aks-getting-started"
	kubernetes_version		= var.kubernetes_version


  default_node_pool {
  	name 			    	= "default"
  	node_count    	= 1
  	vm_size					= "Standard_E4s_v3"
  	type  					= "VirtualMachineScaleSets"
  	os_disk_size_gb = 250
  }

  service_principal {
  	client_id = var.serviceprinciple_id
  	client_secret = var.serviceprinciple_key
  }

  linux_profile {
  	admin_username = "azureuser"
  	ssh_key {
  			key_data = var.ssh_key
  	}
  }

  network_profile {
  		network_plugin = "kubenet"
  		load_balancer_sku = "Standard"
  }

  addon_profile {
  	aci_connector_linux {
  		enabled = false
  	}

  	azure_policy {
  		enabled = false
  	}

  	http_application_routing {
  		enabled = false
  	}

  	kube_dashboard {
  		enabled = false
  	}

  	oms_agent {
  		enabled = false
  	}
  }
}

# see azure provider page and azurerm kubernetes cluster page on the terraform website for more details


12) Generate an SSH key for access to the cluster

ssh-keygen -t rsa -b 4096 -N "VeryStrongSecret123!" -C "your_email@example.com"
export SSH_KEY=$(cat ~/.ssh/id_rsa.pub)


13) Update the variables.tf sitting on the same level as the main.tf file (outside of the cluster folder) with the below

# Updated variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}

variable "ssh_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


14) Re-apply updated terraform plan

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


15) Update main.tf with the cluster module information

# Updated main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}

module "cluster" {
	source 								= "./modules/cluster/"
	serviceprinciple_id		= var.serviceprinciple_id
	serviceprinciple_key  = var.serviceprinciple_key
	ssh_key								= var.ssh_key
	location							= var.location
	kubernetes_version		= var.kubernetes_version
}


16) Re-initialize terraform (confirm if previous plan needs to somehow be wiped first?)

terraform init


17) Re-apply updated terraform plan (this will create 2 plans, 1 for the cluster, 1 for the resource group)

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


18) Add a new resource to demonstrate terraforms capability of updating infrastructure changes via code

# Add to above cluster.tf contents

resource "azurerm_kubernetes_cluster_node_pool" "monitoring" {
	name 										= "monitoring"
	kubernetes_cluster_id 	= azurerm_kubernetes_cluster.aks-getting-started.id
	vm_size									= "Standard_DS2_v2"
	node_count							= 1
	os_disk_size_gb					= 250
	os_type									= "Linux"
}


19) Update cluster.tf values, such as change node_count to 2 in default_node_pool

20) Re-run the same terraform apply command as in step 17 (you will be prompted to say yes or no to apply the changes)

21) Create a new k8s module (create k8s folder under modules first) and a corresponding variables.tf 

# k8s.tf file contents

provider "kubernetes" {
	load_config_file				= "false"
	host 										= var.host
	client_certificate 			= var.client_certificate
	client_key							= var.client.key
	cluster_ca_certificate  = var.cluster_ca_certificate
}


# variables.tf file contents

variable "host" {
}

variable client_certificate {
}

variable client_key {
}

variable cluster_ca_certificate {
}


22) Define a k8s deployment in the k8s.tf file

# Snippet example to put under the provider "kubernetes" section in the file

resource "kubernetes_deployment" "example" {
	metadata {
		name = "terraform-example"
		labels = {
			test = "MyExampleApp"
		}
	}

  ...
  ...
  ...

	}


23) Add a service definition to the k8s.tf file to expose the deployment in step 22

# Snippet example to put under kubernetes_deployment resource section

resource "kubernetes_service" "example" {
	metadata {
		name = "terraform-example"
	}
	spec {
		selector = {
			test = "MyExampleApp"
		}
		port {
			port         = 80
			target_port  = 80
		}

		type - "LoadBalancer"
	}
}


24) Update main.tf with module information required by the new k8s module

# Add to above main.tf file contents

module "k8s" {
	source                 = "./modules/k8s/"
	host                   = "${module.cluster.host}"
	client_certificate     = "${base64decode(module.cluster.client_certificate)}"
	client_key				     = "${base64decode(module.cluster.client_key)}"
	cluster_ca_certificate = "${base64decode(module.cluster.cluster_ca_certificate)}"
}


25) In the cluster module, create a new file called outputs.tf (which terraform uses to grab values from to match with variables we defined. In this case we spit out the contents of kube_config)

# outputs.tf file contents

output "kube_config" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config_raw
}

output "cluster_ca_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.cluster_ca_certificate
}

output "client_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_certificate
}

output "client_key" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_key
}

output "host" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.host
}


26) Go to cluster.tf and remove the monitoring node pool and update default_node_pool value back to 1

27) Re-apply the terraform plan - this will:
	- add 2 plans (the kubernetes deployment and service)
	- update 1 plan (the default_node_pool resource)
	- destroy 1 plan (the monitoring node pool resource)

Note: add a -out param to the terraform apply command to save a plan that you are applying


28) Get cluster credentials to put into kubeconfig

az aks get-credentials -n aks-getting-started -g aks-getting-started


29) Use kubectl to verify what youve set up



----------
traceroute
----------

- definition: a command-line utility used to show the exact route that is taken by data packets as they travel across the internet to their desiination
	- helps to find bottlenecks in routes

- vs. ping - tells you general connnectivity to destination and amount of time. traceroute shows more info.

- eg. of command usage

	tracert google.com      # Windows cmd (also available in Linux) to show trace to google.com

		- sends 3 data packets to each router on its way to the destination
			- helps isolate false issues [instead of sending just 1 packet]
		- router sends back the 3 data packets with info about that router:
				- number of hops to router
				- time/duration of round trip
						- increases with each hop since each hop is further and further, towards destination
						- network slowness: if theres a big difference between hops, could indicate a problem or the distance between two particular hops/routers is very large
						- request timed out: indicates problem or
							- if the trace still goes all the way through and request timed out happens at one hop/router, it could be because that router wasnt configured to return info back to the source
				- IP of router

- time to live [TTL] - the max number of hops a trace will do [can be modified]
										 - prevents data packets from traveling endlessly around the internet
										 	- e.g. misconfigured routers tied together in a loop that receive a trace with no TTL might endlessly pass data packets and slow down a network

	tracert -h 4 google.com    # set max hops to 4 - if destination is further than 4 hops, destination will not be reached

- Linux/MacOS CLI equivalent is "traceroute"



---------------
troubleshooting
---------------

***Kubernetes deployments - general approach***

1. Deployments
2. Pods
For both check:
	a) status
	b) events

- Commands
	kubectl get deploy
	kubectl logs <pod_name>
	kubectl describe deploy <deployment_name>
- failure examples:
	- pod not able to pull image (e.g. its missing)
	- liveliness probe is failing (e.g. the URL doesnt exist)
- status examples and possible causes:			
	- status: ContainerCreating
		- could be stuck because config map missing
	- status: CrashLoopBack
		- pod attempts to continually restart but cannot, usually an app or image issue.
	- status: Pending
		- insufficient memory
		- image may not run (e.g. try docker run on it outside a pod)
	- pods running, but not reachable
		- check selector label in service def matches app selector label
			- kubectl get svc
	- connection refused
		- endpoint may have wrong targetPort (check yaml for port and targetPort)



***Generating Thread Dumps for troubleshooting performance issues***

1.	Identify the process ID (pid) of the tomcat process running JIRA by executing the following command: 
2.	ps aux | grep jira
3.	Execute the following command (be sure to replace both occurrences of $JIRA_PID with the pid you identified in the previous step): 
4.	for i in $(seq 6); do top -b -H -p $JIRA_PID -n 1 > jira_cpu_usage.`date +%s`.txt; kill -3 $JIRA_PID; sleep 10; done
5.	That script will run for one minute, during which it will generate top output of the process threads to six text files in the current directory.
6.	Attach the files generated by the script to this issue.
7.	Generate and send a complete support zip.



***Performance issues in a web app***

- in Chrome or whichever browser, open the console and check the performance tab, see if memory usage and see if it keeps climbing
- go to memory tab and take a snapshot of the memory values of each page or call



-------
vagrant
-------

- open-source software product for building and maintaining portable virtual software development environments
	- grouped under "Virtual Machine Management"


# Vagrantfile example for provisioning three virtual machines for a K8s cluster

# START VAGRANTFILE
# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV['VAGRANT_NO_PARALLEL'] = 'yes'

Vagrant.configure(2) do |config|

  config.vm.provision "shell", path: "bootstrap.sh"

  # Kubernetes Master Server
  config.vm.define "kmaster" do |node|

    node.vm.box               = "generic/ubuntu2004"
    node.vm.box_check_update  = false
    node.vm.box_version       = "3.3.0"
    node.vm.hostname          = "kmaster.example.com"

    node.vm.network "private_network", ip: "172.16.16.100"

    node.vm.provider :virtualbox do |v|
      v.name    = "kmaster"
      v.memory  = 2048
      v.cpus    =  2
    end

    node.vm.provider :libvirt do |v|
      v.memory  = 2048
      v.nested  = true
      v.cpus    = 2
    end

    node.vm.provision "shell", path: "bootstrap_kmaster.sh"

  end


  # Kubernetes Worker Nodes
  NodeCount = 2

  (1..NodeCount).each do |i|

    config.vm.define "kworker#{i}" do |node|

      node.vm.box               = "generic/ubuntu2004"
      node.vm.box_check_update  = false
      node.vm.box_version       = "3.3.0"
      node.vm.hostname          = "kworker#{i}.example.com"

      node.vm.network "private_network", ip: "172.16.16.10#{i}"

      node.vm.provider :virtualbox do |v|
        v.name    = "kworker#{i}"
        v.memory  = 1024
        v.cpus    = 1
      end

      node.vm.provision "shell", path: "bootstrap_kworker.sh"

    end

  end

end

# END VAGRANTFILE


--------
webhooks
--------

Source: https://www.youtube.com/watch?v=rUaDIH5ZXB8&list=WL&index=2

Definition: http messages that are sent in response to an event to a third party service (e.g. you can configure a webhook in Github to send messages to Jenkins based on push events to a given repository)



-----
whois
-----

whois [domain_name]   # will return comprehensive information about that URL/domain (given that domain is publicly registered)