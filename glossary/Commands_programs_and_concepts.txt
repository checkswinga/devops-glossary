--------
802.11ax
--------

# Sources:
# Powercert Animated Videos: Wi-Fi 6 explained: https://www.youtube.com/watch?v=Y7OWUg_kmK4

#### General ####
- a.k.a. wi-fi 6
- released in 2019
- latest wireless standard used in wireless devices
- successor to 802.11ac wireless standard (wi-fi 5)
- is faster than wifi 5
- main goal: make a wireless network perform better with multiple devices connecting to it


#### Differences between Wi-fi 5 and 6 ####

- Wi-Fi 5 vs. Wi-Fi 6

	- speed: 3.5Gbps vs. 9.6Gbps (shared across multiple devices)
		- "Orthogonal Frequency-Division Multiple Access" (OFDMA) - lowers latency and delivers data to multiple devices more efficiently
		- "Multiple-user, multiple-input, multiple-output" (MU-MIMO) - a technology that allows multiple wireless devices to communicate w/ a wi-fi router at the same time
			- breaks up bandwidth into individual streams and pushes it to the connected devices
			- debuted in Wi-Fi 5 version 2, improved in Wi-Fi 6
				- in Wi-Fi 5 version 2, MU-MIMO was
					- only available in download (as opposed to both download and upload)
					- can only support 4 simultaneous streams (as opposed to 12)
			- previous technology was "Single-user, multiple-input, multiple-output" (SU-MIMO) - one device at a time

	- beamforming: a technique that sends transmissions in a specific direction, which strengthens the signal
		- Wi-Fi 5 does not have beamforming

	- Wi-Fi 6 minimizes interference to nearby networks:
		- Basic Service Set Colouring (BSS) - colours or marks the networks so it can distinguish another network from its own 

	- security: WPA3 - provides cutting edge security protocols that enable a more robust authentication that will increase protection from password detection attempts
		- Wi-Fi 6 devices must be WPA3 certified

	- extended battery life:
		- Target Wake Time - a router or access point schedules a time with wi-fi devices on when data is supposed to be sent or received

	- new frequency: 6Ghz band
		- high performance
		- low latency
		- less interference
		- greater capacity



--
AI
--

#### AI vs. Machine Learning ####

# Source: IBM Cloud - https://www.youtube.com/watch?v=4RixMPF4xis

- AI
	- simple definition - AI is basically exceeding or matching the capabilities/intelligence of a human
		- ability to discover, find out new information
		- ability to infer, to read in information from other sources that maybe has not been explicitly stated
		- ability to reason, to figure things out

	- mimicking human abilities
		- natural language processing (NLP)
		- vision - a system thats able to see/hear
		- text to speech
		- motion

- ML
	- a subset of AI
	- involves predictions or decisions based on data
	- sophisiticated form of statistical analysis
	- the more we feed into the system, the more its able to give us accurate predictions and decisions based on data

	- supervised machine learning
		- more human oversight
		- looking at training of the data
		- use labels superimposed on the data

	- unsupervised machine learning
		- more able to run and find things not explicitly stated

- DL (deep learning)
	- a subset of ML
	- neural networks - nodes, statistical relationships between nodes to model the way our minds work
	- multiple layers of neural networks to make it "deep"




---
alt
---

#### General ####

- info section on things related to the Alt key


#### References ####

- Alt Codes cheat sheet - https://www.posterpresentations.com/alt-code-cheatsheet.html





-------
anacron
-------

- Linux/UNIX - works like cron, except if scheduled time is reached and system is not up to run the job, anacron will trigger as soon
			   as the system is up and running.
			 - equivalency:
			 	- Autosys job on hold - anacron
			 	- Autosys job on ice - cron


#### Commands ####

cat /etc/anacrontab             # see anacrontab commands and timings configured




#### Files and directories ####

/var/spool/anacron 				# contains its own set of daily, weekly, monthly crontab files
	- cron.daily
	- cron.monthly
	- cron.weekly

grep START /etc/anacrontab  	# view start timings allowed for all jobs regardless of their configuration (START_HOURS_RANGE env variable)

grep RANDOM /etc/anacrontab   	# view random delay setting for anacrontab for all jobs (used to prevent overload if anacrontab has several
								  jobs configured to run at the same time)




---
and
---

- Linux - used to combine execution of commands together. The execution of the subsequent command depends on the success of the first


#### Example ####

command 1 && command 2       # execute command 2 if command 1 succeeds
							 # unlike "command 1; command2", which will execute command2 regardless of command1 failure]








-------
Android
-------

# Starting out with an Android project [from a DevOps point of view]
# Source: Engineer Man - https://www.youtube.com/watch?v=R3eAMMBh2ng

1) Download Android Studio from the developer.android.com site and follow the instructions to install on your computer or server

2) Open Android Studio and in "Select A Project Template", choose whatever you like [e.g. Empty Activity]

3) In "Configure Your Project", youll need to configure the following:

	- Name: My Application
	- Package Name: com.hq.myapplication       # can leave as default
	- Save location: <whereever_you_like>
	- Language: Java or Kotlin
	- Minimum SDK: recommendation is Android 5.0 [Lollipop] or 6.0 [Marshmallow]       # going too low might make for problems and extra work down the road

- Side note: - designing layouts in Android [you will see the options when in Android studio]
	- purely with code
	- split between code and visual preview
	- purely drag and drop 


4) Explanations on different files and folders created:

- AndroidManifest.xml [under app/manifests]
	- metadata about the application
		- in the application block, see things like the text label, icons, activity block [screen that shows up on Android or a screen that might be switched to]


- java folder
	- contains all the java code for your application
	- other folders: one for main code, one for tests
	- example code: MainActivity.java under com.hq.myapplication


- java [generated] folder
	- never need to directly be concerned with it, though code gets generated here because of content or code you create elsewhere [such as a content xml in the layout folder]


- res folder [resources]
	- contains everything except java code itself

	- sub-folders
		- drawable folder
			- contains vector assets and images
		- layout folder
			- for layouts [e.g. menus, list items, etc.]
			- eg. activity_main.xml - eg. content xml file referenced by the MainActivity.java file


- mipmap folder
	- contains several variations of the same image [for different phone sizes]
	- in code, reference by the same name, and the phone figures out which image to use


- values folder
	- stores various constant values of different things such as colours and strings [e.g. color name="purple_200" matched to a hex value]
	- file: strings.xml - used in case app is gonna be in different languages


- themes folder
	- application wide theme [eg. light and dark themes to apply, phone can toggle depending on user choice]


- Gradle scripts
	- for building the application
		- build.gradle - has the build instructions


5) To test run your app, plug your phone into your computer and click the Play button in Android Studio



-------
Ansible
-------

#### How to automate your Mac setup ####

# Source: https://github.com/geerlingguy/mac-dev-playbook



-----------------
Apache Web Server
-----------------

# Install and run on Rocky Linux
# Source: https://docs.rockylinux.org/guides/web/apache-sites-enabled/
# Note: More of a starter guide

dnf install -y httpd php           # install httpd and php packages
								   # note: php-bcmath and php-mysqlind may be required
mkdir /etc/httpd/sites-available
mkdir /etc/httpd/sites-enabled     # create directories to keep track of sites available and enabled for this server instance

cd /etc/httpd/sites-available      
vi /etc/httpd/sites-available/com.site.www      # Create configuration for a site

# File content
<VirtualHost *:80>
        ServerName your-server-hostname
        ServerAdmin username@rockylinux.org
        Redirect / https://your-server-hostname/
</VirtualHost>
<Virtual Host *:443>
        ServerName your-server-hostname
        ServerAdmin username@rockylinux.org
        DocumentRoot /var/www/sub-domains/your-server-hostname/html
        DirectoryIndex index.php index.htm index.html
        Alias /icons/ /var/www/icons/
        # ScriptAlias /cgi-bin/ /var/www/sub-domains/your-server-hostname/cgi-bin/

    CustomLog "/var/log/httpd/your-server-hostname-access_log" combined
    ErrorLog  "/var/log/httpd/your-server-hostname-error_log"

        SSLEngine on
        SSLProtocol all -SSLv2 -SSLv3 -TLSv1
        SSLHonorCipherOrder on
        SSLCipherSuite EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384
:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

        SSLCertificateFile /var/www/sub-domains/your-server-hostname/ssl/ssl.crt/com.wiki.www.crt
        SSLCertificateKeyFile /var/www/sub-domains/your-server-hostname/ssl/ssl.key/com.wiki.www.key
        SSLCertificateChainFile /var/www/sub-domains/your-server-hostname/ssl/ssl.crt/your_providers_intermediate_certificate.crt

        <Directory /var/www/sub-domains/your-server-hostname/html>
                Options -ExecCGI -Indexes
                AllowOverride None

                Order deny,allow
                Deny from all
                Allow from all

                Satisfy all
        </Directory>
</VirtualHost>
### END CONTENT ###

- Notes on above
	- SSLEngine on - simply says to use SSL
	- SSLProtocol all -SSLv2 -SSLv3 -TLSv1 - says to use all available protocols, except those that have been found to have vulnerabilities. You should research periodically which protocols are currently acceptable for use.
	- SSLHonorCipherOrder on - this deals with the next line that regarding the cipher suites, and says to deal with them in the order that they are given. This is another area where you should review the cipher suites that you want to include periodically
	- SSLCertificateFile - is exactly what it sounds like, the newly purchased and applied certificate file and its location
	- SSLCertificateKeyFile - the key you generated when creating your certificate signing request
	- SSLCertificateChainFile - the certificate from your certificate provider, often referred to as the intermediate certificate.

mkdir -p /var/www/sub-domains/your-server-hostname/html      # Create directory where the site files/content will be stored

cp /root/com.site.www.key /var/www/sub-domains/your-server-hostname/ssl/ssl.key/       # Copy key and ssl certs into ssl dir
cp /root/com.site.www.csr /var/www/sub-domains/your-server-hostname/ssl/ssl.csr/
cp /root/com.site.www.crt /var/www/sub-domains/your-server-hostname/ssl/ssl.crt/


ln -s /etc/httpd/sites-available/your-server-hostname /etc/httpd/sites-enabled/        # soft link to enable the site

systemctl start httpd             # start server



---
apt
---

- Linux Ubuntu''s package manager - various other commands are used under apt that can be used to install packages, check details of installed packages, etc.

apt install [package]          # install package to OS (along with its dependencies). E.g. apt install apache

apt-get install [package]      # install package to OS (along with its dependencies)
apt-get remove [package]       # remove installed package from OS
apt-get autoremove [package]   # remove installed package's dependencies

apt-cache show [package]       # show package details

dpkg -S [package]              # find package details, location of installed package files




--
at
--

- Linux/UNIX - used to schedule a job for later execution (executes ONCE as opposed to cron)


#### Commands ####

yum install at           # or dnf install at, to install at package
at 7:15AM tomorrow       # bring up at command line prompt to enter command to run at 7:15AM tomorrow morning
	at> command1
	at> command2
	at> command3 		 # enter in a command and hit enter to put in next command, as many as you want

at -l                    # see how many jobs are in the at queue
at -d 2 				 # delete job #2 in the at queue
atq 					 # similar to at -l
atrm 3 					 # delete job #3 in at queue (similar to at -d)


#### Files ####

/etc/at.allow 			 # configures list of users to allow access to run and configure at commands
/etc/at.deny  			 # configures list of users to deny access for to run and configure at commands




---
AWS
---

#### Top 50 List of Services ####
# Source: Fireship - https://www.youtube.com/watch?v=JIbIYCM48to&pp=sAQA

# Robots and Large Scale
- Robomaker - simulate and test your robots at scale (e.g. robot vacuums)
- IOT core - collect data from those robots, update their software, manage them remotely
- Ground Station - control satellite communications, process data and scale operations from satellites orbiting Earth
- Bracket - software used to interact with a quantum computer (i.e. to learn about the future of computing)

# Compute
- EC2 (Elastic Compute Cloud) - create a virtual computer in the cloud (OS, memory, computing power), for rent
- elastic load balancing - distribute traffic to multiple instances automatically
- Cloud Watch - collect logs and metrics from each individual instance
- Auto Scaling - create or scale down instances based on the metrics collected from Cloud Watch
- Elastic Beanstalk (PaaS) - interface for easily deploying code to AWS (including scaling, load balancing, etc.)
- Lightsail - even easier deployment tool for AWS, don[t have to worry at all about the underlying infrastructure
- Lambda (FaaS) - serverless, upload your code, decide when it should run (traffic, scaling, networking all work in background). Pay only when the app is used.
- Serverless Repo - if you dont like writing your own code, find pre-built functions to deploy with a click
- Outposts - Run AWS APIs on your own infrastructure (without tossing out your old servers)
- Snow - mini data centers that work without internet in hostile envs (e.g. the Arctic)

# Containers and Container Orchestration
- Container Registry - upload/store container images
- Elastic Container Service - run a container stored in the registry (Elastic Container Service (ECS) is the service for stopping, starting and allocating virtual machines for your containers
															and connect them to other products such as load balancers)
- EKS - Amazons Kubernetes service
- Fargate - make your containers behave like serverless functions (automatic resources)
- App Runner (2021) - deployment tool for AWS (choose your images to run, it takes care of running in AWS, including resource allocation)

# File Storage
- S3 (Simple Storage Service) - store any type of file or object
- Glacier - higher latency, lower cost storage (more for archiving, when access rate is low)
- Elastic Block Storage - fass, handle bigger throughput (more configuration required)
- Elastic File System - high performance, fully manager, much higher cost

# Database
- Simple DB - general purpose no SQL database
- Dynamo DB - document database easy to scale horizontally (cheap, scales easy, fast, but no joins and limited queries, not good at modelling relational data)
- Document DB - controversial: not mongoDB, but exactly like it to get around licensing
- Elastic Search - good for search
- Amazon RDS (Relational Database Service) - supports a variety of SQL flavours, fully manage backups, scale and caching
- Aurora - Amazons own proprietary version of SQL (compatible with Postgres/mySQL, better performance at a lower cost, easy to scale with new serverless option, only pay when in use)
- Neptune - graph database, high performance on highly connected data sets (e.g. social graph, recommendation engine)
- Elastic Cache - ultra fast database, fully managed version of Redis (in=memory database)
- Timestream - time series database
- Quantum Ledger - cryptographically signed transactions

# Analyitcs
- Redshift - data warehouse (shift away from Oracle)
- Lake Formation - tool for creating data lakes or repos that store a large amount of unstructured data (can be used in addition to DWs to query a larger variety of data sources)
- Kinesis - capture real time streams, view the captured data in a business intelligence tool
- Apache Spark (Elastic Map Reduce) - stream data from multiple platforms to a business intelligence tool for analysis
- MSK - AWS fully managed service version of Apache Kafka
- Glue - Auto ETL (Extract, Transform, Load), can connect to different kinds of databases

# Machine Learning
- Data Exchange - purchase/exchange data from third party sources for analysis
- Sagemaker - connect to data exchange, build machine learning models
- Rekognition - identify/classify images
- Lex - chatbot
- Deep Racer - actual miniature race car driven remotely with machine learning code

# Developer Essentials
- IAM - identity and access management, roles
- Cognito - enable to login with different auth methods
- Simple Notification Service (SNS)
- Simple Email Services (SES)
- Cloud Formation - templates based on infrastructure in yaml/json format (I assume like Terraform/Ansible)
- Amplify - provide SDKs to connect to infrastructure from front end apps

# Bonus
- AWS cost explorer - budgeting

# Another bonus 
- App Engine Standard - for hosting NodeJS applications, easily scalable



-----
azure
-----

#### Service Health Alerts ####

# Source: John Savill's Technical Training - https://www.youtube.com/watch?v=KwQBBvyOblk

- https://github.com/checkswinga/devops-glossary/azure/snaps/azure_service_health_alerts
	- high level picture of service health alert setup, configurations, process for how to manage them





-------
backups
-------

Sources:
- PowerCert Animated Videos - https://www.youtube.com/watch?v=o-83E6levzM
- Jeff Geerling - https://www.youtube.com/watch?v=S0KZ5iXTkzg


#### General ####

- fault tolerance - the prevention of data loss if a component fails
- disaster recovery - the process of rebuilding an organizations data after a disaster


Comparison             Data thats backed up           Restore Procedure

- full                 All data (longest)		  	  Full backup only (fastest)

- incremental          Data thats been changed		  Full and incremetals (in the correct order) (longest)
											  		  since last full or incr.
											 		  backup (fastest)

- differential		   Data thats been changed		  Full and last differential (medium)
											 		  since the last full backup
											 		  (medium)


#### Example Strategy ####

- the "3-2-1 method"
	- 3 copies
	- 2 different media
	- 1 offsite

- Data Categories
	- photos
		- 1 on local
		- 2 on NAS
		- 3 on cloud

	- music
		- 1 on local 
		- 2 on NAS
		- 3 on cloud

	- videos
		- 1 on NAS
		- 2 on backup NAS
		- 3 on cloud (eg. AWS Glacier - cheap storage). Mostly for major emergencies only.

	- documents
		- 1 on local
		- 2 on NAS
		- 3 on dropbox

	- local files
		- 1 on local
		- 2 on NAS
		- 3 on ????
	
	- source code
		- 1 on local
		- 2 on NAS via Gickup (see https://github.com/cooperspencer/gickup)
		- 3 on Github


 #### Online tools to assist with above ####
 	- rclone - rclone.org
 	- arq - https://www.arqbackup.com
 	- restic - restic.net
 	- borg - borgbackup.readthedocs.io
 	- crashplan - https://www.crashplan.com



#### Aleem's current ####

- Data Categories
	- photos
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- music
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- videos
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- documents
		- 1 on local
		- 2 on 2nd local
		- 3 on OneDrive

	- local files
		- 1 on local
		- 2 on 2nd local
		- 3 on OneDrive
	
	- source code
		- 1 on local
		- 2 on Github
		- 3 ?????


------
base64
------

# Encrypt a plaintext password

echo -n "password123" | base64 -i -     # Output: cFSzf3skacQrIrR=




----
bash
----

- UNIX/Linux - a UNIX shell and command language (replacement for Bourne shell). Runs and processes commands as text when logged into a Linux machine/server via terminal

- order of execution upon bash login to a terminal: /etc/profile > ~/.bash_profile > ~/.bash_login > ~/.profile. 
	- the bash_profile executes the bashrc file.
	- if .bash_profile does not exist, .bash_login is read. If .bash_login does not exist, .profile is read instead.



#### .bash_logout ####

- if it doesnt exist, you can create it with the contents below:

#BEGIN
#!/bin/bash
# ~/.bash_logout: executed by bash(1) when login shell exits.
echo 'logout'; sleep 2s
if [ "$SHLVL" = 1 ]; then
history -c
cat /dev/null > ~/.bash_history
[ -x /usr/bin/clear_console ] && /usr/bin/clear_console -q
fi
You can use the vi ~/.bash_logout command to create the .bash_logout file.
#END



#### if ####

- flags and other stuff - https://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html


# Check if a directory does not exist

if [ ! -d "$DIR" ]; then
	# statements
fi




#### functions ####

# Example 1

1] Type the following in a bash shell and hit enter after each line:

function temp ()
{
echo "this is a temp function";
ls -l /etc;
}


2] Run the following:

temp

3] You should see the function execute the "echo" and "ls" statements




#### login order of execution ####

~/.bash_profile, ~/.bash_login, ~/.profile - however, if one script, let’s say ~/bash_profile is not found, the next one is executed.





#### profiles ####

/etc/profile           # shows contents of system level settings and attributes for any user's profile e.g. PS1 setting, default size of command history to display, umask settings, etc.
					   # Note: bash reads /etc/profile if invoked interactively using --login option, or invoked as sh.
					   # applies to all shells, including bash

/etc/bashrc        	   # contains system-wide definitions for shell functionns and aliases, similar to /etc/profile. Applies only to bash whereas profile applies to ALL shells
OR
/etc/bash.bashrc

~/.bash_profile   	  	# user configuration file in which the user environment can be configured. By default, some configuration is already defined, 
					    # but it can be changed or altered as per requirements.

~/.bash_logout - triggered when the user is logging off




#### PS1 settings ####

# Sources:
# Bash PS1 customization examples - https://linuxhint.com/bash-ps1-customization/


export PS1="[ \u@\d \t ] $ " 				# command prompt will show 'user@date+time $ '

export PS1="[\d]\n\u@\h: $ " 				# will display date first, and 'user@hostname: $ ' on the next line

export PS1="\e[45m\u@\h :\w$ \e[m" 			# change background colouring the prompt purple (see source for more colours)

export PS1="\e[0;37m\u@\h :\w$ \e[m" 		# change foreground colouring to white








#### scripts - light notes ####

bash ./some_script.sh     # one method used to execute a bash shell script
./some_script.sh          # another method to execute a bash shell script



#### shell commands ####

. ~/.bashrc 			  # source env variables from bashrc
source ~/.bashrc 		  # source env variables from bashrc

CTRL+r 					  # perform reverse search of previously entered commands (type in a few letters afterwards to see historical commands autocomplete)
![history_number] 		  # from "history" command, typing ! with the history number will run that command



---
bat
---

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg
# Github repo - https://github.com/sharkdp/bat

- Linux - alternative to cat
		- provides content-aware highlighting and line numbers
		- installation: download binary from Github repo



#### Commands ####

bat filename.js
bat -p filename.js 		# behave more like cat for file output
bat filename.yaml --paging=never  	   # print the whole file [like cat would do] instead of one pager [like less]



----
btop
----

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI

- Linux - monitoring tool similar to top and htop

	- benefits:
		- very pretty looking
		- also shows disk and network usage
		- very customizable


#### Commands ####

btop  		# can toggle the view with number keys




------
cables
------


# Ethernet Cables
Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=_NX99ad2FUA

- unshielded twisted pair
	- most common type of ethernet cable (used more in homes)
	- 4 pairs of colour-coded wires twisted around each other to prevent electromagnetic interference (crosstalk)

- shielded twisted pair
	- same as UTP except has a foil shield that covers the wires (more protection against interference)
	- both use RJ-45 connector

- types of twisted pairs
	- straight (patch) cable
		- if both ends of a cable are using the same standard
		- allows signals to pass through from end-to-end
		- to connect computers to dissimilar devices together (e.g. from a PC to a modem)
		- most commonly used
	- crossover cable
		- if both ends of a cable are using 2 different standards
		- to connect to similar devices together

- wiring standards
	- 568A
		- order of wires: white-green, green, white-orange, blue, white-blue, orange, white-brown, brown
	- 568B (more commonly used)
		- order of wires: white-orange, orange, white-green, blue, white-blue, green, white-brown, brown
	

- categories
	- the difference being the max speed they can handle without interference/crosstalk
	- the numbers represent the tightness of the twist that are applied to the wires
		- CAT3 - 10 Mbps - obsolete
		- CAT5 - 100 Mbps - obsolete
		- CAT5e - 1 Gbps - Enhanced
		- CAT6 - 1 Gbps  - 10 Gbps (cable length under 100 meters)
		- CAT6a - 10 Gbps - Augmented
		- CAT7 - 10 Gbps - Added shielding to the wires (shielded twisted pair version of CAT6a)
		- CAT8 - 40 Gbps - ultimate copper cable, shielded (4 times faster than CA6a/CAT7)


---
cal
---

- Linux/UNIX - displays calendar in ASCII format, of the given month and year


#### Commands ####

cal  			# prints current month and year [may also highlight the current day]
cal -3  		# shows the previous, current and next month
cal 2023 		# prints all months and days in 2023
cal 5 2014 		# prints month of May in 2014



------------
certificates
------------


#### keytool and openssl ####

- keytool - Java-based key and certificate management utility. It allows users to administer their own public/private key pairs 
			and associated certificates for use in self-authentication (where the user authenticates himself/herself to other users/services)
			or data integrity and authentication services, using digital signatures.
	 	  - It also allows users to cache the public keys (in the form of certificates) of their communicating peers.

- openssl - a robust, commercial-grade, full-featured toolkit for general-purpose cryptography and secure communication


# Download and import website certs into JAVA store (eg. google's certs for dl.google.com/android repo):

openssl s_client -connect google.com:443 -showcerts | openssl x509 -out certfile.txt
keytool -importcert -alias globalproxy -file certfile.txt -trustcacerts -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storetype JKS

OR 

keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts
keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security/cacerts


# check cacerts store

keytool -list -v -keystore /path/to/cacerts


# Convert CRT to PEM

openssl x509 -in mycert.crt -out mycert.pem -outform PEM


# view a JCE keystore

keytool -list -v -keystore something.keystore -storetype jceks


# Remove certificate entry from keystore - from https://docs.oracle.com/cd/E19798-01/821-1751/ghleq/index.html

keytool -delete -noprompt -alias ${cert.alias}  -keystore ${keystore.file} -storepass ${keystore.pass}


#### Change private key password for a secretKey entry within a keystore (JCE keystore in this case) ####
# Source: https://xacmlinfo.org/2014/05/26/how-to-keystore-changing-java-key-store-passwords/

keytool -keypasswd -alias [alias_name] -keystore something.keystore -storetype jceks 			


openssl x509 -inform der -in file.der -noout -text   					# view cert file in DER format
openssl x509 -enddate -noout -in file.pem 								# check the expiry on a cert in PEM format
cat /some/path/to/something.crt | openssl x509 -noout -enddate 			# check the expiry on a cert in CRT format

openssl x509 -pubkey -noout -in cert.cer  > pubkey.pem      			# extract public key from a cer

openssl x509 -inform der -in certificate.cer -out certificate.pem     	# convert cer/der/crt to pem format


# Download and import website certs into JAVA store:

openssl s_client -connect google.com:443 -showcerts | openssl x509 -out certfile.txt
keytool -importcert -alias globalproxy -file certfile.txt -trustcacerts -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storetype JKS


# Create a self-signed certificate

1] rpm -qa | grep -i openssl 							# check if openssl is installed

2] mkdir -p /etc/ssl/private/some_dir   				# create a dir to store the private key in

3] chmod -R 700 /etc/ssl/private/some_dir  				# lock down the permissions to the dir

4] cd /etc/ssl/private/some_dir  						# go to the directory 

5] openssl genrsa -des3 -out something.key 2048 		# generate private key

	- Enter passphrase:
	- Confirm passphrase: 

6] openssl rsa -in something.key -out something.key  			# remove passphrase from private key

	- Enter passphrase:



7] openssl req -new -days 3650 -key something.key -out something.csr  	# generate certificate signing request (CSR)

	- Country Name:
	- State/Province:
	- City:
	- Org Name:
	- Org Unit Name:
	- Common Name: 
	- Email Address:
	- Challenge password: 
	- Optional company name: 


8] openssl x509 -in plab.csr -out something.crt -req -signkey something.key -days 3650  	# generate certificate file (CRT)


#### Resources ####

- Certificate transparency - https://certificate.transparency.dev/
	- Explains how CT logging works and why we use it.

- Online certificate search and log tracker tool - https://crt.sh/

- PKIX progress tracker - https://datatracker.ietf.org/wg/pkix/about/
	- Describes the PKI X initiative with a progress tracker for different items.

- Bad certificate errors glossary - https://badssl.com [do not navigate to internally]




# Update cert store on Rocky Linux

1] Copy PEM into /etc/pki/ca-trust/source/anchors/

2] Run: sudo update-ca-trust




-------------
chattr/lsattr
-------------

- Linux/UNIX - change [and list] the attributes of a file in a directory


#### Sources ####

- full list of attributes meanings - https://man7.org/linux/man-pages/man1/chattr.1.html



#### Commands ####

chattr +i file.txt   				# make file immutable, even from root user (e.g. cannot delete)
lsattr file.txt  					# list attributes for file.txt
chattr -i file.txt 					# remove immutability on file.txt



---------
chkconfig
---------

- used to:
	- list current startup information of services or any particular service
	- updating runlevel settings of service
	- adding or removing service from management


### Commands ###

chkconfig --list              			# list running system-level services
chkconfig --del netconsole    			# remove the "netconsole" system-level service
chkconfig --add netconsole    			# add the "netconsole" system-level service
chkconfig --level 4 netconsole on       # configure "netconsole" at run level 4



-----------------
chmod/chgrp/chown
-----------------

- Linux/UNIX - change file mode bits (permissions), group ownership and user ownership management


#### Commands ####

chmod 750 [file]          # change permissions for file (read/write/execute for user, read/execute for group, no perms for other)
chmod -R 750 [dir]		  # change perms for directory recursively
chmod g+x [file]  		  # grant primary group for current user execute access to file

chmod 4750 [file] 		  # same as 1st command except implement sticky bit for user (i.e. only the user that owns the file (or root) can execute it)

	- output: -rwsr-xr-x. 1 root root 33544 Dec 13  2019 file

chmod g+ws [file_or_dir]  # set sticky bit at group level (sgid) for file or dir

chgrp adm file1.txt 	  # change group for file1.txt to adm
chown adm file1.txt 	  # change owner for file1.txt to adm

chown adm:admgrp file1.txt 	  	# change owner and group for file1.txt. Must be superuser to perform.
chown -R adm:admgrp [dir] 	  	# change owner and group for directory and all subdirs and files under dir. Must be superuser to perform

#### NEEDS MORE INFO ####






-------
ciphers
-------

- (in computing) - an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure (wikipedia)


#### Cipher groups ####


Group 1: These cipher suites have perfect forward secrecy (ECDHE) and authenticated encryption (GCM):
	E.g.: ECDHE-RSA-AES128-GCM-SHA256, ECDHE-RSA-AES256-GCM-SHA384

Group 2:  Perfect forward secrecy, but do not have authenticated encryption.
	E.g.: ECDHE-RSA-AES128-SHA256, ECDHE-RSA-AES256-SHA384

Group 3: Authenticated encryption but no perfect forward secrecy:
	E.g. : AES128-GCM-SHA256, AES256-GCM-SHA384

Group 4: Standard algorithms but no perfect forward secrecy or authenticated encryption:
	E.g.: AES128-SHA256, AES256-SHA256
	
Group 5: Perfect forward secrecy, but use SHA-1: (Do Not Include if possible)
	E.g.: ECDHE-RSA-AES128-SHA, ECDHE-RSA-AES256-SHA

Group 6: Standard algorithms but use SHA-1: (Do Not Include if possible)
	E.g.: AES128-SHA, AES256-SHA, DES-CBC3-SHA

Group 7: RC4 gets its own special category: (Do Not Include. This is deviation)
	E.g.: RC4-SHA

Group 8: For the love of God, do not use these: (Do Not Include. This is deviation)
	E.g.: DES-CBC-SHA, RC4-MD5, IDEA-CBC-SHA



-----
CISSP
-----

##### See CISSP_2021_curriculum_notes.txt #####



---------------
cloud - general
---------------

- IaaS vs. PaaS vs. SaaS

	- IaaS
		- You provide: Applications, Data, Operating System, Middleware, Runtime
		- Cloud provider provides: Servers, storage, virtualization, networking
			- egs. online data backup services (iDrive, carbonite)

	- PaaS
		- You provide: Applications, Data
		- Cloud provider provides: Servers, storage, virtualization, networking, operating system, middleware, runtime

	- SaaS
		- You provide: nothing
		- Cloud provider provides: Servers, storage, virtualization, networking, operating system, middleware, runtime, applications, data
			- eg. Google Docs



---
cmp
---

# Source: https://www.geeksforgeeks.org/cmp-command-in-linux-with-examples/


- Linux/UNIX - compare two files byte by byte to find out whether the files are identical or not


#### Commands ####

cmp file1.txt file2.txt 						# _ indicating that the files are identical
$ _

cmp file1.txt file2.txt
file1.txt file2.txt differ: byte 9, line 2  	# indicating that the first mismatch found in two files at byte 20 in second line

cmp -b file1.txt file2.txt
file1.txt file2.txt differ: 12 byte, line 2 is 154 l 151 i  	# indicating that the difference is in the 12th byte

$cmp -i 10 file1.txt file2.txt 					
$_ 												# indicating that both files are identical after 10 bytes skipped from both the files



-----
cntlm
-----

- Linux/UNIX/MacOS/Windows - a proxy client mainly used to connect to a proxy server in order to allow connectivity not directly allowed
						   - authenticating HTTP(S) proxy with TCP/IP tunneling and acceleration (man pages)
						   - last updated in 2013
						   - various alternatives available: squid, charles proxy (for MacOS)


# Installing via package manager and configuring
# Source: https://medium.com/hacker-toolbelt/cntlm-how-to-9709cde8dd02
# Manual: https://cntlm.sourceforge.net/cntlm_manual.pdf


1) Install
sudo apt-get install cntlm 				# install on Ubuntu/Debian
    OR
git clone https://aur.archlinux.org/cntlm.git
cd cntlm
makepkg -si 							# install on Arch Linux
	OR
You can install manually from sourceforge


2) Configure
sudo nano /etc/cntlm.conf 				# Edit cntlm configurations below
	Username: active_directory_domain_username
	Domain: active_directory_domain
	Proxy: upstream NTLM HTTP Proxy IP and port (xxx.xxx.xxx.xxx:yyyy)
	Listen: local port for cntlm proxy (default is 3128)

sudo cntlm -u username -d active_directory_domain -H   			# Generate hash for your AD password and copy/paste output into password fields

sudo ufw allow 3128 					# Run only if using ufw or other firewall


3) Run

sudo service cntlm restart 						# Restart cntlm service

OR

cd $CNTLM_HOME/bin
./cntlm -f -c $CNTLM_HOME/etc/cntlm.conf   		# Run cntlm in foreground. Add -v flag for verbose output/troubleshooting

export http_proxy=http://127.0.0.1:3128
export https_proxy=https://127.0.0.1:3128  		# export proxy env variables so outbound http/https connections will go through cntlm proxy

- Notes:
	- uncomment "Gateway yes" if you want to allow other machines to use the target machine for proxying




------
coding
------

##### Code Quality #####

# Source - Engineer Man - https://www.youtube.com/watch?v=MQM_BpZ1gOM

- plan and map out high level your code before you write it
	- use diagrams, jot notes, high level goals or steps

- use helpful comments in your code

- stick to a standard
	- consistency of how you write it
		- design patterns
		- variable naming
		- function naming
		- level of abstraction

- code clearly/avoid "clever" code
	- avoid using obscure syntax
	- hard to understand
	- fewer number of lines if possible
	- avoid OOP if possible

- introduce automated testing/test driven development
	- especially for critical code

- limit heavily abstracted and tightly coupled code
	- hard to extend
	- hard to maintain
	- hard to test



##### General Concepts #####

# Source: Engineer Man - https://www.youtube.com/watch?v=loGWCJGmv7Q

- Recursion - a function that calls itself within the body of that function
			-




-------------------------------
CompTIA A+ Certifcation 220-801
-------------------------------

- see "CompTIA_Aplus_Certification_220-801_notes.txt"



----
cpio
----

- (Linux/UNIX) - a tool for creating and extracting archives, or copying files from one place to another


#### Commands ####

ls | cpio -ov > text.cpio        # List files in current dir and put result into cpio archive
cpio -iv < text.cpio             # Extract cpio archive into current directory. Note if the current dir contains
								 # the same file names as in the archive, the files will not be extracted.



-------
crontab
-------

- Linux/UNIX - used to schedule recurring periodic activities
			 - The cron command uses text files that contain commands to execute them. These files are known as crontabs. These files must be located in a directory named /var/spool/cron
			 - "cron" is not a program in itself. It consists of two components:
			 	- "crond" is the daemon that executes the commands
			 	- "crontab" is a table manipulation program accesses the crontab files.


# Sources:
- https://crontab.guru - translate any cron entry and tells you what its timing interval is
- https://www.youtube.com/watch?v=QEdHAwHfGPc - Engineer Mans cron video
- boson.com CompTIA Linux+ labs


# Of the 5 stars:
- 1st - minute (0 - 59)
- 2nd - hour (0 - 23)
- 3rd - day of the month (1 - 31)
- 4th - month (1 - 12)
- 5th - day of the week (0 - 6) (Sunday to Saturday; 7 is also Sunday on some systems)



#### Format ####

* * * * * [user] [command]

# Quick examples

# every minute
* * * * * /path/to/script

# every minute as user tim
* * * * * tim /path/to/script

# every hour at the top of the hour
0 * * * * /path/to/script

# nightly at 11pm
0 23 * * * /path/to/script

# first of the month at 3pm, every month
0 15 1 * * /path/to/script

# every day at 8am, 10am, 12pm and 2pm
0 8,10,12,14 * * * /path/to/script

# every half hour
*/30 * * * * /path/to/script

# every tuesday at 8am
0 8 * * 2 /path/to/scropt

# first wednesday of each month
0 23 1-7 * 3 /path/to/script


#### Commands ####

rpm -q cronie cronie-anacron           # Verify if cron packages are installed
yum install cronie cronie-anacron      # or dnf install for Rocky Linux. Install if packages not present
ls -l /var/spool/cron 				   # Check if any crontabs are present

service crond status 				   # Check if cron service is running
service cron start   				   # Start crontab service (if not already running)

cat /etc/crontab 					   # See all current commands and timings configured
crontab -l 							   # Check crontab for current user
crontab -l -u root 					   # Check crontab for a specific user
crontab -e 							   # Edit crontab for current user




#### Files ####

/etc/cron.allow 			 # configures list of users to allow access to run and configure cron commands
/etc/cron.deny  			 # configures list of users to deny access for to run and configure cron commands

/etc/cron.hourly/ 			 # Note that the values in crontab can be used to execute the scripts in the following directories:
/etc/cron.daily/
/etc/cron.weekly/
/etc/cron.monthly/ 



----
CUPS
----

- Linux/UNIX - Common UNIX Printing System - acts as a print spooler, scheduler, print job manager, and can store information for numerous local or network printers.


#### Commands ####

/etc/init.d/cups status  			# check status of cups service (need sudo or run as root)
service cups restart  				# restart cups service (especially if any config updates - need sudo or run as root)
usermod -aG lpadmin [user]  		# add user to lpadmin group so that you can perform admin tasks on the cups user interface

lp -d [printer_name] -o media=legal -o sides=two-sided-long-edge /etc/cups/cupsd.conf       # print cupsd.conf file contents
lpq -P [printer_name] [user] 		# monitor printer queue
cancel -a [printer_name]  				# cancel all jobs in the printer queue


#### Files/configs ####

/etc/cups/cupsd.conf  				# cups configuration 
/var/log/cups/error_log 			# contains errors for printers
/var/log/cups/access_log 			# access log for printers via 


#### Add printer through cups web interface ####

1a] Update /etc/cups/cupsd.conf under "Only listen for connections from the local machine" section with the following:

Listen [machine_IP]:631


1b] Ensure /etc/cups/cupsd.conf has "Web Interface setting" set to "Yes"


2] Restart cups service

service cups restart


3] Go to http://localhost:631 in a browser window and perform the following:

	- click Administration tab
	- click Add Printers under Printers section
	- Enter user and password to continue [either root or different user if they are part of lpadmin group]
	- Choose from list of printers
	- Type the following:
		- Connection: http://localhost:631/[printer_name]
		- Name:
		- Location:
	- Select "Share this printer" checkbox, and then click Continue
	- Select printer model
	- Go through default options for the printer and continue to see successful setup of printer
	- You will see maintenance and administration page now published for this [and other] printers


4] Try printing a page

lp -d [printer_name] -o media=legal -o sides=two-sided-long-edge /etc/cups/cupsd.conf       # print cupsd.conf file contents


5] Monitor print queue

lpq -P [printer_name] [user]


6] From the maintenance dropdown in the UI, you can also:

- Print Test Page
- Pause Printer
- Reject Jobs
- Move All Jobs
- Cancel All Jobs




---------
curl/wget
---------

- (UNIX/Linux) a tool for transferring data to or from a server. Supports multiple protocols


#### Commands ####

# Download a file (eg. Artifactory, using credentials)
curl -u <user>:<password> http://artifactory_url:8081/artifactory/path/to/package/something.rpm -o ./something.rpm


# Upload a file (eg. to Artifactory w/ creds)
curl -X PUT -u <user>:<password> -T something.rpm "http://artifactory_url:8081/artifactory/path/to/package/something.rpm"


# recursive download with wget
wget --no-parent -r http://WEBSITE.com/DIRECTORY


# set insecure
echo insecure >> $HOME/.curlrc


# curl with telnet (in case telnet not available)
curl -v telnet://someurl.com:[port]

# show the request/response time duration of the curl command from the host to the destination
time curl <URL> 		

	- "real" or total or elapsed (wall clock time) is the time from start to finish of the call.
		- it is the time from the moment you hit the Enter key until the moment the curl command is completed.
	- "user" - amount of CPU time spent in user mode.
	- system or "sys" - amount of CPU time spent in kernel mode.


# curl down artifact from Artifactory with authentication
curl -o helm_3.11.1.tar -H 'X-JFrog-Art-Api:<your_api_key>' "https://artifactory.somewhere.com/artifactory/local-generic-repo/some_folder/some_tar.tar"


#### References and tools ####

https://reqbin.com/curl - excellent site for testing different curl commands (e.g. with POST, GET, POST JSON, PUT, DELETE, etc)
https://everything.curl.dev/ - an extensive guide for all things curl



---
cut
---

- Linux/UNIX - Print selected parts of lines from a file to standard output


#### Commands ####

cut -d: -f 1 /etc/passwd 			# display only the first column of the passwd file




----
date
----

- (Linux) used to display system date and time


### Commands ###

date                		# default format: Tue Oct 10 22:55:01 PDT 2017
date "+%Y-%m-%d"    		# format displays as "YY-MM-DD". Good for putting into filenames.
date +%D -s 2019-12-20		# set the date to Dec 20 2019
date +%T -s 23:00:00 -u     # set the time to 11pm, using UTC format (-u flag)




------
datree
------

# Sources
# https://www.datree.io/
# https://hub.datree.io/
# Github repo - https://github.com/datreeio/datree

- Kubernetes - program used to prevent misconfigurations by blocking resources that do not meet your policy
			 
- main features - get insights on the status of your cluster and enforce your desired policies on new resources.
			 	- scans Kubernetes resources against a centrally managed policy, and blocks those that violate your desired policies.
			 	- comes with multiple pre-built policies covering various use-cases, such as:
			 	- workload security
			 	- high availability
			 	- ArgoCD best practices
			 	- NSA hardening guide
			 	- and many more.

- Additional features
	Monitoring - Datree is first installed in monitoring mode that reports on policy violations, rather than block their deployments.
	CLI - Help your developers find misconfigurations in their configs before deploying them, by integrating Datree into their CI.
	Misconfiguration prioritization - Datree makes it easy to improve the quality of your cluster by prioritizing the misconfigurations to be fixed.
	Cluster score - Rank the stability of your cluster based on the number of detected misconfigurations.


#### Quick Start ####

- Pre-requisites:
	- helm cli (v3)
	- kubernetes cluster

1] Add the Datree Helm repository

helm repo add datree-webhook https://datreeio.github.io/admission-webhook-datree
helm repo update


2] Install Datree on your cluster

# Replace <DATREE_TOKEN> with the token from your dashboard, and run the following command in your terminal:

helm install -n datree datree-webhook datree-webhook/datree-admission-webhook --debug \
--create-namespace \
--set datree.token=<DATREE_TOKEN> \
--set datree.clusterName=$(kubectl config current-context)

	- this will create a new namespace (datree), where Datree’s services and application resources will reside.
	- datree.token is used to connect your dashboard to your cluster. Note that the installation can take up to 5 minutes.


#### NEEDS MORE MEAT ####




---------------
default gateway
---------------

Source: https://www.youtube.com/watch?v=pCcJFdYNamc

- definition: a device that forwards data from one network to another [usually a router]
	- lets devices from one network communicate with devices on another network
	- "default" means the designated device is the first option thats looked upon when data needs to exit the network
	- to connect out to the internet from a local network, it has to go through a gateway, usually the default
	- not required for internetwork communication

	- an IP address consists of two parts: network address and host address
	- subnet mask: reveals how many bits in the IP address are used for the network by masking the network portion of the address

		- eg.
			- IP address  - 192.168.0.2   -   11000000.10101000.00000000.00000010
			- Subnet mask - 255.255.255.0 -   11111111.11111111.11111111.00000000
																				network  network network     host

			  - for each of the 4 digits, the sections with all 1s indicate the network portion, and these "mask" the corresponding parts of the IP address of the network, telling it
			  	that it does NOT have to go through the default gateway to communicate with the device on that subnet
			  	- 192.168.0 is the subnet in this case
			  	- see 3:39 in source vid for examples of communication


----
deno
----

- What is it? A secure runtime for javascript and typescript

***to be continued***




---------
denyhosts
---------

- Linux - Python-based firewall that monitors SSH and can deny intruding IPs from attempting to connect


#### Commands ####

apt-get install denyhosts  				# install denyhosts on Ubuntu (need root)
/etc/init.d/denyhosts restart  			# restart denyhosts service (need root)
systemctl status denyhosts  			# check status of denyhosts service



#### Files and configs ####

/etc/hosts.allow  						# config file for IPs to allow
/etc/hosts.deny  						# config file for IPs to deny



----
DHCP
----

# Additional notes not covered in detail in CompTIA notes

- DHCP is a service that runs on a server, such as a Microsoft server or a Linux server
	- also a service that runs on routers


- scope - the range of IP addresses a DHCP server can hand out [from a start IP address to an end IP address]
				- customizable


- lease - DHCP assigns the IP address to a computer for a certain amount of time
				- to make sure DHCP server does not run out of IP addresses in its scope
				- renewal - lease expires if a computer does not automatically renew its lease [e.g. if a computer is not on the network, in which case lease expires]


- reservation - ensures that a specific computer or device [identified by its MAC address] will always be given the same IP address
						  - can configure this in DHCP settings on a DHCP server
						  - typically given to special devices such as network printers, servers, routers, etc.



-----
dmesg
-----

- Linux/UNIX - print or control the kernel ring buffer


#### Commands ####

dmesg  						# displays all the kernel messages
dmesg | grep -i memory 		# check amount of physical memory on system




---
DNS
---

# Sources:
# Powercert Animated Videos - https://www.youtube.com/watch?v=mpQZVYPuDGU
# https://support.dnsimple.com/articles/differences-between-a-cname-alias-url/


#### General ####

- Domain Name Service - resolves names to numbers [domain names to IP addresses]

- The "A record" maps a name to one or more IP addresses when the IP are known and stable.
- The "CNAME" record maps a name to another name. It should only be used when there are no other records on that name.
- The "ALIAS" record maps a name to another name, but can coexist with other records on that name.
- The "URL record" redirects the name to the target name using the HTTP 301 status code.


#### Domain Name Resolution process ####

1] User types in URL in browser and hits Enter

2] System first checks in-memory cache for IP address

3] Request then goes out to "Resolver" server (i.e. your ISP)

4a] If resolver cannot find it within its database, it will send the request to the "root" server: the top level of the DNS hierarchy

4b] 13 sets of root servers placed around the world, operated by 12 different organizations. Each set has its own unique IP address

5] Root server will not know what the IP address is, but will direct the request to the "Top Level Domain" server (or TLD) to resolve the IP address

6] Top level domain server stores the address information for top level domains such as .com, .net. org, etc.

7] Even the TLD server will not know, and will direct the resolver to the "Authoritative Name Servers"

8] Authoritative Name Server knows everything about a specific domain (such as .com). They are the final authority.

9] ANS -> Resolver -> computer

10] Resolver will store the IP for that specific address in its in-memory cache



#### Configure DNS server on Windows ####

1] Go to "Server Manager" dashboard

2] From the right pane, click Add roles and features.

3] On the Before you begin page of the Add Roles and Features Wizard, click Next.

4] On the Select Installation Type page, keep the default selection and click Next.

5] On the Select destination server page, keep the default selection and click Next.

6] On the Select server role page, select DNS Server. The Add features that are required for DNS Server? dialog box is displayed. Click Add Features.

7] On the Select server roles page, DNS Server is now selected. Click Next.

8] On the Select features page, keep the default selection and click Next.

9] On the DNS Server page, click Next.

10] On the Confirm installation selections page, click Install.

11] Installation of DNS server starts. After installation is completed, click Close. Close the Server Manager window.


#### Configure client-side DNS on Linux ####

1] Click Applications, select System Tools, and then select Settings.

2] From the Settings window, click Network in the left pane and then click the icon next to ON in the Wired section.

3] In the Wired dialog box, click the IPv4 tab.

4] Select Manual and provide the following details:

- address: 192.168.0.2
- netmask: 255.255.255.0
- gateway: 192.168.0.1
- dns: 192.168.0.1

5] Close settings window


#### Query Remote DNS servers on Linux with getent, host and dig ####

getent hosts localhost       			# list the entry for the localhost in the /etc/hosts file
getent hosts 127.0.0.1 		 			# perform reverse name resolution (i.e. get DNS name associated with an IP address)
getent passwd | cut -d: -f1 | sort      # list usernames in alphabetical order

host localhost 							# resolve localhost DNS name

dig 127.0.0.1 							# reverse lookup of localhost IP with more detail than host command
dig 127.0.0.1 							# forward lookup of localhost with more detail than host command


# files

/etc/hosts 							# contains manual entries of hostnames and associated IPs
/etc/resolv.conf 					# stores info about nameservers and DNS servers that lookup will use to resolve names/IPs
/etc/nsswitch.conf  				# defines the order in which the name resolution is to be performed
									# order:
										# files: defines the /etc/hosts file
										# dns: defines the DNS
										# myhostname: 







---
DMZ
---

- Demilitarized Zone
	- a.k.a a perimeter network
	- used to improve the security of an organizations network by segregating devices such as computers and servers on opposite sides of a firewall

	- the servers are behind a companys firewall and are inside the companys private network
	 	- the company is letting in people from an untrusted network [the internet] and are given access behind the companys firewall

	- divides a network into two parts by taking devices from inside the firewall and putting them outside
		- a more secure network would use 2 [or more] firewalls for extra layers of protection

	- in the real world, a DMZ is an area where the military is forbidden
	- in the computing world, a DMZ is where firewall protection is forbidden


------
docker
------

- Linux/Windows/Mac - a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called "containers"
					- used to automate the deployment of application in lightweight containers so that application can work efficiently in different environments.
					- ***containers are just Linux processes***


#### Installation - Rocky Linux ####

1] Add docker repo

sudo dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo


2] Update repos to ensure latest packages are available

sudo dnf update


3] Install docker packages
	- Note:
		- "docker-ce" is the main package.
		- "docker-ce-cli" provides the command-line interface.
		- "containerd.io" installs the containerd container runtime.

- Install the above packages with the following command:

	sudo dnf install docker-ce docker-ce-cli containerd.io


4] Start docker

	sudo systemctl enable docker


5] Update rc.local permissions (Rocky Linux only)

	- If below message appears:

	/etc/rc.d/rc.local is not marked executable, skipping.

  
   - Execute the following command:

   sudo chmod +x /etc/rc.d/rc.local


   - Try starting docker again:

   systemctl start docker

   systemctl status docker


6] Enable non-root user access

sudo usermod -aG docker $USER 				# add $USER to docker group

id $USER 									# verify user is in docker group

systemctl restart docker

systemctl status docker


7] Verification

docker ps 					# check for running docker processes. There should be none.

docker pull busybox 		# pull busybox image from docker hub

docker images 				# check local docker registry to see if busybox image is available




#### Commands ####

docker run -d image_tag:version  									# Run specified image as a container (-d for detach so you dont login to container automatically)

docker images --filter=reference='*/*/image_name/*/*:4.1.4' -q 		# get image id by filter

docker rmi $(docker images --filter=reference="*:stuff_*" -q) 		# Remove images by id, only works if this id does not have multiple tags

docker rmi -f fd484f19954f 											# Forcefully delete image (by ID) and all its associated tags

dockerd --debug  													# Debug docker service (useful for startup issues)



#### Docker Bench Security for host and container scanning ####
# Source Github repo: https://github.com/docker/docker-bench-security
# Image: docker/docker-bench-security:latest

1) Pull down the docker bench security image from docker hub
2) Run it according to the documentation in the Github repo:

	On MacOSX:
		docker run --rm --net host --pid host --userns host --cap-add
		audit_control \
		-e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST \
		-v /etc:/etc \
		-v /var/lib:/var/lib:ro \
		-v /var/run/docker.sock:/var/run/docker.sock:ro \
		--label docker_bench_security \
		docker/docker-bench-security

	On Redhat Linux:
		docker run --rm --net host --pid host --userns host --cap-add
		audit_control \
		-e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST \
		-v /etc:/etc:ro \
		-v /usr/bin/containerd:/usr/bin/containerd:ro \
		-v /usr/bin/runc:/usr/bin/runc:ro \
		-v /usr/lib/systemd:/usr/lib/systemd:ro \
		-v /var/lib:/var/lib:ro \
		-v /var/run/docker.sock:/var/run/docker.sock:ro \
		--label docker_bench_security \
		docker/docker-bench-security

- It will scan the host and all the containers running on the host
- At the end, you will see the number of checks it ran + the score
- The higher the score the better – scoring is:
	- Warn Scored -1
	- Pass Scored +1
	- Note Score -0


#### Dockscan ####

- dockscan - https://github.com/kost/dockscan
	- needs ruby 2.0
	- installation via gem install


# Docker for Windows config location
C:/Users/Username/.docker/machine/default/config.json


# docker run example (ssl scan of a domain)
docker run --rm -it shamelesscookie/sslscan:latest yourURLminushttporhttps


# sharing namespaces: https://www.guidodiepen.nl/2017/04/accessing-container-contents-from-another-container/

# exposing docker port with bobrik/socat container (e.g. to make accessible from Jenkins)
docker run -d -v /var/run/docker.sock:/var/run/docker.sock -p 2376:2375 bobrik/socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock


# publish a port (convention: -p [host_port]:[container_port])

docker run -d -p 80:80 [image] 


# run container with proxy settings within container

docker run -d -e HTTP_PROXY=http://localhost:3128/ -e HTTPS_PROXY=http://localhost:3128/ [image]


# bobrik/socat container as a service:
docker service create --mode=global --name socat \
--publish 2376:2375 \
--mount "type=bind,source=/var/run/docker.sock,destination=/var/run/docker.sock" \
--entrypoint "socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock" \
bobrik/socat

# docker login troubleshooting
	- docker login not working (user interaction not allowed msg)
   		- remove credsStore from config.json
   		- if above doesn't work, uncheck 'Securely store Docker logins in macOS keychain'
   		- if above doesn't work, rm /usr/local/bin/docker-credential-osxkeychain

# checking container logs

docker run -it -v /var/lib/docker:/var/lib/docker <image_ID> bash - check /var/lib/docker/containers/<ID>/<container-id>-json.log

# persistent container, ensure auto restarts if machine goes down
docker update --restart=always <container_id>


# sidecar pattern - https://www.magalix.com/blog/the-sidecar-pattern

# spin up Jenkins in docker
docker run \
    --name dcct-mobile-callisto-jenkins \
    --detach \
    --network jenkins \
    --env DOCKER_HOST=tcp://docker:2376 \
    --env DOCKER_CERT_PATH=/certs/client \
    --env DOCKER_TLS_VERIFY=1 \
    --publish 80:8080 \
    --publish 50000:50000 \
    --volume jenkins-data:/var/jenkins_home \
    --volume jenkins-docker-certs:/certs/client:ro \
    <ARTIFACTORY_URL>/local-docker-dcct/mobile/dev-jenkins-lts-2.263.1:1.0

# spin up cntlm on docker
docker run --restart always --name cntlm \
  -e "USERNAME=username" \
  -e "DOMAIN=mydomain" \
  -e "PASSNTLMV2=640937B847F8C6439D87155508FA8479" \
  -e "PROXY=123.123.123.123:8080" \
  -p 3128:3128 \
  robertdebock/docker-cntlm



# Multi-stage docker image build - Building an image with multi-stage (cut down the size) - example Dockerfile  (2021/04/08)

#### First stage

FROM ubuntu:18.04 as builder					 # notice the 'as builder' in the first stage
RUN apt-get update
RUN apt-get install -y make yasm nasm as31 binutils
COPY . .
RUN make release


#### Second stage

FROM scratch            					     # can use alpine here if scratch is too empty
COPY --from=builder /asttpd /asmttpd             # this is where the executable produced from stage 1 gets copied over to the second stage
COPY /web_root/index.html /web_root/index.html

CMD ["/asmttpd", "/web_root", "8080"]



# Multi-stage docker image build - 2nd example of a python Dockerfile where you can build either a debugger or regular image, depending on the scenario
# Source: DevOps Directive - https://www.youtube.com/watch?v=qCCj7qy72Bg

###############
# base
###############

FROM python:3.8.4-slim AS base

RUN pip install pytz

WORKDIR /src
COPY . ./


######################
# debugger
######################

FROM base AS debugger

RUN pip install debuggy

ENTRYPOINT ["python","-m", "debugger", "--listen", "0.0.0.0:5678", "--wait-for-client", "-m"]


######################
# primary
######################

FROM base as primary

ENTRYPOINT [ "python", "-m"]


# END DOCKERFILE

- to build the debugger:
	docker build -t debugger:<version> --target=debugger .

- to run debugger:
	docker run -p 5678:5678 debugger:<version> unittest         # where unittest is the py module used in the video example

- to build the primary image:
  docker build -t primary:<version> --target=primary .



#### References ####

https://docker-curriculum.com/ - beginner/starter guide on Docker
https://blog.jessfraz.com/post/docker-containers-on-the-desktop - Jess Frazelle''s blog about creating containers for Windows desktop applications
https://github.com/jessfraz/dockerfiles - Jess Frazelle''s GIT repo with all her dockerfiles :D
https://github.com/marcel-dempers/my-desktop - Marcel Dempers'' Docker Desktop setup repo




--------------
docker compose
--------------

# Source: Marcel Dempers (That DevOps Guy) - https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/messaging/kafka/docker-compose.yaml

- side notes
	- docker compose is a separate installer package on Linux, but is included in Docker Desktop for Mac and Windows and does not need to be installed separately in this case.
	- docker compose allows you to describe the building and runninig of multiple container images all in one file [in YAML format]


# Example of docker-compose.yaml from Marcel's Kafka with docker compose video
# Note: you would still need to create the network like in the docker example for building/running the containers individually

version: "3.8"
services:
  zookeeper-1:
    container_name: zookeeper-1
    image: aimvector/zookeeper:2.7.0
    build:
      context: ./zookeeper
    volumes:
    - ./config/zookeeper-1/zookeeper.properties:/kafka/config/zookeeper.properties
    - ./data/zookeeper-1/:/tmp/zookeeper/
    networks:
    - kafka
  kafka-1:
    container_name: kafka-1
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-1/server.properties:/kafka/config/server.properties
    - ./data/kafka-1/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-2:
    container_name: kafka-2
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-2/server.properties:/kafka/config/server.properties
    - ./data/kafka-2/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-3:
    container_name: kafka-3
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-3/server.properties:/kafka/config/server.properties
    - ./data/kafka-3/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-producer:
    container_name: kafka-producer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer:
    container_name: kafka-consumer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer-go:
    container_name: kafka-consumer-go
    image: aimvector/kafka-consumer-go:1.0.0
    build: 
      context: ./applications/consumer
    environment:
    - "KAFKA_PEERS=kafka-1:9092,kafka-2:9092,kafka-3:9092"
    - "KAFKA_TOPIC=Orders"
    - "KAFKA_VERSION=2.7.0"
    - "KAFKA_GROUP=orders"
    networks:
    - kafka
networks: 
  kafka:
    name: kafka


# How to Use the above file

docker network create kafka   # pre-requisite to running the containers
docker compose build       # will build all of the container images described in the file. Notice the different context or folder paths for each image shown in the build context section
docker compose up          # will run all of the container images that were built from this docker compose file



----
entr
----

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg

- Linux - watch files, when they change it will execute them


#### Commands ####

entr ruby main.rb
		OR
entr some_shell_script.sh 		# if shell script is updated, entr will execute it immediately in the terminal you ran entr in




---------------------
environment variables
---------------------

- Linux/UNIX - name/value pair settings that apply at the user or OS level


#### Command examples ####

printenv            # prints full or partial environment.
printenv HOSTNAME   # print value of single env variable (HOSTNAME in this case)
env 				# prints all exported environment.

set 				# prints the name and value of each shell variable available. Can also be parameterized to set or change variables
unset VAR  			# unset an env variable 

export VAR 								# export a local variable and make an env variable
export -n VAR 							# convert env variable to a local variable
export -p  								# view all exported environment variables for current shell
export PATH=$PATH:/usr/local/bin 		# add a path to PATH in order to easily find executables

echo $VAR 			# show the value of a variable

source ~/.bashrc    # set environment variables configured in .bashrc




---
exa
---

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg

- Linux - a nicer alternative to ls command


#### Commands ####

exa - list files in current directory
exa -lah - list files in current directory with more detail
exa --tree - list files in current directory and subdirectories
exa --tree --long - list files in current directory and subdirectories with more detail




------
expand
------

- Linux/UNIX - read a file but convert tabs to spaces

expand file.txt  			 					# show content of file.txt converting any tabs to spaces
expand --tab=20 newfile.txt 					# increase the number of spaces from 1 to 20 for each tab found
expand --tab=20 newfile.txt > newfile1.txt 		# redirect the output into a file

unexpand newfile1.txt  							# convert the spaces back to tabs from a file which has been expanded



------------------------------
fail2ban - for blocking access
------------------------------

- Source article: https://www.digitalocean.com/community/tutorials/how-to-protect-ssh-with-fail2ban-on-centos-7

To install on CentOS [for Rocky Linux, you would use dnf, but need to test it]:

yum install epel-release
yum install fail2ban

- Edit /etc/fail2ban/jail.local (see jail.local.txt in this repo for details or see content below)
vi /etc/fail2ban/jail.local

### CONTENT TO ADD - START ###

[DEFAULT]
# Ban hosts for one hour:
bantime = 360000
ignoreip = 127.0.0.1/8 76.68.78.155/8 174.88.74.105/8
maxretry = 1

# Override /etc/fail2ban/jail.d/00-firewalld.conf:
banaction = iptables-multiport

[sshd]
enabled = true

#### CONTENT TO ADD - END ####

systemctl enable fail2ban
systemctl start fail2ban
fail2ban-client status
fail2ban-client status sshd



--
fc
--

- Linux/UNIX - lists, edits and reexecutes commands previously entered to an interactive shell

fc  							# Edits the last command executed.
	> Edit last command as needed and exit 

fc -l                			# list previous command history (similar to history command)
fc -s 1001  					# list command that was run from index 1000




-----
fdisk
-----

#### General ####

- Linux command: partition table manipulator
	- hard disks can be divided into one or more logical disks called partitions. This division is described in the partition table found in sector 0 of the disk.


#### Commands ####

fdisk -l 				 # lists all partitions
fdisk -l /dev/sda        # lists the partitions on the /dev/sda device

# Remove and add partition
fdisk /dev/sdb           # displays list of options for the device, type "m" and ENTER to see complete list, e.g. delete a partition, change a partition type, etc.
	d                    # Delete a partition, will ask you which one
	2                    # Delete partition #2 for example
	p                    # Add a primary partition
	[Enter]				 # Hit Enter to keep the default for the starting disk sector of the partition
	[number_then_enter]  # Put in an end sector number, example: 35000000 and hit enter
	Y or N               # for removing a particular signature on the sector, ext4 for example. In the boson labs I chose yes
	w                    # write the newly updated partition table and exit the fdisk utility prompt


# Example of fdisk menu

[root@server ~]# fdisk /dev/sda
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table
Building a new DOS disklabel with disk identifier 0x031547c5.

Command (m for help): m
Command action
   a   toggle a bootable flag
   b   edit bsd disklabel
   c   toggle the dos compatibility flag
   d   delete a partition
   g   create a new empty GPT partition table
   G   create an IRIX (SGI) partition table
   l   list known partition types
   m   print this menu
   n   add a new partition
   o   create a new empty DOS partition table
   p   print the partition table
   q   quit without saving changes
   s   create a new empty Sun disklabel
   t   change a partitions system id
   u   change display/entry units
   v   verify the partition table
   w   write table to disk and exit
   x   extra functionality (experts only)

Command (m for help):



--------------
File transfers
--------------

# Linux large tar transfer w/ untar at the same time, nohup the log into tmp:
nohup bash -c "cat somearchive.tar | ssh -t linuxuser@x.x.x.x 'tar -C /path/to/dest -xvf - /path/to/tarup'" > /tmp/filexfer.out &

# using netcat

tar -cvf - ~/var | nc -vv -l 127.0.0.1 -p 1234    # better to use the localhost IP for security

# then on client
cd /tmp
nc 127.0.0.1 1234 | tar -xvf -


-----------
Filesystems
-----------

# General

- file systems divide the storage space on a drive into virtual compartments known as "clusters"
- maintain an index of where individual files are located

# Linux file system directory assignment meanings

# Sources:
# Engineer Man: https://www.youtube.com/watch?v=UFIoRLqhFpo
# Fireship: https://www.youtube.com/watch?v=42iQKuQodW4

/bin - stores common/essential executables available for everyone, egs. cp rm ls

/boot - kernel and boot configuration needed to boot the system
	  - you may see the boot mount point by running "mount" command, e.g. /dev/sda1

/dev - device files: files which point to both physical and pseudo devices

/etc - editable text config: system and program configuration files

/home - non-root user home directories for user data

/lib
/lib32
/lib64 - library files used by the system, includes .so files and others

/lost+found - saved files due to failure

/media - auto-mounting place for certain external devices on some distros

/mnt - place to mount various file systems

/opt - optional/add-on software: various software

/proc - virtual filesystem for resources, processes and more

/root - root user home directory

/sbin - essential executables for super user (root)

/tmp - temporary files not persisted between system reboots

/usr/bin - non-essential installed binaries

/usr/local/bin - locally compiled binaries

/var - this directory contains files which may change in size, such as spool and log files.

/var/lib - Variable state information for programs.



# Types

auto - this is a special one. It will try to guess the fs type when you use this.

exFAT - Extended file allocation table
	- file system optimized for high-capacity USB flash drives and memory cards
	- maximum file size = 16EB
	- default FS for SDXC mem cards
	- broader non-Windows OS support than NTFS [including read/write on MacOS]


ext - Extended File System
	- launched in 1992 for Linux

ext2 - launched in 1993
	- default FS in many Linux systems for years

ext3
	- launched in 2001
	- this is the most common Linux fs type from a couple years back
	- introduced journaling to protect against corruption in the event of crashes or power failures

ext4
	- launched 2008
	- this is probably the most common Linux fs type of the last few years
	- maximum file size of 16TB, max volume size of 1EB
	- no native Windows or MacOS support


FAT - File Allocation Table
	- major variants: FAT12, FAT16, FAT32
		- each has increasing number of clusters, maximum file and volume sizes
		- FAT32 still widely used for removeable media and popular due to wide OS compatibility

		- max file size
			- FAT12 - 32MB [8KB clusters] or 16MB w/ 4KB clusters
			- FAT16 - 2GB/4GB
			- FAT32 - 4GB

		- max volume size
			- FAT12 - 32MB [8KB clusters]
			- FAT16 - 16GB [256KB clusters]
			- FAT32 - 32GB [Windows format]
						  - 2TB [other OS]
						  - 16TB [theoretical]


glusterfs - for Gluster


HFS - Hierarchical File System [for MacOS]
	- aka MacOS Standard
	- was introduced w/ journaling as HSF+ [HSF Extended]
	- files and volumes up to 8EB [as of MacOS 10.4]
	- 2017 - APFS launched [Apple file system]
	- no native Windows or Linux support



NTFS - New Technology File System

	- this is the most common Windows fs type or larger external hard drives
	- file size limit of 16 exabytes
	- journaling file system [maintains a record of changes in case of failures]
	- supports file permissions and encryption
	- all Windows must be installed on NTFS
		- downside: limited non-Windows OS compatibility - eg. read-only in MacOS and older Linux distros


vfat - this is the most common fs type used for smaller external hard drives


ZFS - zed file system
- created by Sun Microsystems, now developed by OpenZFS project
- an advanced fs fype that pools disk storage
- integrated volume manager to control storage hardware
	- provides increased data protection
- available for Linux, FreeBSD and TrueOS



# General commands

- in Linux,
	- "blkid" shows attributes of block devices (such as filesystem type)
	- "lsblk" shows partition information, including the type of FS on each partition
	- "mkfs" is for creating a file system on a partition
	- "mkswap" is used to create swap space on a disk/partition


# Creating swap space on a partition, followed by creation of an FS
umount /dev/sdb2               # unmounts a partition [in this case, /dev/sdb2, to prepare for creating swap space on it]
mkswap /dev/sdb2               # creates swap space on a partition. NOTE: this will wipe the file system signature as well
mkfs -t <fs_type> /dev/sdb2    # creates a file system, where fs_type can be ext3 for example, on the /dev/sdb2 partition
lsblk -f                       # verify the partition type


#### Command options ####

# lsblk

lsblk -a                      # include empty devices in the list
lsblk -d                      # show only the main devices, e.g. /dev/sdb, but don't show /dev/sdb1 and sdb2
lsblk -m                      # show ownerm group and mode of each device
lsblk -o NAME,SIZE            # show only name and partition size of each device


# blkid

blkid -i /dev/sdb1            # display I/O limits on /dev/sdb1
blkid -p /dev/sdb1            # show additional information about the /dev/sdb1 block device
blkid -o list                 
   OR
cat /proc/mounts 			  # show filesystem type and mount points of each block device


# debugfs - file system debugger

debugfs /dev/sdb1 			  # open debugfs prompt for checking /dev/sdb1
	? 						  # see list of commands that can be used
	q  						  # quit debugfs command line


# df - file system disk usage 

df -h                         # show general info about mounted filesystems (e.g. available and used space in MB)
df -i  						  # display info on inodes (can combine with h flag as well)
df . 						  # determine partition of current working directory


# du - file space usage

du  						  # check disk usage of every file and directory within current working directory
du -ah [directory]			  # get the size of the directory in KB, MB, or GB
du -mh [directory] 			  # summary of disk usage of directories and subdirectories in Megabytes (MB)
du -sh [directory]			  # display total size of directory


# dumpe2fs - dump ext2/ext3/ext4 file system information

dumpe2fs -h /dev/sdb1 		  # view the super block and blocks group information of an extended filesystem



# fsck - file system checker for errors, can use to repair as well

umount /dev/sdb1 			  # unmount sdb1 fs to run next command
fsck /dev/sdb1                # check for errors on the /dev/sdb1 fs (note: can be used on any unmounted fs)
fsck -a /dev/sdb1             # check and auto-repair if errors. can use -y option as well if available on OS
fsck -M /dev/sdb1             # check mounted file system - not recommended
ls /sbin/fsck*                # check for fs specific command options to run with fsck
fsck -t [fs_type] [fs] 		  # check specific fs type
fsck /dev/sdb1 -f             # force fs check and generate report
fsck -fv /dev/sdb1  		  # get verbose output


# tune2fs - allows the system administrator to adjust various tunable filesystem parameters on Linux ext2, ext3, or ext4 filesystems.

tune2fs -l /dev/sdb1
tune2fs -l /dev/sdb1 | grep -i mount     # check when FS was mounted, options used, max # the fs can be mounted, etc.
tune2fs -c 20 /dev/sdb1                  # change max number of times fs can be mounted
tune2fs -l /dev/sdb1 | grep -i name      # see volume label
tune2fs -l /dev/sdb1 | grep -i check     # when was fs was checked last
tune2fs -i 5d /dev/sdb1                  # check fs every 5 days
tune2fs -i 0 -c 0 /dev/sdb1 			 # switch off the maximum mount count and also perform interval tests on /dev/sdb1 (0 seconds between intervals)
tune2fs -L Main /dev/sdb1 				 # create volume label "Main" for /dev/sdb1


# e2label - set a label for a volume

e2label /dev/sdb1         # check for label of a volume or partition
e2label /dev/sdb1 Data    # set name "Data" for sdb1 device




----
find
----

- in Linux, used to find [recursively] files and directories that match a specific pattern or attribute

# find jax2b cached artifacts in maven local repo
find . | grep jaxb2-plugin | grep 0.8.1 | grep jar


# grep - using find with grep

find . -type f -exec grep -iH "rbac" {} \;      # searches for the string "rbac" in current directory and all subdirectories.
												# using -type flag so the search doesn't print unnecessary "this is a directory" output
										        # It will also print name of the file matching the string, remove -H flag if this is not necessary

# exec rm files - using find to search for all csv files in current folder (without going into subfolders) and removing them if they are older than 1 day (1440 minutes = 1 day)

find . -maxdepth 1 -type f -mmin 1440 -name "*.csv.*" -exec rm -f {} \;


# search for 1 occurrence of "linux" in all files/subdirectories under "/" using xargs

find / -type f | xargs -n 1 grep -H linux


# search for all files which have sticky bit set at user level (SUID) owned by root. Also command for searching SGID and for both at the same time.

find / -user root -perm -4000 -print
find / -group root -perm -2000 -print
find / -perm -4000 -o -perm -2000 -print


# print the symbolic and octal values of all files/dirs within current working dir

find . -printf “%M %m %f\n”




--------
firewall
--------

# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=a8pyWnBRVV8

- a system that is designed to prevent unauthorized access from entering a private network
	- filters the information that comes in by the rules in its "access control list" (determined by network administrator)
	- blocks unwanted traffic

- purpose is to create a safety barrier between a private network and public internet



---------------------------
firewalld (on CentOS/RHEL7)
---------------------------

# configuration file location
/etc/firewalld/firewalld.conf


# check firewalld status
systemctl status firewalld
	# or
firewall-cmd --state


# check default configuration
firewall-cmd --list-all


# check zones
firewall-cmd --get-zones   		# output: block dmz drop external home internal public trusted work

# or get default zone
firewall-cmd --get-default-zone


# check firewalld services, see all the apps which firewalld can be configured for
firewall-cmd --get-services


# allow a specific port
firewall-cmd --add-port=3306/tcp


# change default zone to dmz and open up incoming connections (this is supposed to make it internally accessible only, but you can also limit the source address entries instead of putting 0.0.0.0 as shown below)
firewall-cmd --set-default-zone=dmz
firewall-cmd --zone=dmz --add-rich-rule='rule family="ipv4" source address="0.0.0.0/0" accept'


# restart firewalld service
systemctl restart firewalld
   # or
firewall-cmd --reload


# add/remove a specific service(s)
firewall-cmd --add-service=mysql --permanent    # the permanent flag will make this setting survive a reload or restart
  # or
firewall-cmd --add-service={mysql,http,https,ldap} --permanent
  # or remove
firewall-cmd --remove-service={mysql,http,https,ldap} --permanent
firewall-cmd --reload


# port forwarding
firewall-cmd --add-forward-port=port=8080:proto=tcp:toport=80   # can add toaddr= if you want to redirect to another host


# add traffic "rich rule"
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.102" accept' # will accept all traffic from this IP 
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.103" drop'   # will not accept traffic from this IP


# set all temporary or runtime firewalld configurations permanently ***
firewall-cmd --runtime-to-permanent



# Zones list

drop: The lowest level of trust. All incoming connections are dropped without reply and only outgoing connections are possible.

block: Similar to the above, but instead of simply dropping connections, incoming requests are rejected with an icmp-host-prohibited or icmp6-adm-prohibited message.

public: Represents public, untrusted networks. You don’t trust other computers but may allow selected incoming connections on a case-by-case basis.

external: External networks in the event that you are using the firewall as your gateway. It is configured for NAT masquerading so that your internal network remains private but reachable.

internal: The other side of the external zone, used for the internal portion of a gateway. The computers are fairly trustworthy and some additional services are available.

dmz: Used for computers located in a DMZ (isolated computers that will not have access to the rest of your network). Only certain incoming connections are allowed.

work: Used for work machines. Trust most of the computers in the network. A few more services might be allowed.

home: A home environment. It generally implies that you trust most of the other computers and that a few more services will be accepted.

trusted: Trust all of the machines in the network. The most open of the available options and should be used sparingly.



------------
font flipper
------------

Site: https://fontflipper.com

- a website that helps you choose/experiment with different fonts for your designs with a Tinder-style UI
- can download the font you like from Google fonts


-------
Fortify
-------

# upload command:
./fortifyclient -url https://fortify_url.com/ssc -authtoken xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx uploadFPR -file ../something.fpr -project "project_name" -version "latest"



---
fmt
---

- Linux/UNIX - simple optimal text formatter. Reformat each paragraph in the FILE(s), writing to standard output



#### Commands ####

fmt -u newfile1.txt  			# one space between words, two after sentences



----
free
----

- Linux/UNIX - Display amount of free and used memory in the system


#### Commands ####

free -m 				# memory usage in MB
free -g 4  				# show mem usage in GB, update every 4 seconds
free -l 				# show the high and low mem usage statistics 




-----
fuser
-----

- Linux/UNIX -  displays the list of processes that are using files and sockets


#### Commands ####

fuser -all  			# displays for all users
fuser . 				# display only for processes from the current directory
fuser -v ./ 			# with more detail



---
fzf
---

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg

- aka fuzzy finder
- Linux - searches for files when the name is not quite known


#### Commands ####

fzf             
   ipad product awes keyb       # searches for a file called boost-your-ipad-productivity-with-these-awesome-keyboard-shortcuts.mov   
								# the keywords (which don't have to be complete) help narrow down the filename
   ctrl+r (with fzf installed)  # does fuzzy matching on command history




---------------
getfacl/setfacl
---------------

# Sources: https://www.computerhope.com/unix/usetfacl.htm

- Linux/UNIX - getfacl - displays the file name, owner, the group, and the ACL (Access Control List)
			 - setfacl - modifies the access control list of a file or files.


#### Commands ####

getfacl /some/path          								# get access control list for target directory
getfacl /path/to/file 										# get access control list for target file
setfacl -Rm g:logfsg:r-x d:g:logfsg:r-x /some/path 			# set default ACL for specific directory (recursively) to a particular group (logfsg)

setfacl -b /data/plab.txt                                       # removes all applied ACLs from specified file (or directory)
setfacl --set u::rwx,g::r-x,o::- plab 							# set ACL for a directory "plab" (750)
setfacl -d --set u::rwx,u:administrator:rwx,u:matt:rx,g::rx,g:administrator:rx,o::- plab    # set default ACL for dir "plab"

getfacl -d plab | setfacl -d --set-file=- plabnew               # apply ACL from dir "plab" to new dir "plabnew"
getfacl -R plab > plab.facl 									# put ACL for "plab" into an facl file
setfacl --restore plab.facl 									# restore ACL for "plab"


#### files ####

cat /boot/config-3.10.0-957.1.3.el7.x86_64 | grep _ACL      	# check if kernel supports ACL



---
GIT
---


### Commands and configs ###

# Sources
# Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o
# git-scm - https://git-scm.com
# General experience :D


# General usage - once you install git onto your Linux, Mac or Windows machine, below is the typical process when working with the git client to
# update and check-in files to a source code repository hosted on Github, Bitbucket or any other remote Git-based source control application

# 1) Getting Started
mkdir git_workspace             # Create a workspace on your local or server machine
git clone [git_repo_url]        # Clones a entire copy of the target remote repository onto your local
cd [repp_name]                  # Go into locally cloned copy of the repo
git remote -v 					# Shows remote repo URL (this will sohw the fetch and pull remote repo URL, which sometimes can be different)


# 2) Everyday commands

git pull                        # Syncs your local repository with the remote repository. Note: ALWAYS run git pull before you start making updates or are about to commit/push updates
git status -s                   # check to see which files have been added/modified/deleted
git branch                      # Shows the current working branch + lists previous working branches

git checkout [branch_name]      # Switches your working branch to a different one
OR
git checkout -b [branch_name]   # Create new branch based on current working branch and switches to it

git diff                        # See differences made between latest version committed and local updates (all files)
OR
git diff [filename]             # See differences made between latest version committed and local updates (single file or folder)

git add [filename(s)]           # prepare filename(s) to be committed to repo
OR
git add .                       # prepare all files and folders in parent and subfolders to be committed

git commit -m "Your msg" 		# commit files into repo with your commit message on what was updated
git push -u origin [branch_name]    # push files into remote to the target branch (which matches your working branch locally).


# 3) Other useful commnads

git log                        # see the repo's commit history
git log [filemame/foldername]  # see a target file or folder's commit history

git reset                      # reverts the "git add" command. Will un-prep the files originally prepped to be committed. Useful if you need to make more changes or do not want
							   # to add specific files for committing.

git branch -m [current_branch_name] [new_branch_name]        # Rename a branch
git merge [branch_to_merge_files_from]                       # Merge the target branch into your working branch

git stash save [stash_name]   								 # Saves all files currently in added/modified/deleted state, which are not yet prepped for committing, into a stash,
															 # which can later be restored
git stash pop [stash_name]									 # Restores all files from the target stash

git config --global user.name "John Doe"                     # Globally configure the username you will use for updates to any GIT repo
git config --global user.email johndoe@example.com           # Globally configure the email address you will use for updates to any GIT repo



# .gitattributes - use .gitattributes within a repo to define file behaviours/attributes. Eg below of contents within a .gitattributes file:

*           text=auto
*.txt		text
*.vcproj	text eol=crlf
*.sh		text eol=lf
*.jpg		-text


# Alias - creating command alias and storing within git configuration

git config --global alias.ac "commit -am"    # where ac is the new alias command
git ac "Your commit message"								 # usage for above alias



# Bisect - do a binary search through commits to figure out where a bug was introduced
# Source: git-scm - https://git-scm.com/docs/git-bisect

1] As an example, suppose you are trying to find the commit that broke a feature that was known to work in version v2.6.13-rc2 of your project. You start a bisect session as follows:

git bisect start
git bisect bad                 # Current version is bad
git bisect good v2.6.13-rc2    # v2.6.13-rc2 is known to be good


2] Once you have specified at least one bad and one good commit, git bisect selects a commit in the middle of that range of history, checks it out, and outputs something similar to the following:

# Output
      Bisecting: 675 revisions left to test after this (roughly 10 steps)
  

3] You should now compile the checked-out version and test it. If that version works correctly, type:

git bisect good


4] If that version is broken, type

git bisect bad

- then git bisect will respond with something like

# Output
	Bisecting: 337 revisions left to test after this (roughly 9 steps)


5] Keep repeating the process: 
	- compile the tree
	- test it
	- depending on whether it is good or bad run "git bisect good" or "git bisect bad" to ask for the next commit that needs testing.
	- Eventually, there will be no more revisions left to inspect, and the command will print out a description of the first bad commit.
	- The reference refs/bisect/bad will be left pointing at that commit.



# Clean - cleans up hanging or loose file references

git clean -df


# Combine add and commit in one command (add applies to working directory like using "git add .")

git commit -am "Your commit message"

	# the long equivalent of the above is:
	
	git add [files]
	# or
	git add .                               # for prepping all files and folders recursively to be committed to the branch/repo

	git commit -m "Your message here"       # this will commit all the "staged" or prepped files to your repo

    
    # and then to push to your remote site

    git push -u origin [branch_name]        # where -u = upstream, origin = the default short name for the remote location




# Commit message update (of an existing commit)

git commit --amend -m "Your new commit message"



# Commit additional files to your last commit (without updating the commit message) - only works locally, if you haven't pushed your commit to the remote

git commit --amend --no-edit



# Credentials storing - set up storing creds in OSX
git config --global credential.helper store




#### Hooks - tell GIT to do stuff upon certain CLI actions ####

- in local repo, look under ".git/hooks" folder, you can create shell scripts for different things like pre-commit actions, post-commit actions, etc.
- husky - an npm package that makes implementing GIT hooks easier [go to Github repo and "npm install husky -D" locally to find out more]

#### Example ####

# Source: https://blog.mrhaki.com/2012/10/groovy-goodness-using-groovy-for-git.html

1] In your GIT bash terminal, create a file in your GIT repo:

	cd <your_local_repo>/.git/hooks
	touch post-commit

2] Copy and paste the code from https://github.com/checkswinga/devops-glossary/groovy/post-commit.groovy into your post-commit file
	- implements a hook to send a notification via Growl
	- code needs a bit of editing to change local references

3] Create another file in your hooks directory called "commit-msg"

4] Copy and paste the code from https://github.com/checkswinga/devops-glossary/groovy/commit-msg.groovy into your commit-msg file
	- implements a hook to stop a commit to the remote if the commit msg is empty




# Log - an advanced command for an easier look at git commit history locally

git log --graph --oneline --decorate


# Migrate repos from one remote to another
git clone --bare <repo_url>
cd <cloned_repo>.git
git push --mirror  https://github.com/<new-repo>


# Merge - locally merge the contents of some branch into your own branch

git checkout <target_branch>
git pull
git checkout <your_branch>
git pull
git merge <target_branch>



# Squash - take two or more commits in a branch and "squash" them together into a single commit
# See medium.com for more details - https://medium.com/@slamflipstrom/a-beginners-guide-to-squashing-commits-with-git-rebase-8185cf6e62ec

git rebase <branch_name> --interactive

	# the above opens a file that shows all of the commits for that branch
	# then you would "pick" the commit you want to use, and "squash" the commits you want to go into the picked commit.


# Squash automatically or auto-squash - tell GIT in advance that you will be squashing commits

git commit --fixup fb2f677       # fixup is like squash except it ignores the commit message when squashing occurs
git commit --squash fc2f55       # preps commit for squashing upon commit
git rebase -i --autosquash       



# Stash - used to set local changes aside for later

git stash        
git pop          # you can use stash and pop if you only plan to use stash for one set of changes to put away


# Stash - commands below are for saving multiple stashes

git stash save <name_of_stash>
git stash list                                  # list all stashes
git stash apply <index_of_stash_name>           # pop the selected stash from the index list


# Status - check your local repo to see added, deleted or modified files

git status -s       # show added/deleted/modified/renamed files in short list.
					# green status - ready to be commited
					# red status - unstaged. Running 'git commit' successfully will not commit a file with this status

	# types of status
		' ' = unmodified
		M = modified
		A = added
		D = deleted
		R = renamed
		C = copied
		U = updated but unmerged
		DD = two files together which are unmerged, and both deleted
		UU = two files unmerged and both modified. This usually happens when you attempt to merge two branches and the same file from both branches have conflicting lines between them


# yum install git - install newer version on CentOS7
yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm
yum install git



------
Github
------

#### Commands ####

curl -v -H "Authorization: token <your token>" https://github.cibcdevops.com/api/v3/repos/org/repo/issues     # Example of REST API call, pulling all issues for target repo
sudo reboot - reboot Linux server [which will ultimately reboot the Github service - note needs updating]
sudo systemctl start elasticsearch - restart search service
sudo systemctl status elasticsearch - check search services status


# Cool Stuff
# Source: Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o

- to use VSCode in a browser to work on a specific repo in Github, go to the repo you want to work on, and hit the "." key
	- you can use a terminal if you set up Github codespace, option can be found within the VSCode browser editor






------
GitOps
------

#### References ####

Tech Talk: ArgoCD + Tekton = Better GitOps - https://www.youtube.com/watch?v=nOtxRNQAKXA



---
gpg
---

- Linux/UNIX - GNU Privacy Guard - encryption and signing tool

# Sources:
# - boson.com CompTIA Linux+ labs
# - http://blog.chapagain.com.np/gpg-revoking-your-public-key-and-notifiying-key-server/

#### Commands ####

gpg --list-keys 					# list all keys from keyring

gpg --gen-key 						# generate pgp key pair [run with sudo or as root]
	- options
		- 1] RSA and RSA (default)
		- 2] DSA and Elgamal
		- 3] DSA [sign only]
		- 4] RSA [sign only]
	- key size: 2048 is default
	- key validity: 0 is default [infinite years]
	- Real Name: [type in a name for the key]
	- Email address: [type in email address]
	- Comments: [type in comments for key]
	- type 'o' to confirm
	- Enter passphrase

gpg --fingerprint [Real Name]   								 # check fingerprint, public key and UID of pgp key
gpg --export-secret-keys --armor admin > admin_private.asc 		 # take a backup of the key
gpg --export --armor admin > admin-pub.asc  					 # put public key into file to share

gpg --output revoke.asc --gen-revoke admin  					 # revoke a pgp key
gpg --import revoke.asc 										 # import pgp revocation certificate into keyring

gpg --keyserver pgp.something.com --search-keys admin  			 # search for pgp public key "admin" on remote server, which has been revoked locally
gpg --keyserver pgp.something.com --send-keys key-ID 			 # revoke pgp key on the remote server
 



#### files, dirs and configs ####

~/.gnupg/



---
GPT
---

#### General ####

- GPT - GUID Partition Table
	- the latest standard for defining the partitions on a hard disk
	- uses globally unique identifiers (GUID) to define the partition, and you can create theoretically unlimited partitions on the hard disk. 

- gdisk - note there are a lot more options for partition types with the gdisk utility than with fdisk [including creating partitions other than just Linux file system]


#### Commands ####

sudo gdisk /dev/sdb            # displays partition table scan results of /dev/sdb via gdisk utility
	l                          # type 'l' to see full list of partition types


# Example of gdisk menu

[root@li1961-156 ~]# gdisk /dev/sdb
GPT fdisk (gdisk) version 0.8.10

Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries.

Command (? for help): ?
b       back up GPT data to a file
c       change a partitions name
d       delete a partition
i       show detailed information on a partition
l       list known partition types
n       add a new partition
o       create a new empty GUID partition table (GPT)
p       print the partition table
q       quit without saving changes
r       recovery and transformation options (experts only)
s       sort partitions
t       change a partitions type code
v       verify disk
w       write table to disk and exit
x       extra functionality (experts only)
?       print this menu



----
grep
----

- Linux/UNIX - search for and print lines matching a pattern


#### Command examples ####

# grep

grep yum /etc/yum.conf               # Search for the word  "yum" in yum.conf
grep -n "old" /etc/yum.conf          # Search for the word "old" in yum.conf. Print the line number where the word is found.
grep -c "old" /etc/yum.conf          # Print the number of occurrences only of "old". Good (maybe better) alternative to wc -l
grep -o "y" /etc/yum.conf | wc -w    # Count the number of words that start with "y" in yum.conf
grep -v "yum" /etc/yum.conf          # NON-matching: find/print all lines that do not have the word "yum"
grep -n "^$" /etc/yum.conf           # List all blank lines with their line numbers


# egrep - extended grep

ls | egrep "yum" /etc/yum.conf       # egrep is needed when using pipe (|). egrep is deprecated, can also use grep -E
egrep -c '^1|01$' /etc/yum.conf      # Count number of lines that start with 1 and end in 01 in yum.conf
egrep 'Fedora|yum' /etc/yum.conf     # Search for both Fedora and yum simultaneously in yum.conf


# fgrep - fixed grep, does not interpret regex special characters

fgrep -c 'yum' /etc/yum.conf            # Will return same result as grep or egrep
fgrep -c 'yum|Fedora' /etc/yum.conf     # This will not return a result because it reads | literally


# multiple grep
grep "some_text" file.txt | grep "some_more_text" | egrep "this|that|theother"


### Misc ###

- GREP_COLORS - "https://linuxaria.com/pills/coloring-grep-to-easier-research"




------
groovy
------

# General
# Source: Derek Banas - https://www.youtube.com/watch?v=B98jc8hdu9g


# Example class

class GroovyTut {
	static void main(String[] args) {

		println("Hello World")

		def age = "Dog";
		age = 40;
		def name = "Derek";
		def multString = '''I am
		a String that goes on
		for many lines''';

		println("5 + 4 = " + (5 + 4));
		println("5 - 4 = " + (5 - 4));
		println("5 / 4 = " + (5.intdiv(4)));

		println("5.2 + 4.4 = " + (5.2.plus(4.4)));
		println("(3 + 2) * 5 = " + ((3 + 2) * 5));

		println("age++ = " + (age++));              				# prints 40, then increments
		println("++age = " + (++age));              				# prints 42 - increment the value of age then print the value [age being 41 at this line]

		println("Biggest Int " + Integer.MAX_VALUE);        # Biggest Int 2147483647
		println("Smallest Int " + Integer.MIN_VALUE);				# Smallest Int -2147483647

		println("Biggest Float " + Float.MAX_VALUE);        # Biggest Float 3.4028235E38

		println ('I am ${name}\n');													# Single quotes: string is taken literally, newline is not ignored. Output: I am name
		println ("I am ${name}\n");													# Double quotes: prints content of variable, newline is not ignored. Output: I am Derek
		println (multString);																# Triple quotes: allows to print on multiple lines

		println("3rd index of name " + name[3]);						# name[3] would resolve to 'r'
		println("Index of r " + name.indexOf('r'))					# name.indexOf('r') would resolve to '2'

		println("Derek == Derek " + ('Derek'.equalsIgnoreCase(Derek)));

	}

}


# Analysis of example 

- static - method or function that belongs to the class
- void - doesnt return anything when function runs
- main - main executable function that runs everytime you call the file this code is in
- String[] args - a string array called 'args' - pass in a bunch of numbers or strings, all of them would be stored in this array
- def - for declaring variables
- println - print whatever is in parenthesis to the screen/console

- arithmetic operations - see examples above [and more in video]
	- (Math.xxx(y)) - use Math class xxx function on a number y
		- eg. Math.abs() - absolute value of a number

- var.substring(x) - get substring of the contents of var starting with the x index
- var.split('x') - split everything in var with the occurrence of x [separated by commas]
- var.toList() - split var into a list of every character [separated by commas]




# Notes

- everything in groovy is an object


# Command-line

groovy groovytut.groovy       # execute groovytut.groovy script




----
grub
----

- Linux/UNIX - complete program for loading and managing the boot process


#### Commands ####

update-grub  						# update grub (after config update). Run with root.


#### files and configs ####

/etc/default/grub 					# contains boot settings. NOTE: CRITICAL file - any typos will cause system not to boot
	- GRUB_DEFAULT=0 				# value of 0 makes it so current OS is the one to boot to upon system restart on a dual boot system
	- GRUB_TIMEOUT=5  				# means boot up will wait 5 seconds before booting this OS

	- GRUB_DEFAULT=saved
	- GRUB_SAVEDEFAULT=true  		# means system will boot to last OS in the list. Both of these settings needs to be adjacent to each other in file

/boot/grub  						# grub boot directory
/boot/grub/grub.cfg 				# config that gets updated when you reload grub from changes made in /etc/default/grub



--------------------
gzip/bzip/xz and tar
--------------------

- Linux/UNIX - compress, archive or expand files


#### Commands ####

# gzip

gzip [file_or_dir] 					# create zip archive for file(s) or directory. Can include as many files as gzip can handle
gzip -l [file_or_dir].gz			# list files within gzip archive 
gzip -d [file_or_dir].gz 			# extract gzip file
gzip -9 [file_or_dir] 				# use highest compression. 0-9, 9 being the highest but the slowest to complete


# bzip
bzip2 [file_or_dir]                 # all gzip commands above work the same using bzip2


# xz - better compression

xz [file_or_dir]                 # all gzip commands above work the same using xz


# tar - tape archiver

tar czvf something.tar [file_or_dir]    	# create tar file and package in files and/or directories
tar czvf something.tar.gz [file_or_dir] 	# create tar gzip file and package in files and/or directories
tar xzvf something.tar.gz 					# decompress tar.gz

tar cjvf something.tar.bz2 [file_or_dir]  	# with j flag since we're creating a bz2 (bzip) 
tar tjvf something.tar.bz2 [file_or_dir]    # view contents of archive

tar cJvf something.tar.xz [file_or_dir]  	# create tar.xz archive (J flag)



----
head
----

- Linux/UNIX - show certain number of lines from a file (default: first 10 lines)


#### Commands ####

head /etc/yum.conf  					# show first 10 lines in yum.conf
head -n20 /etc/yum.conf 				# show first 20 lines in yum.conf
head -n5 /etc/yum.conf /etc/passwd 		# show first 5 lines in yum.conf and then passwd file




----
helm
----

Definition: a package manager for kubernetes, defines package specifications in the form of custom YAMLs called 'charts'

Directory structure:

folder/
	Chart.yaml     			# contains info about the chart
	LICENSE 			 			# OPTIONAL: license for the chart
	README.md 		 			# OPTIONAL: human readable readme
	values.yaml    			# Default config values for the chart
	values.schema.json  # OPTIONAL: impose a structure on the values file with a JSON schema
	charts/           	# A directory containing any charts upon which this chart depends
	crds/								# Custom Resource Definitions
	templates/					# A dir of templates when combined with values generate valid K8s manifest files
	templates/NOTES.txt # OPTIONAL: plan text file containing short usage notes  


#### NEEDS UPDATES ####

# Reference: https://www.youtube.com/watch?v=WugC_mbbiWU

- Items to update
	- Chart.yaml details
	- values.yaml details
	- requirements.yaml
	- templates folder details
	- NOTES.txt - for post deployment output (e.g. of instructions, what to do next, etc.)
	- helm create
	- helm create starter templates (helm create --starter)
	- configmap updates
	- custom config maps
	- helm hooks (e.g. post-install)
	- helm hook types
		- install
		- delete
		- upgrade
		- rollback
	- allow reuse of existing PVC for data
	- ingress config
	- helm test
	- namespacing helper templates
	- dependencies and conditional dependencies 
	- toYaml



-------
history
-------

- history
	- ![history_number] - run the command based on the number from the 'history' listing
	- HISTTIMEFORMAT="%Y-%m-%d %T "
		- history command will now show the date and time of previous command executions
		- does not work on ksh

- Ctrl+r - open reverse search. Start to type in command and it will autocomplete if it can find a previous command in the history




--------
hostname
--------

- Linux/UNIX - identifies the name/domain name of a system on a network



#### Commands ###

hostname            # displays hostname
hostname -s         # displays short version of hostname
hostname -f         # displays fully qualified domain name (FQDN)

cat /etc/hostname   # you can see the hostname configured here

hostnamectl         # view detailed info about the host/hostname

hostnamectl set-hostname plablinux2 			# change the hostname to plablinux2. Requires reboot of system




----
htop
----

# Sources:
# https://htop.dev/

- Linux/UNIX - a more advanced version of top command, an interactive process viewer




------
httpie
------

# Sources
- https://pypi.org/project/httpie/ - homepage, lots of examples

# on Ubuntu, the httpie CLI utility for interacting with websites via CLI
apt-get install python3-pip
pip3 install --upgrade httpie
http <website_URL>


### Examples ###

# Custom HTTP method, HTTP headers and JSON data:

http PUT pie.dev/put X-API-Token:123 name=John


# Submitting forms:

http -f POST pie.dev/post hello=World


# See the request that is being sent using one of the output options:

http -v pie.dev/get


# Build and print a request without sending it using offline mode:

http --offline pie.dev/post hello=offline


# Use GitHub API to post a comment on an issue with authentication:

http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body='HTTPie is awesome! :heart:'


# Upload a file using redirected input:

http pie.dev/post < files/data.json


# Download a file and save it via redirected output:

http pie.dev/image/png > image.png


# Download a file wget style:

http --download pie.dev/image/png




-------
hwclock
-------

- Linux/UNIX - displays/modifies the current time that the hardware clock on the computer shows


#### Commands ####

hwclock             							# display current time on hardware clock (same output with -r or --show flag)
hwclock -w 										# set the hwclock to the current system time
hwclock --set --date 12/19/2018  				# set the hwclock date manually
hwclock --set --date "12/20/2018 23:10:00"		# set the hwclock date and time manually
hwclock -s 										# copy hardware time to system time




------------
if/else/case
------------

#### shell scripting ####

# case statements - if some value, do this; if another value, do this instead

#!/bin/sh
CAR="Honda"
case "$CAR" in
   "Ferrari") echo "Ferrari is quite expansive."
   ;;
   "Jaguar") echo "I like Jaguar."
   ;;
   "Honda") echo "Honda is the car of the year."
   ;;
esac



# if statements (using elif and else)
# if - if a certain statement is true, do something
# elif - or if another statement is true, do somethin else
# else - otherwise if nothing else is true, do this

#!/bin/sh
x=10
z=20
if [ $x == $z ]
then
   echo "x is same as z"
elif [ $x -gt $z ]
then
   echo "x is more than z"
elif [ $x -lt $z ]
then
   echo "x is less than z"
else
   echo "None of them match"
fi



-----
iftop
-----

# Sources:
# iftop command in Linux with Examples - https://www.geeksforgeeks.org/iftop-command-in-linux-with-examples/

- Linux - a network analyzing tool used by system administrators to view the bandwidth related stats
		- shows a quick overview of the networking activities on an interface
		- stands from Interface TOP and the top is derived from op command in Linux
		- acts as a diagnostics to diagnose which program is causing the problem to the network


#### Installation ####
yum install epel-release 				# install pre-requisite epel-release repo
yum install iftop 						# for RHEL-based systems
OR
apt-get install iftop   				# for Ubuntu/Debian based. Must be superuser.



#### Commands ####

iftop  					# display basic bandwidth usage of the default interface
iftop -i eth0 			# display bandwidth details of a specific router
iftop -n -i eth0 		# stop hostname lookup
iftop -N -i eth0 		# stop the conversion of port number to services
iftop -b 				# stop bargraph display
iftop -t 				# stop text interface without ncurses (i.e. without the special text screen, will print the text to the terminal directly instead)
iftop -o source 		# sort output by source address
iftop -o destination  	# sort output by destination address
iftop -L 2 -i eth0 		# specify number of lines to be printed
iftop -h 				# display help



-------
ingress
-------

# Responsible for
	- accept/deny incoming requests
	- SSL termination
	- routing
	- load balancing


------
init.d
------

- /etc/init.d - contains a number of start/stop scripts for various services on your system.

- usage:
	/etc/init.d/[command] OPTION

	- where OPTION can be any of the following:
		- start
		- stop
		- reload
		- restart
		- force-reload


### related ####

cat /etc/inittab           # view runlevels of different programs. Is no longer used by systemd, which uses targets instead of runlevels




------
iostat
------

- Linux/UNIX - displays the CPU and input/output statistics for devices and partitions


#### Commands ####

iostat                     # display the CPU and input/output statistics for devices and partitions
iostat -c 			       # display only CPU utilization




-----
iotop
-----

# Sources:
# iotop Command in Linux with Examples - https://www.geeksforgeeks.org/iotop-command-in-linux-with-examples/

- Linux - used to display and monitor the disk IO usage details
		- gets a table of existing IO utilization by the process
		- designed in Python, needs kernel modules for execution
		- used by system administrators to trace the specific process that may be causing a high disk I/O read/writes
		- requires a python interpreter for its execution
		- produces output similar to that of top command
		- usually requires root privileges for its execution.


#### Installation ####

yum install iotop 				# on RHEL-based Linux
OR
apt-get install iotop 			# on Ubuntu/Debian-based. Must be root for both.



#### Commands ####

# Note: will usually require root to execute

iotop 				# To get the list of processes and their current disk IO usage.
iotop -o 			# show processes that are actually doing IO
iotop --version   	# get iotop version
iotop -h 			# display help section
iotop -b 			# display output in non interactive mode
iotop -n 3 			# change the number of iterations or updations
iotop -p 10989 		# display a specific process
iotop -a 			# show accumulated output
iotop -t 			# add a time stamp to each line
iotop -q 			# suppress some lines of header






--
ip
--

# Sources
	# https://linuxize.com/post/linux-ip-command/
	# https://linux.die.net/man/8/ip

- Linux/UNIX - show/manipulate routing, devices, policy routing and tunnels
			 - associated Linux package: iproute2


#### Commands ####

ip addr               					# shows all IP address associated with the device logged into
ip route 			  					# shows all network routes available to the device
ip route add 192.168.0.250 dev eth0  	# add route to a specific host (where dev = physical device, eth0 = network interface, likely main 											# ethernet card)
ip route del 192.168.0.250 dev eth0  	# delete route to a specific host
ip route add 192.168.0.0/24 dev eth0    # add network to routing table

ip link set dev {DEVICE} {up|down} 		# bring device online/offline



--------
ipconfig
--------

Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=ZKhorleA5aA

- ipconfig command line tool displays TCP/IP network configuration of the network adapters on a Windows computer


# command variations/switches

ipconfig /all                # displays full TCP/IP configuration of your network adapters (eg. additional info such as hostname, DHCP enabled, physical address, DHCP server address, etc.)
ipconfig /flushdns           # flushes the DNS resolver cache on the computer (side note: computers only understand numbers, not names, hence DNS translation)
	- DNS resolver cache contains a list of DNS names matched up with IPs that youve visited, so its faster on subsequent visits to those websites
		- this means less load on DNS server for lookups
	- reasons to flush DNS
		- if a website changes its IP and youre not able to resolve
		- if your cache has been hacked, redirecting you to malicious websites
			- helps hide website search behaviour

ipconfig /displaydns         # displays the contents of the DNS resolver cache
	- normally shows a bunch of records
	- Time To Live - amount of time before entry is erased from the cached





--------
IPTables
--------

# To block 116.10.191.* addresses:
sudo iptables -A INPUT -s 116.10.191.0/24 -j DROP

# To block 116.10.*.* addresses:
sudo iptables -A INPUT -s 116.10.0.0/16 -j DROP

# To block 116.*.*.* addresses:
sudo iptables -A INPUT -s 116.0.0.0/8 -j DROP

# Open a port (RHEL6)
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 5667 -j ACCEPT
service iptables save

# set rule for allowing incoming connections to port 22 from specific ip range
iptables -A INPUT -s 192.168.0.0/24 -p tcp --dport 22 -j ACCEPT



-----
istio
-----

Video source: Istio Service Mesh Explained - https://www.youtube.com/watch?v=KUHzxTCe5Uc

- istiod - control plane of the Istio service mesh, responsible for injecting sidecar proxies into pods (which happen when apps "opt-in" to the service mesh)
 - components
    - pilot - traffic management, injecting/managing lifecycle of sidecar proxies
    - citadel - certificate authority, helps achieve mutual TLS between services within the mesh
    - galley - translates kubernetes YAML into format for Istio to process


# install Istio

curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.12 TARGET_ARCH=x86_64 bash -
mv istio-1.6.12/bin/istioctl /usr/local/bin
chmod +x /usr/local/bin/istioctl
mv istio-1.6.12 /tmp/


# Pre-flight check for compatibility with a cluster (k8s api, version, if istio already installed, can setup within cluster, auto sidecar injector)

istioctl x precheck


# Istio profile check and install with default profile

istioctl profile list
istioctl install --set profile=default
kubectl -n istio-system get pods
istioctl proxy-status


- two ways to opt-in
 	- label a namespace (all pods within the namespace will join the service mesh)
		kubectl label namespace/default istio-injection=enabled    # takes effect for new pods, you'll see an additional container within each pod
 
	- use istioctl to grab the deployment YAML and inject the sidecar proxy using istioctl
		kubectl -n ingress-nginx get deploy nginx-ingress-controller -o yaml | istioctl kube-inject -f - | kubectl apply -f -    # takes effect for new pods
  

- add-ons
	- comes pre-shipped with a Grafana and Kiali dashboard add-ons for rich metrics/telemetry
	- to install Grafana (to the Istio namespace):
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/prometheus.yaml
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/grafana.yaml
		kubectl get pods --namespace istio-system

	- to expose Grafana dashboard
		kubectl -b istio-system port-forward svc/grafana 3000

    - see something wrong in Grafana? Command below to check logs within the cluster of pods throwing errors:
    	kubectl logs <pod_name> -c <container_name> --tail 50

    - need to buy dev some time to fix something? Implement virtual service in Istio (i.e. automated retries, canary deploys, traffic splitting)
    	- see https://www.youtube.com/watch?v=KUHzxTCe5Uc @ 26:10




-------
Jenkins
-------

# when login is broken
	- Stop Jenkins (the easiest way to do this is to kill the servlet container. Best way is to run a stop script, either a custom shell or stopping the service if its running that way)
	- Go to $JENKINS_HOME in the file system and find config.xml file.
	- Open this file in the editor.
	- Look for the false element in this file.
	- Replace true with false
	- Remove the elements authorizationStrategy and securityRealm
	- Start Jenkins


# Decrypt secret from Credentials

- In your Jenkins instance
	- go to Credentials -> Your_secret -> Global credentials (unrestricted) -> Update (Your_secret)
	- right click inside "Secret" field -> Inspect
	- copy the value inside the _.secret input name entry (e.g. should be like this: {AQAAABAAAAAwJABqfeWZl6KviKrG/uq1JcSwL1ru1/82OSGQkS3JreDRp54r7fBoAwEiO/+D8uCASaD1UTH0GAyet8z+YZIzZQ==} )
    - open a new tab and go to: <your Jenkins instance URL>/script
	- Paste the following script with your secret value into the console and click Run:
		- println hudson.util.Secret.decrypt("{AQAAABAAAAAwJABqfeWZl6KviKrG/uq1JcSwL1ru1/82OSGQkS3JreDRp54r7fBoAwEiO/+D8uCASaD1UTH0GAyet8z+YZIzZQ==}")
	- you will see the decrypted value printed if it ran successfully



----
join
----

- Linux/UNIX - join lines of two files on a common field


#### Commands ####

cat file1.txt 			# displaying the contents of first file
1 AAYUSH
2 APAAR
3 HEMANT
4 KARTIK
5 DEEPAK

cat file2.txt 			# displaying contents of second file
1 101
2 102
3 103
4 104

join file1.txt file2.txt   # join the two files together
1 AAYUSH 101
2 APAAR 102
3 HEMANT 103
4 KARTIK 104


join file1.txt file2.txt -a 1   # print unpairable lines from the first file (put 2 for the second file)

output:
1 AAYUSH 101
2 APAAR 102
3 HEMANT 103
4 KARTIK 104
5 DEEPAK

join file1.txt file2.txt -v 1    # print ONLY unpairable lines

output:
5 DEEPAK


join -1 1 -2 1 file1.txt file2.txt      
OR
join -j1 file1.txt file2.txt 			# join file1 and file2 based on their common field (in this case, column 1)

output:
1 AAYUSH 101
2 AAPAR 102
3 HEMANT 103
4 KARTIK 104




---
k9s
---

# Github repo: https://github.com/derailed/k9s
# Learning sources: Marcel Dempers - That DevOps Guy: https://www.youtube.com/watch?v=qqR0c-qcILI

- an enhanced kubernetes CLI tool
- provides a terminal UI to interact with your Kubernetes clusters 





-----
kafka
-----

# Introduction
# Source: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=heR3I3Wxgro
#																					 - https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/messaging/kafka  [source code + README with instructions]

- Apache Kafka - an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration,
								 and mission-critical applications.

- zookeeper - centralized service for maintaining configuration information [naming and providing distributed synchronization for apache services such as kafka]
						- keeps track of status of kafka cluster nodes
						- also keeps track of partitions and topics


- logistics
	- client [producer] ----> kafka broker	----> application server[s]

		- clients are referred to as "producers"
		- messages go to "brokers" which are kafka instances

		- servers that consume messages are called "consumers"
			- consumer can subscribe to a topic and receive messages, in order, indexing what it has received so far
				- if a consumer dies/shuts down, once its back online it can use the index number to pick up consuming messages from the index it left off on

		- messages are stored on the brokers in "topics"
			- topics can be divided into partitions, messages/copies of messages go into the partitions
				- if a broker dies, messages are not lost as a result of this replication

		- in source vid and repo, the example for testing will make use of pre-packed scripts that come with the Kafka installation to mock producers and consumers for testing purposes


 - testing

	- the Github repo above has a Dockerfile to build a Debian/Java11 container with Kafka installed [see dockerfile under messaging/kafka folder]
 		- can spin up multiple instances of this image for having more than one broker to replicate topics/partitions/messages
 			- minor configuration updates requires for server.properties, such as broker ID, which has to be unique for each container
 		- for zookeeper, its the same dockerfile except you use the start-zookeeper.sh script as the entrypoint. For testing, one zookeeper container is sufficient
 			- see zookeeper dockerfile and start script under messaging/kafka/zookeeper




----------
kubernetes
----------


# General

- control nodes and control plane - the administrative node[s] of a cluster which instruct and manage the worker nodes which run workloads

	- kube-api server - main component which communicates instructions to/from all cluster components, and validates/configures data for api objects such as pods, services, RCs, etc.
	- etcd - key/value store for a cluster
	- scheduler - responsible for telling api server where a pod needs to be scheduled and when
	- kubelet - the agent which sits on the worker node and communicates with the api server
	- kube-proxy - intercommunication mechanism in a cluster between nodes
	- controller-manager - responsible for telling the api server to match the desired state with the actual state of pods within a cluster
	- daemonset - ensures that all or some nodes run a copy of a pod [e.g. kube-proxy daemonset would ensure kube-proxy pod runs on every available node in the cluster]



# Installing a Kubernetes cluster using kubeadm [Centos7]
# Source: Just Me And Opensource - https://www.youtube.com/watch?v=v6Jh2sZClDY
#								 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-centos-7.md
# 								 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-ubuntu-20.md [for install on Ubuntu 20.04]         


For Centos 7 install:

On both master and worker,

1. Disable firewalld

systemctl disable firewalld; systemctl stop firewalld


2. Disable swap

swapoff -a; sed -i '/swap/d' /etc/fstab


3. Disable SELinux

setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux


4. Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


5. Install docker  #### note will have to check how this works for containerd since docker is deprecated in K8s as of v1.21

yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-19.03.12 
systemctl enable --now docker


6. Add yum repo for Kubernetes install

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


7. Install Kubernetes

yum install -y kubeadm-1.18.5-0 kubelet-1.18.5-0 kubectl-1.18.5-0


8. Enable and start kubelet service

systemctl enable --now kubelet



On master node,

9. Initialize Kubernetes cluster

kubeadm init --apiserver-advertise-address=172.16.16.100 --pod-network-cidr=192.168.0.0/16


10. Deploy Calico network

kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml


11. Cluster join command [generating token for worker nodes]

kubeadm token create --print-join-command


Side note: if you want to be able to run kubectl commands as non-root user, then as a non-root user perform these:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


12. On the worker node[s], use the output from the "kubeadm token create" command and run it.


13. Verify cluster

kubectl get nodes
kubectl get cs         # get component status



####### kind - a software for running kubernetes clusters within docker containers ########
# Sources
- https://www.youtube.com/watch?v=m-IlbCgSzkc&list=WL&index=1

# Prerequisites
- docker
- go (1.11+)
- kubectl

# Install kind
- can download binary from Github, place in any PATH (e.g. /usr/local/bin/kind)

1] Create kind kubernetes cluster

kind create cluster --name <cluster_name>
	- this will pull the kind docker image to spin up for creating a single master node cluster

kind get clusters
	- show running clusters

kind delete cluster
	- to remove the created cluster

2] Check running nodes
kubectl get nodes

3] set kubeconfig to point to your new cluster (can run more than once to add clusters)
export KUBECONFIG="$(kind get kubeconfig-path --name="<cluster_name>"


# Multi-node cluster - its possible to create a kind cluster with worker nodes added

1] create a yaml with the following def
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker


2] create cluster with this yaml definition

kind create cluster --config /path/to/kind.yaml


3] verify

kind get clusters
docker ps 					                   # will show running kind containers, one which is your master/control plane as in the first example, and one or more as your worker nodes
kubectl cluster-info                   # show info about cluster, including the URL to hit it
kubectl get nodes -o wide  					   # see the nodes not as docker containers but as kind nodes

docker exec -ti <container_ID> bash    # log into any of the kind containers
which ctr 														 # container runtime binary similar to docker command line
ctr namespaces list  									 # can see namespaces within the cluster
ctr namespace k8s.io containers list   # can see all the containers that make up the cluster



########## k0S - a frictionless, easy-to-install-and-use kubernetes distro ##########

# General website - https://k0sproject.io/
# Getting started steps - https://docs.k0sproject.io/v1.22.2+k0s.0/install/
# Youtube getting started guide from Just Me and Opensource - https://www.youtube.com/watch?v=aHPGf6FsY7Y



########## kubectl ##########


# Random useful kubectl commands

kubectl expose deploy <pod> --port <port> --dry-run -o yaml > somefile.yaml    # Expose a deployment as a service, dry run to check yaml definition, and can output to a file
kubectl -n <namespace> port-forward <pod_name> <port> 			               # Port forward to expose an app as a service
kubectl scale deploy <deployment_name> --replicas=<number>                     # Manually scale a deployment
kubectl rollout restart sts/<sts_name>                                         # Rolling restart of stateful set pods of name sts_name
kubectl rollout restart deployments/<deploy_name>                              # Rolling restart of deployments of name deploy_name
kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod  # Delete all pods in namespace which are in Evicted state




########## kURL - a custom Kubernetes distro creator ############

# Website - https://kurl.sh/

# Demo for installation: Just Me and Opensource - https://www.youtube.com/watch?v=_4edisHDWzs

- the kURL tool helps you to create a Kubernetes cluster which can be customized off the bat with different tools that are more typical/standard that are not provided out of the box right away
	with a vanilla K8s installation such as monitoring tools [Prometheus], cluster backup and restore mechanism [Velero], certificate management [CertManager], and more. See kurl.sh for more details.






#### Webhooks ####
# Source - Marcel Dempers (That DevOps Guy) - https://www.youtube.com/watch?v=1mNYSn2KMZk
#											- https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/kubernetes/admissioncontrollers/introduction


- admission webhooks
	- types
		- mutating - intercepts object/YAML before it hits API server, allows us to make changes first
			- egs. inject CPU limits, labels, ulimits, go to a specific node, etc.

		- validation - accept or reject request
			- egs.
				- policy enforcement - if CPU/mem limits not set, do not create pod
														 - only allow pods if the image is part of a specific image registry
				- notfiications when events occur


- steps to create a webhook from scratch

	1. Create a K8s cluster [can use kind or k3s for ease]

	2. Create a TLS certificate for the webhook [can use Cloudflare SSL utility as shown below, the swiss army knife for dealing with TLS certs]

		cd kubernetes/admissioncontrollers/introduction      # within the Github repo in the source above
		docker run -it --rm -v ${PWD}}:/work -w /work debian bash


		# a. Install Cloudflare SSL and json utilities

		apt-get update && apt-get install -y curl &&
		curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl && \
		curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson && \
		chmod +x /usr/local/bin/cfssl && \
		chmod +x /usr/local/bin/cfssljson


		# b. Generate a CA certificate in /tmp
		cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca


		# c. Generate webhook certificate

    cfssl gencert \
    	-ca=/tmp/ca.pem \
    	-ca-key=/tmp/ca-key.pem \
    	-config=./tls/ca-config.json \
  	  -hostname="example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1" \
  		-profile=default \
  		./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook



		# d. Make a secret

cat <<EOF > ./tls/example-webhook-tls.yaml
apiVersion: v1
kind: Secret
metadata:
	name: example-webhook-tls
type: Opaque
data:
	tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\n')
  tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\n') 
EOF


		# e. Generate CA Bundle + inject into template
		
		ca_pem_b64="$(openssl base64 -A <"/tmp/ca.pem")"         # where -A means no line breaks so the full certificate string can be stored in the variable

		sed -e 's@${CA_PEM_B64}@'"$ca_pem_b64"'@g' <"webhook-template.yaml" \
    > webhook.yaml


  3. Create the webhook with the generated CA

  # a. YAML templated definition - see https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/kubernetes/admissioncontrollers/introduction/webhook-template.yaml

  apiVersion: admissionregistration.k8s.io/v1
  kind: MutatingWebhookConfiguration
  metadata:
    name: example-webhook
  webhooks:
    - name: example-webhook.default.svc.cluster.local
      admissionReviewVersions:
        - "v1beta1"
      sideEffects: "None"
      timeoutSeconds: 30
      objectSelector:
        matchLabels:
          example-webhook-enabled: "true"
      clientConfig:
        service:
          name: example-webhook
          namespace: default
          path: "/mutate"
        caBundle: "${CA_PEM_B64}"
      rules:
        - operations: [ "CREATE" ]
          apiGroups: [""]
          apiVersions: ["v1"]
          resources: ["pods"]

   # End YAML
 
     - note the CA_PEM_B64 variable, referencing step 2e. Running that will replace the variable with the actual value in webhook.yaml

   #### INCOMPLETE ####



----------
ldapsearch
----------

# search for AD ID "ad_username" within active directory

ldapsearch -vvv -x -H ldap://ad.example.com:389 -b "dc=ad,dc=example,dc=com" -D "ad_username@ad.example.com" -W "sAMAccountName=ad_username"



---
ldd
---

# Linux - checking library dependencies
ldd -d 



--------------
Linux [system]
--------------

#### Command-line - special/"heroic" commands ####
# Source: Engineer Man - https://www.youtube.com/watch?v=Zuwa8zlfXSY

# 1. redo last command but as root
sudo !!

# 2. open an editor to run a command
ctrl+x+e

# 3. create a super fast ram disk
mkdir -p /mnt/ram
mount -t tmpfs tmpfs /mnt/ram -o size=8192M

# 4. don't add command to history (note the leading space)
 ls -l

# 5. fix a really long command that you messed up
fc

# 6. tunnel with ssh (local port 3337 -> remote host's 127.0.0.1 on port 6379)
ssh -L 3337:127.0.0.1:6379 root@emkc.org -N

# 7. quickly create folders
mkdir -p folder/{sub1,sub2}/{sub1,sub2,sub3}

# 8. intercept stdout and log to file
cat file | tee -a log | cat > /dev/null

# bonus: exit terminal but leave all processes running
disown -a && exit



#### Disadvantages (which are all manageable/solvable of Linux ####

# Source: The Linux Experiment - 6 Things I HATE about using LINUX: https://www.youtube.com/watch?v=pCPDWQ5yGFQ

- Different packaging formats
- limited hardware support + expense
- Linux Desktop seen as a niche market [and not very popular in general]
- Bad battery life on Linux laptops





#### Kernel commands ####

lsmod       # shows the kernel modules loaded into the kernel

modinfo     								# shows information about a specific kernel module
modinfo -d [kernel_module_name]     		# prints generic info about the module
modinfo -a [kernel_module_name]     		# prints the author of the module
modinfo -n [kernel_module_name]     		# prints location of the module
modinfo -F depends [kernel_module_name]     # prints dependencies of the target module

rmmod [kernel_module_name]                  # removes target module from the kernel
modprobe [kernel_module_name]               # add module to the running kernel. Note the module should exist under /lib/modules/`uname -r`



#### Kernel - General info ####
#### Sources: Engineer Man - https://www.youtube.com/watch?v=CWihl19mJig

- privilege rings in Linux
	- Ring 3 - most outer ring
			 - least privilege
			 - user space/applications - cannot talk directly to hardware

	- Ring 2 - between Ring 3 and 1
			 - device drivers
			 - more privilege than Ring 3

	- Ring 1 - device drivers [pseudo hardware]

	- Ring 0 - the kernel space
			 - inner most ring
			 - most privilege
			 - kernel modules/apps connect user space applications to physical and pseudo hardware
			 - kernel programs [unlike user space applications] run reactionally, responding to events [such as user applications]

# How to build a Linux loadable kernel module [that Rickrolls people]
# Source: Engineer Man - https://www.youtube.com/watch?v=CWihl19mJig


1] Re-creating the example of kernel module called "rickroll.c", which prints a message when the module has been loaded and unloaded

# START
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>

static int __init rickroll_init(void) {
	printk(KERN_INFO "Rickroll module has been loaded\n");
	return 0;
}

static void __exit rickroll_exit(void) {
	printk(KERN_INFO "Rickroll module has been unloaded\n");
}


module_init(rickroll_init);
module_exit(rickroll_exit);

# END


2] Create a Makefile

obj-m += rickroll.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules

clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean



3] Build the module

make     	

- this will create a bunch of files
	- modules.order
	- Module.symvers
	- rickroll.c
	- rickroll.ko     # ko stands for "kernel object"
	- rickroll.mod.c
	- rickroll.mod.o
	- rickroll.o

4] Tail the kernel log in a different terminal

tail -f /var/log/kern.log


5] Inject the kernel module

sudo insmod rickroll.ko 

- the log should print that the module has been loaded


6] Test removing the module

sudo rmmod rickroll

- the log should print that the module has been removed


7] Expand the module to build a device to "Rickroll" people if they read from the device

# Updated module code below for rickroll.c

#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/uaccess.h>

#define DEVICE_NAME "rickroll"

static int dev_open(struct inode*, struct file*);
static int dev_release(struct inode*, struct file*);
static ssize_t dev_read(struct file*, char*, size_t, loff_t*);
static ssize_t dev_write(struct file*, const char*, size_t, loff_t*);

static struct file_operations fops = {
	.open = dev_open,
	.read = dev_read,
	.write = dev_write,
	.release = dev_release,
};

static int major;


static int __init rickroll_init(void) {
	major = register_chrdev(0, DEVICE_NAME, &fops);

	if (major < 0) {
		printk(KERN_ALERT "Rickroll load has failed\n");
		return major;
	}

	printk(KERN_INFO "Rickroll module has been loaded\n");
	return 0;
}


static void __exit rickroll_exit(void) {
	unregister_chrdev(major, DEVICE_NAME);
	printk(KERN_INFO "Rickroll module has been unloaded\n");
}


static int dev_open(struct inode *inodep, struct file *filep) {
	printk(KERN_INFO "Rickroll device opened\n");
	return 0;
}


static ssize_t dev_write(struct file *filep, const char *buffer, size_t len, loff_t *offset) {
	printl(KERN_INFO "Sorry, rickroll is read only\n");
	return -EFAULT;
}


static int dev_release(struct inode *inodep, struct file filep*) {
	printk(KERN_INFO "Rickroll device closed");
	return 0;
}


static ssize_t dev_read(struct file *filep, char *buffer, size_t len, loff_t *offset) {
	int errors = 0;
	char *message = "never gonna give you up, never gonna let you down...;      # missing end double quote on purpose, make sure to add back
	int message_len = strlen(message);

	errors = copy_to_user(buffer, message, message_len);

	return errors == 0 ? message_len : -EFAULT;
}

module_init(rickroll_init);
module_exit(rickroll_exit);

# END 


- Notes on updated rickroll.c
	- dev_open - when the device is open
	- dev_release - when the device is released or closed
	- dev_read - when the device is read from
	- dev_write - when the device is written to
	- register_chrdev - chrdev stands for character device
		- the 0 in this function means dynamically generate a major number
		- the major number is a small integer that serves as the index into a static array of char driver
			- i.e. every device has a major number associated with it
	- return 0 if successful
	- char *buffer is from user space to copy error message into
	- on the return errors line, the ? message_len mean to return it if no errors, or return -EFAULT if neither are true

 #### SOME MORE INFO NEEDED ON OTHER FUNCTIONS IN THE CODE ####


8] Repeat step 5 and make note of the major number from the log

9] Register the new "device" [into /dev directory] and point it to the rickroll module

cd /dev
sudo mknod rickroll c <major_number> 0
ls -l | grep rick


10] Test the new "device"

cat /dev/rickroll

- should print the char *message a lot
- Ctrl ^C to close the device, and the log should print that the device is closed



#### Processes ####

# Sources
	# Engineer Man - https://www.youtube.com/watch?v=TJzltwv7jJs
	# boson.com CompTIA Linux+ labs

- fork/exec - when executing any command
				- Linux "forks" the shell terminal [or parent] process into two: itself and a copy of itself.
				- then it "execs" by replacing the second [child] process with the command that was entered


#### process commands ####

^C [or Ctrl ^C] - if the child process is still running [i.e. your terminal is still executing a command], this will send a SIGINT to terminate the process
^Z [or Ctrl ^Z] - sends a TSTP signal to Linux to "suspend" the process [equivalent of "kill -20" on a process]


# jobs

jobs        	  # see the jobs table for the user (background processes). You will also see stopped processes that can be restarted.


# fg

fg          	  # execute in the foreground - if there's one stopped job in the jobs table, this will run it by default
fg %3       	  # this will run the 3rd job in the jobs table
fg 2  			  # this will bring the 2nd job in the jobs list (background) to the foreground


# bg

bg                # background
bg %3             # this will run the 3rd job in the jobs table in the background


# kill

kill -15 [pid]    # sends sigterm to terminate the process cleanly
kill -2 [pid]     # sends sigint to terminate process 
kill -1 [pid]     # sends sighup to terminate process
kill -9 [pid]     # sends sigkill to kill the process ungracefully or the "dirty" way
kill -20 [pid]    # stop or suspend a running process
kill -18 [pid]    # restart a suspended process. An example of how kill sends terminal signals and not necessarily all process killers


# pidof

pidof firefox          # lists all process IDs containing firefox in the command


# killall

killall -9 firefox     # kill all processes which contain firefox in the command


# ps (see ps section)



### Reboot commands ###

init 6                  # Reboots and starts back up to run level 6
systemctl reboot        # Reboots via systemctl
shutdown -r 			# -r flag denotes reboot



---------
localtime 
---------

# change time zone

rm -f /etc/localtime
ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime
date



------
locate
------

- (Linux/UNIX) - searches the location of files from a specific database
			   - e.g. it can display all the directies on the system that has a certain name as part of the filepath

#### commands ####

locate /etc            		# find all directories that has /etc in the filepath
locate -S              		# show the database that locate uses to find files and directories
				       		# e.g. /var/lib/mlocate/mlocate.db
				       		# some db names show as mlocate or slocate, depending on the Linux distro

updatedb               		# updates the locate database
cat /etc/updatedb.conf      # show contents of updatedb conf file





--
ln
--

- in Linux, creates a hard or soft link to an existing file or directory
- Symbolic links: It is a pointer to the source file. It can point to a source file on the local or remote filesystem.
- Hard links: It is another directory entry for the source file and carries those same properties, such as file permissions, of the source file.
			  If you delete one file, the other file remains intact. A hard link must exist in the same local filesystem.


#### Commands ####

ln -s <real_path> <soft_link_to_be_created>
ln <real_path> <hard_link_to_be_created>



-----
loops
-----

#### shell scripting ####

# for loop (example) - iterating through all files under /etc/ directory and printing to stdout

#!/bin/bash
echo "Files in the /etc directory are:"
for f in $(ls /etc/*)
do
  echo $f
done


# until loop - do something until certain criteria is met

#!/bin/sh
a=1
until [ $a -gt 6 ]
do
  echo $a
  a=$(( a+1 ))
done


# while loop - do something while something is true

#!/bin/bash
a=0
while [ $a -le 10 ]
do
  echo "Welcome $a times."
  a=$(( a+1 ))  
done



--
ls
--

- (Linux/UNIX) - list files in current directory


#### Commands ####

ls [!Aa]*         # list all files that don't start with an 'a' or 'A'
ls D[eo]*         # list all files that start with 'D' and with second letter 'e' or 'o'
ls -la 			  # list all files (with details) in current dir + files/dirs with a '.' in front of them
ls -latr 		  # same as above except sort by time (latest will show up at bottom of result)



--------
lscolors
--------

# Colourize text in terminal:
export LSCOLORS=gxBxhxDxfxhxhxhxhxcxcx
eval "$(dircolors /etc/DIR_COLORS)"




----
lsof
----

lsof <filename>       # check which processes have this file open
lsof -p <PID>         # check a specific pids open files [could be multiple]
lsof -u <userid>      # check list of open files for specific user
lsof -i <port>        # check which processes are listening on specific port
lsof -i <protocol>    # check which processes are listening on specific protocol [i.e. tcp]



---
Lua
---

# Source: Fireship - https://www.youtube.com/watch?v=jUuqBZwwkQw

- Lua is a lightweight dynamic scripting language often embedded into other programs like World of Warcraft and Roblox.
- Its minimal syntax makes it easier to learn than Python, while being much more performant than other interpreted languages. 


	

---
lxd
---

# Source: https://linuxcontainers.org/

- a next generation system container and virtual machine manager
- the lxc cli that lxd uses should not be confused with lxc itself [which is OS-level virtualization for running multiple Linux containers on a single host]. lxd is really just
  an alternative to lxc, and makes use of lxc tools via liblxc, such as the cli
	


# Commands - the below are lxc cli commands that can be executed within an lxd environment
# Can also try online without installing locally as well at https://linuxcontainers.org/lxd/try-it/


# Basics for listing and running container images

lxc image list        							      # list images stored locally (docker equivalent command: docker images)
lxc image list ubuntu: | less  			    		  # list of images from the ubuntu: registry
lxc launch images:ubuntu/18.04 [container_name]       # run a container called container_name based on the ubuntu 18.04 image
lxc list  											  # check running containers
lxc info [container_name]               			  # inspect running container named container_name
lxc config show [container_name]					  # show configuration details of container_name
lxc stop [container name] 							  # stop running container container_name
lxc delete [container_name]  						  # remove running container container_name
lxc start [container_name]							  # start running container container_name
lxc delete --force [container_name]                   # force stop and delete container_name


# Container interaction commands

lxc exec [container_name] -- free -m                	        # execute free -m within container_name
lxc config set [container_name] limits.memory 128MB   			# set memory limit on container_name
lxc exec [container_name] -- apt-get update           			# run apt-get update within container_name
lxc exec [container_name] -- apt-get install sl -y    			# install sl package within container_name
lxc exec [container_name] -- /usr/games/sl            			# run sl program within container_name [sl stands for 'Steam Locomotive' - displays a train on-screen when you mistype ls command :D ]
lxc snapshot [container_name] clean  							# take a snapshot called clean of running container_name, which can be used for restoration if something messes up in the container
lxc restore [container_name] clean  							# restore the clean snapshot of container_name
lxc file pull [container_name]/etc/hosts .            			# copy a file from container_name to local
lxc file push hosts [container_name]/etc/hosts        			# copy a file from local to container_name
lxc file pull [container_name]/var/log/syslog - | less          # check log files in the container
lxc exec lxdserver:[container_3] bash  				 			# log into remote container on remote lxdserver
lxc copy lxdserver:[container_3] lxdserver:[container_4]        # create a copy of that container on that same remote
lxc move lxdserver:[container_4] [container_5]                  # move the copied remote container to local and rename it to container_5
lxc start [container_5]                                         # and then start container_5


# Image commands

lxc publish [container_name]/clean --alias clean-ubuntu     	# publish the snapshot "clean" of container_name to the default registry
lxc launch clean-ubuntu [container_2]							# launch a new container based on the published image
lxc remote list                                                 # list all remote registries
lxc list lxdserver:  											# list all remote containers on lxdserver
lxc image list lxdserver:	 									# list all images on remote lxdserver
lxc launch clean-ubuntu lxdserver:[container_3]       			# create new container called container_3 on remote lxdserver
lxc image delete clean-ubuntu                                   # delete the published image



---------
Mac OS X
---------

   # check RAM:
   		system_profiler SPHardwareDataType | grep "Memory:"

   # check # of CPU cores:
   		sysctl -a | grep cpu.core_count

   # check GPU:
	 	  system_profiler SPDisplaysDataType

   # stop and start Jenkins
   		sudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist
		  sudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plist

   # update Jenkins URL
   	    sudo defaults write /Library/Preferences/org.jenkins-ci httpPort 8082
   	    sudo defaults write /Library/Preferences/org.jenkins-ci prefix /jenkins
   	    # for brew:
   	       /usr/local/Cellar/jenkins/2.x.x/homebrew.mxcl.jenkins.plist
   	       		update:
   	       			  <string>--httpPort=8082</string>
  					  <string>--prefix=/jenkins</string>
   	       brew services restart jenkins-lts

   # update Jenkins heap size/permGen
        sudo defaults write /Library/Preferences/org.jenkins-ci minPermGen 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci permGen 2048m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci minHeapSize 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci heapSize 2048m


   # check installed java versions:
   		  /usr/libexec/java_home -V



--
mc
--

- midnight commander - graphical file manager for UNIX/Linux OS.


# Install

yum install mc      # RHEL-based OS'


# Navigating the GUI - see https://linuxcommand.org/lc3_adv_mc.php

- type 'mc' and ENTER to enter mc graphical interface
- can still type out UNIX/Linux commands and at the same time, use mouse to click directly on files in the directory you are in
- click on file and press 'F3' to view contents 
- for an RPM file, "double click" on RPM, click the cpio file and press "F3" to view the contents



-----
mkdir
-----

- Linux: creates a directory

mkdir -p /var/www/html/repos/{base,centosplus,extras,updates}      # create directories under var (-p flag creates if any directories in command do not exist).
																   # base, centosplus, extras and updates will all be created under repos directory




-----
mount
-----


#### General command line in Linux/UNIX ####

mount               # shows all partitions on a hard disk (possibly all hard disks attached to a Linux server?) and their mount points
umount [fs_path]    # unmount a file system e.g. umount /dev/sdb1


#### fstab ####
cat /etc/fstab      # show the list of partitions detected at boot time. Also shows fs which is mounted on the root partition or /

- format of fstab: <file system>   <dir>   <type>  <options>   <dump>  <pass>
			   eg. UUID="14314872-abd5-24e7-a850-db36fab2c6a1" /lpo/sda ext4 defaults,noatime 0 0


#### RAM disk creation ####
# Source: Engineer Man - https://www.youtube.com/watch?v=Zuwa8zlfXSY

mkdir -p /mnt/ram
mount -t tmpfs tmpfs /mnt/ram -o size=8192M         # mount tmpfs device onto /mnt/ram directory, which will create a new FS
cd /mnt/ram
dd if=/dev/zero of=test.iso bs=1M count=8000

	- "dd" copies a file and converts the data.
		- In the case above, its being used to test the write speed on the new RAM disk
	- "if" - input file
	- "of" - output file
	- "bs" - block size
	- "/dev/zero" - produces a continuous stream of NULL (zero value) bytes.



##### Volume mounting in Linux #####
# Source article: https://www.linode.com/docs/platform/block-storage/how-to-use-block-storage-with-your-linode/#add-a-volume-from-the-linode-detail-page

1] One-time step for defining the FS type of the volume
mkfs.ext4 $volume_path   # check the file system path of the volume in your cloud console, it will show you the device drive location


2] Steps for mounting the volume
mkdir /mnt/my-volume
mount $volume_path /mnt/my-volume
df -h


3] Create entry in /etc/fstab:
FILE_SYSTEM_PATH /mnt/my-volume ext4 defaults,noatime,nofail 0 2

* noatime - This will save space and time by preventing writes made to the filesystem for data being read on the volume.
* nofail - If the volume is not attached, this will allow your server to boot/reboot normally without hanging at dependency failures if the volume is not attached.


4] if you need to unmount it or reboot your Linode without affecting the volume
umount /mnt/my-volume
Remove /etc/fstab entry



#### Re-mount existing volume after fstab update ####

- If you add an entry in fstab for an existing volume that''s already mounted, ensure you unmount the volume after adding to fstab.

1] Entry to add in fstab:

/dev/sdb1  /home  ext4  defaults,usrquota  1  1


2] Commands to remount the FS

umount /dev/sdb1
df -h
mount /dev/sdb1
mount -o remount /path       # remount the file system path previously mounted before fstab update. E.g. "mount -o remount /home" if its part of /dev/sdb1


NOTE: see quotacheck section for managing disk quotas



------
mpstat
------

- Linux/UNIX - displays CPU information. Useful when you have a system with multiple CPUs.


#### Commands ####

mpstat               # shows consolidated cpu usage (of all cpus in the same line)
mpstat -P ALL  		 # shows consolidated + individual cpu usage




----
nano
----

# Sources:
# nano documentation - https://www.nano-editor.org/docs.php

- Linux/UNIX - a text editor [licensed under GNU]






----
ncdu
----

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI

- Linux - a tool for checking disk usage in a more efficient and consolidated way than du


#### Commands ####

ncdu
	# scans the directory you launched it from [and all subdirectories]
	# provides you with a breakdown of where all your storage is being spent [sorted from largest to smallest]
	# you can drill down and immediately see the largest subdirectories/files
	# hit 'd' to select deleting whatever is highlighted, followed by yes to delete




--------
netgroup
--------

# Check netgroup list (Linux):
> ldaplist -l netgroup <netgroup_name> 



---------
netcat/nc
---------

### SOURCE page: https://www.linode.com/docs/guides/netcat/ ###


# make netcat act as the telnet utility (TCP protocol is the default, use -u for UDP procotol)

nc localhost 22


# make netcat act as a server, accept incoming connection on a given port

nc -l -p 1234


# get more info from remote server (eg. for connectivity issues)

nc -v localhost 1234  [or nc -vv]


# port scanning (eg. on localhost, from ports 1-30)

nc -z -vv -n 127.0.0.1 1-30


# transferring files (a client connects to port 4567 below to receive the access.log contents)

cat access.log | nc -vv -l -p 4567

# and then

nc -vv localhost 4567 > fileToGet
^C [to close the connection]


# turn a process into a server (when client connects, nc will execute /bin/bash, to give shell access to machine)

nc -vv -l -p 12345 -e /bin/bash


# executing a command after connecting

nc -vv -c "ls -l" -l 127.0.0.1 -p 1234


# act as simple web server (eg. can use curl or wget to connect to this port to get the index.html contents)

nc -vv -l 127.0.0.1 -p 4567 < index.html
wget -qO- http://localhost:4567/


# get data from web servers

nc www.linode.com 80

# OR

echo -en "GET / HTTP/1.0\n\n\n" | netcat www.linode.com 80



# Create a chat server

nc -vv -l 127.0.0.1 -p 1234

# and from the client

nc -vv 127.0.0.1 1234

# then type in terminal




-------
nethogs
-------

# Sources
# Nethogs – Monitor Linux Network Traffic Usage Per Process
# https://www.tecmint.com/nethogs-monitor-per-process-network-bandwidth-usage-in-real-time/#:~:text=What%20is%20NetHogs%3F,process%20or%20application%20in%20Linux.

- Linux - an open-source command-line program [similar to top command] that is used to monitor real-time network traffic bandwidth used by each process or application.
		- a small ‘net top’ tool. Instead of breaking the traffic down per protocol or per subnet, as most tools do, it groups bandwidth by process.
		- does not rely on a special kernel module to be loaded
		- if there’s suddenly a lot of network traffic, you can fire up NetHogs and immediately see which PID is causing this.
			- makes it easy to identify programs that have gone wild and are suddenly taking up your bandwidth.



#### Installation RHEL-based Linux ####

yum install epel-release  				# install pre-requisite epel-release repo first

yum install nethogs 					# yum install
OR
dnf install nethogs 					# dnf for Rocky Linux or Fedora
OR
apt-get install nethogs 				# for Ubuntu, Debian, Mint



#### Commands ####

nethogs 				# default launch of nethogs program, will see list of processes sorted by highest network consumption
nethogs -d 5 			# lookup refresh every 5 seconds
nethogs eth0 eth1		# monitor specific device/network interface[s]


#### Flags/Options ####

-d – delay for refresh rate.
-h – list available commands usage.
-p – sniff in promiscuous mode (not recommended).
-t – tracemode.
-V – show version info.


#### Keyboard shortcuts ####

-m – Change the units displayed for the bandwidth in units like KB/sec -> KB -> B-> MB.
-r – Sort by the magnitude of respective traffic.
-s – Sort by the magnitude of sent traffic.
-q – Hit quit to the shell prompt.



-------
netstat
-------

netstat -tupln    # display network connections, tcp and udp, show which processes are using which sockets (need root for p option), -l is for listening sockets
netstat -npl 	  # check both internet connections and processes running on ports not connecting externally
netstat --tcp -n  # display the active TCP connections


#### flags ####

-n - show only numbers, not names [i.e. DNS names, which takes time to calculate when not using the -n switch]
-a - show active connections, where TCP and UDP are listening
-b - show which program is creating the connection [e.g. you will see something like chrome.exe]
-f - shows FQDN in foreign address column
-s - show protocol statistics (usage for tcp, udp, etc.)
-t - show active TCP connections
-u - show active UDP connections
-w - show active raw connections
-x - show active UNIX socket connections
-? - show all options




--------------------
networking - general
--------------------

- An example LAN [NetworkChucks setup]: PC -> Switch -> Router -> Firewall -> Modem -> Fiber optic -> Internet


#### ARP - Address Resolution Protocol ####
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=cn8Zxh9bPio

- definition - used to resolve IP addresses to MAC addresses [the physical address of a device, eg. 00-04-5A-63-AI-66]

- info
	- devices need the MAC address for communication on a LAN
	- devices use ARP to acquire MAC address for a device
		- IP address is used to locate device on a network
		- MAC address is used to identify the actual device

- process
	1. device checks its ARP cache table, the internal list of all IP/MAC address combinations

		arp -a  				# Windows cmd to check list

	2. device sends a broadcast message to every device on the network for a specific destination IP address it wants to talk to
		- asks for MAC address
	3. receiving device with the destination IP responds with MAC address
	4. stores IP/MAC address info in arp cache [which can be checked with arp -a command]
	5. communication between devices can begin
	6. subsequent communication to that device, it can check the ARP cache instead of broadcasting request to whole network


- two types of ARP entries

	- dynamic - entry is created automatically when a device sends out broadcast message requesting a MAC address
						- entries are not permanent, flushed periodically
	- static - a manual entry of a IP/MAC address into the ARP cache
					 - often used to reduce any unnecessary ARP broadcast traffic on a network

		arp -s x.x.x.x  yy-yy-yy-yy-yy-yy     # command for entering static entry into ARP cache, with IP followed by MAC address




#### Bluetooth vs. Wi-Fi ####
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=mPMGRILsOVk&list=WL&index=1

- both are radio frequency technlogies for wirelessly connecting electronic devices

- Bluetooth
	- a low power wireless technology that uses a short-range radio that provides a way to connect nearby devices to each other
	- has a computer chip that will broadcast a signal for other devices to connect and exchange data - known as "pairing"

	- most common usages:
		- wireless audio streaming [e.g. wireless earbuds to iPhone]
		- headphones to a TV
		- wireless keyboard/mouse to a computer
		- cellphone to car audio for hands free

	- range: 30ft or 10m


- Wi-Fi
	- a wireless technology that uses radio waves that allows devices to be able to connect to the internet
	- most common method: via Wi-Fi router in a network, for devices to connect to
	- range: 100 - 300ft [30 - 91m]


- differences
	- Bluetooth
		- used to connect devices to each other
		- less power
		- slower transfer rate
		- shorter range
		- longer battery life
		- smaller battery
		- simpler to connect to than Wi-Fi

	
	- Wi-Fi
		- used to connect devices to the internet
		- faster than Bluetooth
		- 10x longer ranger
		- often requires a password

	
- both operate at 2.4GHz
	- other devices also operate at this frequency. Bluetooth is less vulernerable/highly resistant to interference because it uses "Frequency Hopping Spread Spectrum" [FHSS]
		- trasmits signals in a specific pattern that only the transmitting and receiving devices know
		- hops between 79 different channels
		- changes channels 1600 times per second



#### Configure IP on CentOS ####

1] On CentOS gui, go to Applications -> System Tools -> Settings
2] Click Network in left nav
3] Click gear icon next to "Wired"
4] Click IPv4
5] Configure the following and Apply:
	- Address: 192.168.0.2
	- Netmask: 255.255.255.0
	- Gateway: 192.168.0.250


#### Define static routes on Linux ####

vi /etc/sysconfig/network-scripts/route-eth0       # create and vi into file for the specific network interface

- Add the following:
# begin
	default 192.168.0.250 dev eth0
	10.10.0.0/24 via 192.168.0.250 dev eth0
	172.17.1.0/24 via 192.168.0.250 dev eth0
# end

	- 192.168.0.250 is the default gateway
	- 10.10.0.0/24 is one static route
	- 172.17.1.0/24 is another static route



#### hosts file ####

/etc/hosts            # file that contains ip to dns name association
	- format inside file: [ip_address] [hostname]
	- then the system will be able to recognize the hostname and associate it with the ip_address



###### Hubs vs. Switches vs. Routers ######
# Source: https://www.youtube.com/watch?v=1z0ULvg_pW8

- hub - connect all network devices together
			- not intelligent, only knows when something is connected to it
			- data is copied/transmitted to all ports which have a connection to it
				- can be a security concern if you dont want every device to see data being transmitted between one device on the network and another

- switch - like a hub except intelligent, only broadcasts between communicating devices, not every device on network
			   - can detect specific devices that are connected to it
			   - keeps track of mac addresses of the devices connected
			   

- both of the above
			- for internal communication within a network
			- cannot read IP addresses
			- used to create networks


- router - route/fwds data based on the IP address
				 - inspects packet for the IP address before it routes it
				 - the gateway of a network
				 	 - accepts only packets fron the internet intended for its own network
				 - used to connect networks


##### IPv4 and IPv6 Dual Stack #####
- IPv4 and IPv6 together is referred to as "dual stack"
	- IPv4 addresses are running out and are being migrated to IPv6
	- dual stack connectivity allows your ISP to process IPv4 and IPv6 data simultaneously



#### Linux commands ####
 
route -n        # find routing table of server
nestat -rn      # find routing table of server
ip route        # find routing table of server



#### Misc ####

- default gateway - IP address of your router or modem/router combo that your computer connects to
- subnet mask - defines which parts of the IP address refers to the network and host



##### Subnet vs VLANs #####

# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=6_giEv20En0

- both are used to separate or break down a network into smaller networks
	- improves performance
	- security
	- manageability

- main difference:
	- subnets separate networks physically
	- VLANs separate networks virtually

- subnet aka subnetwork
	- a smaller network within a larger network
	- creating subnets will separate and physically breakdown a network into smaller networks (with routers)
		- will alleviate excess traffic because of broadcasts, which do not go past routers
		- broadcasts from different devices will stay only within the subnet they are a part of
		- security: different subnets wont be able to see each other''s data

- VLAN - Virtual Local Area Network
	- break down and separate a network virtually
	- created on a VLAN-enabled switch
	- ports on a VLAN switch can be configured to separate network traffic WITHOUT using routers
	- e.g. VLAN switch configure a few ports to be part of one VLAN, configure a few others to be part of another VLAN, etc.
		- you will have separate netowrks with separate broadcast domains, just like with subnetting
		- no need to add routers, firewalls, etc.
	- VLAN router e.g. - Netgear GS308E 8-Port Gigabit Ethernet Smart Managed Plus Switch



##### Ports ######

# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=g2fT-g9PX9o

- definition: a logical connection that is used by programs and services to exchange information
  - NOT a physical connection
- specifically determines which program or service on a computer or server taht is going to be used
- will have unique number that identifies them [0 - 65535]
	- eg.
		- 80, 443 - Web pages [http, https]
		- 21 - FTP [file transfer protocol]
		- 25 - email [SMTP]
- port number is always associated with an IP address
- IP:port work together to exchange data on a network
- port numbers are assigned by IANA (Internet assigned Numbers Authority)

- broken down into 3 categories
	- 0-1023 - System or Well-Known ports
		- egs. 80, 443, 25, 21
	- 1024-49151 - User or Registered ports
		- can be registered by companies and developers for a particular services
			- egs.
				- 1102 - Adobe Server
				- 1433 - Microsoft SQL server
				- 1416 - Novell
				- 1527 - Oracle
	- 49152-65535 - Dynamic or Private ports
		- client-side ports that are free to use
		- your computer assigns temporarily during a sesssion, e.g. when viewing a webpage

	- the first two are server-side, the third is client-side





##### Port Forwarding #####
# Source: https://www.youtube.com/watch?v=2G1ueMDgwxw

- definition: allows computers over the internet to connect to a specfic computer or service within a private network

- info
	- port - a logical connection that is used by programs and services to exchange information
				 - is always associated with an IP address
				 - identified by a unique number
				 - IP address purpose is determined by the port [e.g. some_ip:80 usually means a http webpage, 22 for SSH/SFTP, 443 for HTTPS]
				 - "Well Known Ports" - priviledged category of ports ranging from "0 - 1023"

- eg. Remote Desktop Connection from a public computer to a destination computer on a private network behind a router

	- the router will receive the RDP request and needs to know where to forward for port 3389, the default RDP port
	
		- need to configure the destination computers private IP into the routers webpage
			- under single port forwarding -> app name: RDP, external port: 3389, internal port: 3389, protocol: BOTH, To IP Address: <dest_pc_ip>, Enabled: True
			- source computer will try to hit the router IP:port -> router forwards to internal IP:port 

	

#### TCP wrapper (Linux) ####

- For access control, "inetd" and "xinetd" files use the TCP_WRAPPER service. 
- The xinetd binary file has built-in support for the TCP_WRAPPERS. TCP_WRAPPERS is configured in two different files. These files are:

	/etc/hosts.allow
	/etc/hosts.deny

#######updates needed to above section#######



-----
nginx
-----

# Install nginx on Linux CentOS

yum install -y epel-release       # install epel repo (includes gpg keys for package signing and repository information. Gives OS access to a wider range of available packages to install)
yum install -y nginx              # install nginx (epel repo above is a dependency)


# Enable nginx as a service
systemctl start nginx             # start nginx service
systemctl enable nginx            # enable auto restart if system reboots
systemctl status nginx            # check status of nginx service


# firewall rules to allow http/https traffic
firewall-cmd --zone=public --permanent --add-service=http
firewall-cmd --zone=public --permanent --add-service=https
firewall-cmd --reload

- start firefox within Ubuntu and in address bar, type "localhost" and hit Enter to see default webpage running from nginx


# stop nginx (manual)
  nginx -s stop

# start nginx (manual)
  ./nginx


# Concept in K8s
- ingress.yaml -> service.yaml -> application-deploy.yaml
	- ingress has the routing rules, service pods act as the load balancers to the application pods/nodes



-----------
nice/renice
-----------

# definition: can run a program with a modified scheduling priority
# range: from -20 to 19. Default is 0, the lower the value, the higher the priority (so -20 is highest priority)


# start process with nice value (10 by default)
nice [command]

# start process with a specific nice value (2 in this case)
nice -n 2 [command]

# change nice value of a running/active process
renice -n 2 -p [pid]

# change nice value for all running processes for a user
renice -n 2 -u [user] 



--
nl
--

- Linux/UNIX - number the lines in a file and print to stdout


#### Commands ####

nl -ba /etc/yum.conf  			# number the blank lines as well as non-blank lines in a file







----
nmap
----

- Linux/UNIX - Network exploration tool and security / port scanner


#### Commands ####

yum install nmap 			# install nmap [CentOS]
nmap 192.168.0.1 			# shows the open ports with the protocols for the target IP
nmap -O 192.168.0.1 		# also show system info on the target IP
nmap -sP 192.168.0.0/24  	# scan a subnet




-----
nmcli
-----

- Linux/UNIX - network manager cli. Controls NetworkManager program and reports on network status.
			 - can also be used to manage hostname of system


#### Commands ####

nmcli general hostname           			# View current hostname of system
sudo nmcli general hostname plablinux02     # Change hostname to plablinux02




------
nodejs
------

# NodeJS Debugging in Docker
# Source Video: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=ktvgr9VZ4dc
# Source Code: https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/nodejs    [for docker stuff and source code]
# 						 https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/.vscode   [for vscode add-on launch.json which runs the debugger]
# Source Video for Debugger example: DevOps Directive - https://www.youtube.com/watch?v=5JQlFK6MdVQ


# Prerequisites
	- VScode
	- Docker


# Components

- dockerfile [in the nodejs folder in the Github repo]
	- image: the debugger image can be built using docker compose [YAML can be found at the top of the repo]
	- port exposing: need to expose the server port of the nodejs app and another port for the debugger
	- hot reloading: the source code itself is mounted into the container, technically not needing nodejs installed on the local since its in the container,
		so you just update the code locally and it automatically reflects in the container
		- rebuilding the image: the only time you need to rebuild the actual image is when package.json gets updated since you may need to npm install any updated dependencies
	- debugger: in server.js, inside the request/response function, you can put a "debugger;" line and execution will stop there so you can see whats going on in the execution at that point in time



---
npm
---

# Set auth token in npmrc for a target registry
npm config set '//artifactory_url/artifactory/api/npm/npm/:_authToken' 'zsASDKLNvasdfoloi3w123lk1jhof8jaspodfj'


# Set node-sass binary site for pulling sass bindings during an npm install (if required)
npm config set sass_binary_site=http://artifactory_url:8081/artifactory/node-sass


# Set npm registry for resolving dependencies, publishing
npm config set registry http://artifactory_url:8081/artifactory/api/npm/npm/


# Set root user (in case installs fail with permission denied, only for root or sudo installs)
npm -g config set user root


# Set SSL check to false (use only temporarily as a workaround for certificate errors)
npm config set strict-ssl false


# Remove all node_modules folders on an FS in multiple locations
npx npkill



----
ntfy
----

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI
# Installation - https://docs.ntfy.sh/install/
# Integration https://ntfy.readthedocs.io/en/v2.0.1/#pushover-pushover


- Linux - send notifications directly from the terminal



#### Commands ####

ntfy send hello   			# see config details below
rm foo && ntfy send success || ntfy send failure



#### Configuration ####

- example config (ntfy.yml), using pushbullet as the app to notify
		---
		backends:
		  - pushover
		  - darwin
		pushover:
			user_key: apsodjapsodjkapjdpasojd



----
nvim
----

# Sources:
# Main site - https://neovim.io/
# Github - https://github.com/neovim/neovim
# Fireship, NVim in 100 seconds - https://youtu.be/c4OyfL5o7DU


- Linux/UNIX - a refactor of the Vim text editor
			 - aka Neovim




--
od
--

- Linux/UNIX - used to convert the content of input in different formats (octal format is the default format)


#### Commands ####

od -t c file.txt 				# displays the ASCII characters




---
PAM
---

- Linux/UNIX - Pluggable Authentication Modules - a mechanism to integrate multiple low-level authentication schemes into a high-level API (wiki)


#### Commands ####

ldd /usr/sbin/sshd | grep libpam.so   				# check to see if a package is PAM-aware, such as sshd in this case



#### Files and configs ####

/etc/pam.conf
/etc/pam.d/ 						# where all PAM rules are configured for different programs
/lib/libpam.so.*
/usr/lib/libpam.so.*


#### How to deny root SSH access ####

1] Add to pam.conf the following line:
auth required pam_listfile.so \
        onerr=succeed  item=user  sense=deny  file=/etc/ssh/deniedusers

2] Add to /etc/pam.d/login the following line:
auth required pam_listfile.so \
        onerr=succeed  item=user  sense=deny  file=/etc/ssh/deniedusers


3] Add to /etc/ssh/deniedusers the following:
root


4] Lock down deniedusers file:

chmod 600 /etc/ssh/deniedusers


5] Test attempt to ssh to root to the target server from another machine




------
parted
------

- Linux command: allows you to easily manage hard disk partition

parted         					# check parted version and enter parted prompt
parted -l 					   # list partiions


# while in parted menu
	select /dev/sdb 			# choose /dev/sdb device for parted utility to use
	print                       # print partitions on /dev/sdb
	rm                          # remove a partition, will prompt to choose one. NOTE: make sure to unmount a partition before removing it


parted /dev/sda u s p          # display partition table, where:
									# u = abbrev. for unit command
									# s = unit sector [in bytes]
									# p = abbrev. for print command




----------
partitions
----------

*** sections to see also: fdisk, GPT ***


#### General - notes about partitions in Linux ####

- Primary partition - a bootable partition and it contains the operating system/s of the computer
					- generally assigned the first letters in alphabet as the drive letters [e.g. C, D]
- Extended partition - a partition that is not bootable
					 - typically contains multiple logical partitions
	                 - used to store data
					 - a disk drive can contain multiple primary partitions, but it can contain only a single extended partition

- multiple partitions – a boot system can be created using several primary partitions



#### Partitions file ####
# Sources:
	# boson
	# https://www.networkworld.com/article/3297916/examining-partitions-on-linux-systems.html


cat /proc/partitions 			# see table of partitioned devices and their major minor numbers
								# Major number: defines device type
								# Minor number: defines unique instance of the device type

# Eg. /proc/partitions
major minor  #blocks  name
   8        0  117220824 sda
   8        1  117219328 sda1
  11        0    1048575 sr0
   8       16  488386584 sdb
   8       17     512000 sdb1
   8       18  487872512 sdb2


# List of major numbers for devices - see https://www.kernel.org/doc/Documentation/admin-guide/devices.txt


#### Other partition utilities ####

# Commands to bring up editors or display/modify partitions

cfdisk           
sfdisk -l -uM 
fdisk -l 
df                       # shows disk usage
lsscsi                   # lists SCSI devices on a system
smartctl -i [device]     # print drive info about a device
ls /dev/disk/by-id       # determine model and serial number of a disk



-----
paste
-----

- Linux/UNIX - join files horizontally (parallel merging) by outputting lines consisting of lines from each file specified,
			   separated by tab as delimiter, to the standard output


#### commands ####

paste file1.txt file2.txt 			# print contents of files side by side (vertically)
paste -s fruits.txt rates.txt 		# print contents of files one after another (horizontally)



----
perf
----

- Linux/UNIX - measure the CPU performance of specific commands 



#### Commands ####

perf stat cp -r some_folder some_destination/      # show performance stats for the cp -r command
perf stat -a -- sleep 10                           # show system wide cpu profile/stats (note: run as root)
perf stat -p [PID] 								   # show CPU performance for specific process

perf record -e cycles:u -a -- sleep 10  		   # measure the CPU performance for a specific time period and record it (run as root)
perf report  									   # view recorded report from above command (run as root)

perf top -e cpu-clock 							   # display all cpu-clock related events (real-time). Run as root




--------------------
performance analysis
--------------------

# Sources
- https://www.slideshare.net/brendangregg/container-performance-analysis

# To review
- Brendan Greggs ftrace repo - https://github.com/brendangregg/perf-tools
- BGs eBPF blog post - http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html
- BGs eBPF article - http://www.brendangregg.com/ebpf.html
- eBPF [aka BPF] - https://github.com/iovisor/bcc
- Intel snap - a metric collector used by monitoring GUIs: https://github.com/intelsdi-x/snap
- Collectd plugin - https://github.com/bobrik/collectd-docker


# Basic commands

dmesg | tail    		# shows kernel errors
free -m [or -g]			# memory usage
free -g 4  				# show mem usage every 4 seconds
free -l 				# show the high and low mem usage statistics 
iostat -xz 1			# disk I/O
mpstat -P ALL 1			# CPU balance
netstat -s 				# shows statistics by protocol (network)
pidstat 1				# process usage
perf stat cp -r some_folder some_destination/      # show performance stats for the cp -r command
sar -n DEV 1			# network I/O
sar -n TCP,ETCP, 1  	# TCP stats
top 					# overview of processes (hit 1 to see CPU usage). Note: does not show container ID
uptime     				# shows time passed since last reboot of server
vmstat 1				# overall stats by time


# Advanced tools

iosnoop					# disk I/O events w/ latency
btrfsdist				# latency histogram
zfsslower 1			# file system latency is a better pain indicator than disk latency


# BPF (Berkeley Packet Filter)
runqlat -p <PID> 10 1						 # show queue latency, 10 lines, update every second
runqlat --pidnss -m 						 # show per namespace


# Namespaces - limit what you can see, whereas cgroups limit what you can use

cat /proc/<PID>/cgroup 						 					 # events for target container within the cgroup
docker stats 								 								 # a top for containers
dockerpsns.sh [or docker ps --namespaces]    # an initial check before deep dive. More info at https://github.com/docker/docker/issues/32501
grep <PID> /sys/fs/cgroup/cpu,cpuacct/docker/*/tasks | cut -d/ -f7      # will show container associated with PID
htop 										 # can show cgroup (unlike top), but may truncate important info
ls -l /proc/<PID>/ns/*						 # shows ns info about a process
nsenter -t <PID> -u hostname				 # check which host target PID is running on
	-m, -u, -i, -n, -p, -U 					 # mount, uts, ipc, net, pid, user
nsenter -t <PID> -n netstat -i 				 # container netstat
nsenter -t <PID> -m -p df -h 				 # container file system usage
nsenter -t <PID> -p top 					 # container top
nsenter -t <PID> -m -p top 					 # -m for more?
	grep NSpid /proc/<PID>/status 			 # an alternative to above command
systemd-cgtop								 # a top for cgroups



# cgroup metrics cont'd

cd /sys/fs/cgroup/cpu,cpuacct/docker/<container_ID>
ls
cat cpuacct.usage
cat cpu.stat 								 # will show total time throttled



# CPU profiling

perf record -F 49 -a -g -- sleep 30

	# Limitations
	- perf wont be able to find /tmp/perf-PID.map files on the host, PID would be different as well
	- perf cant find container binaries in host paths [i.e. what /usr/bin/java?]

	# Other notes
	- Can copy files to the host, map PIDs, then run perf script/report:
		- https://blog.alicegoldfuss.com/making-flamegraphs-with-containerized-java/
		- http://batey.info/docker-jvm-flamegraphs.html
	- Can nsenter (-m -u -i -n -p) a "power" shell, and then run perf -p PID
	- perf should be fixed to be namespace aware


# CPU Flame Graphs

git clone --depth 1 https://github.com/brendangregg/FlameGraph
cd FlameGraph
perf record -F 40 -a -g -- sleep 30
perf script | ./stackcollapse-perf.pl | ./flamegraph.pl > perf.avg

	# Notes
	- see CPU Profiling section for getting perf symbols to work
	- from the host, can study all containers, as well as container overheads

strace -fp <PID>							 # trace/debugging target PID


# ftrace

funccount '*ovl*'							 # show kernel function calls (overlay)
kprobe -s 'p:ovl_fill_merge ctx=%di name=+0(%si):string'   # look into more



# /proc directory 

ls -l /proc  					# see all running processes 
ls | grep '[0-9]' | wc -l  		# count the number of processes (with PIDs as names only)
echo $$  						# check process ID of current shell

cd /proc/[PID] 				 	# go into a specific process' directory
	cat stat 					# provides process info
	cat status 					# provides process info, more human readable
	cat maps  					# view the virtual regions being used



#### Summary ####

Identify bottlenecks:
1. in the host vs. container, using system metrics
2. in application code on contaienrs, using CPU flame graphs
3. deeper in the kernel, using tracing tools





##### performance testing CLI tools #####
# source: Tech Craft - https://www.youtube.com/watch?v=gbVNQHSpKeI

- speedtest-cli - tests the speed of your internet connection

	- usage:
		speedtest-cli    		  # will generate a report of your upload and download speed (using multiple connections by default for max results)
		speedtest-cli --single    # more useful for testing file upload and download speeds



- 7z - a file compression tool which also has a great set of CPU benchmarks built in

	- usage:
		7z b    			  # runs a basic CPU benchmark (how many CPU cores + frequencies)
							  # 4 other tests:
								# speed: how fast data can be compressed or decompressed
								# usage percentage (total of all cores)
								# rating (MIPS - million instructions per second)
								# normalized rating - (MIPS / usage)

		7z b -mm='*'          # a more comprehensive CPU benchmark check (different algorithm)



- iozone - benchmark tool for storage devices

	- usage
		iozone -a -I -b results.xls      # writes the results to excel
											# 1st column: size of file that was processed (in KB)
											# 1st row: record size in KB (how large are individual reads and writes in each test)
											# each cell: test result in Kbps


- iperf3 - testing speed of local network
	
	- usage (need two machines)
		iperf3 -s      				 # on server side
		iperf3 -c local.hostname     # run on client against the server hostname
									 # will print out results for connection speed

	- applications
		- good for testing wifi speeds (where a device will get the best or worst speeds)



- sysbench - flexible benchmarking tool for memory

	- usage
		sysbench memory run          # default test runs for 10 seconds, shows read/write speed of RAM


- stress-ng - stress tests a machine in various ways

	- usage
		stress-ng --cpu 0 --cpu-load 50 --io 1 --malloc 1 -t 10
			# runs for 10 seconds
			# cpu 0 means stress test all cores
			# test at 50% load
			# io 1 - single io stressor
			# malloc 1 - single memory allocation stressor




----
perl
----

- Practical Extraction and Report Language

- Linux/UNIX - optimized for scanning arbitrary text files, extracting information from those text files, and printing reports based on that information

- Note: perl is far more comprehensive than notes shown here


#### Commands ####

perl -p -i -e 's/ip1/ip2/g' file.txt 			# use perl like sed to replace globally. In this case, using ip1 and ip2 since sed won't work for doing IP address replacement




----
ping
----

- Linux/UNIX/Windows CLI - a simple utility used to check whether a network is available and if a host is reachable.


#### Commands ####

ping [ip_address]  				# check if destination is alive [or reachable] or not
ping -c 5 [ip_address] 			# will ping every second, 5 times total

ping6 [ip_address] 				# ping IPv6 ip_address of the destination





------
Piston
------

# Source: Engineer Man - https://www.youtube.com/watch?v=SD4KgwdjmdI
#									     - https://github.com/engineer-man/piston

- This is a code execution engine built by Engineer Man, originally running on Docker but was moved to LXC for better performance [and incidentally, better security]
	- you can take any source code and put it into the engine, and it will execute it for you
	- it even runs untrusted and possibly malicious code without fear of any harmful effects


### To be filled in with more details at a later date if a useful place to apply it is found ###



--
pr
--

- Linux/UNIX - convert a text file into a more appropriately formatted file with the headers, pagination, and column fills


#### Commands ####

pr -d /etc/yum.conf 			# add double space between lines




----------
processors
----------

# Sources
# Fireship - CPU vs GPU vs TPU vs DPU vs QPU - https://www.youtube.com/watch?v=r5NQecwZs1A
# https://cpu.land/ - breaks down how CPU executes programs (excellent resource, written by high schoolers!)

- CPU
	- Central Processing Unit
	- the "brain" of the computer
	- executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output [I/O] operations
	- also runs the operating system, manages hardware, has access to the systems RAM
	- optimized for sequential computation
	- have multiple cores, allows for work in parallel
	- can handle complex logic and branching
	- 24 cores are the biggest [as of 09/05/2023] - Apple M2 ultra, Intel i9
	- CPU processing among the biggest expenses in a data center

	- types of CPUs [architecture]:
		- x86_64
	  		- on most modern desktops
	  	- arm
	  		- mobile devices
	  		- more simplified instruction set
	  		- better power efficiency
	  		- apple silicon
	  			- proving that arm architecture can also work for high-performance computing on laptops and desktops
	  		- Project Voltera - Microsoft project for making Windows work with arm
	  		- becoming more popular with cloud providers [2023]
	  			- neoverse chip
	  			- Amazon Graviton 3
	  				- allows for more processing with less power usage
	  	- amd
	  	- risc-v
	  	- mips
	  	- powerpc

- GPU
	- Graphical Processing Unit
	- highly optimized for parallel computing
	- has far more [though smaller and less powerful] cores than a CPU [e.g. 16 CPU cores vs almost 10,000 cuda cores]
	- can perform a lot of linear algebra to render graphics instantly [e.g. in video games every time you push a button]
	- also essential for training deep learning models
		- perform tons of matrix multiplication on large data sets
	- designed for simple computation
 	- 


- TPU
	- Tensor Processing Unit
	- similar to GPUs, designed specifically for tensor operations [e.g. matrix multiplication for deep learning, but without needing to access registers or shared memory like GPUs]
	- developed by Google in 2016

- DPU
	- Data Processing Unit
	- "The Third Pillar" of major computing [CEO of NVidia]
	- designed specifically for big data centers
	- like a CPU, based on ARM, but optimized for moving data around
		- networking functions like packet processing, routing, security
		- storage: compression and encryption 
	- main goal: relieve the CPU from any data processing jobs

- QPU
	- Quantum Processing Unit
	- theoretical - does not currently exist physically
	- deals with Qubits
		- quantum bits - can exists in superposition of multiple states simultaneously
	- collapses into the most possible/probable state
	- subject to quantum entanglement - state of one is directly related to another, no matter the distance between them
	- quantum gates - like logic gates in regular computers 


-----
proxy
-----

# Configuration on Rocky Linux

1] In /etc/dnf/dnf.conf, add:

proxy=URL:port
proxy_username=username
proxy_password=secretePasswordHere


#### NEEDS UPDATES ####




---------
ps/pstree
---------

- Linux/UNIX - displays processes running
			 - pstree will display in a tree-like structure


#### Commands ####


# ps

ps eww -p [PID]          		# Check full process args on Linux
ps -auwwwx | grep [anything]    # Alternative to above
									# -a lists all running processes
									# -u: Lists the user who is running the process
									# -x: Lists all running processes even if they are not part of the current terminal session
									# grep for specific criteria

ps -A   						# view all active processes (similar to -e flag)
ps -ef                          # full format listing of all running processes the user has access to
ps -e --forest  				# check how processes are linked with each other
ps f 1                          # list init process. NOTE: can also check multiple PIDs in the same line [ps f 1 115 2616]
ps -fu root 					# check processes for specific user (root in this case)

ps -eo pcpu,pid,user | sort -k 1 -r | head -10  		# show top 10 processes using the most CPU on a system
ps -eo comm,etime,user | grep [proc_command] 			# display the execution time of a process

ps -l 							# show detailed info about running processes, including priority

ps -p 15457 -o pid,ppid,fgroup  						# show pid, ppid and FS group for a specific process

ps aux | sort -k 6 -n | more 							# sort process list by the 6th field, numerically. au means in BSD format

ps -fL -C [proc_command] 								# display all the process threads of a process

ps -p 15457 -o comm=				  					# display the process name by using its PID


# pstree

pstree 							# display all processes in tree-like structure
pstree -a 						# view the list of arguments for the processes in the process tree
pstree -p 						# display the PID for each process in the process tree
pstree -np  					# sort the child processes under the parent PID in the process tree
pstree -u  						# display the uid changes of a process in the process tree
pstree -H 15457 				# highlight the process in the process tree




------
python
------

# Constructors (quick example below) - used to enter values to variables at the time of instantiating an object

class Customer:
  def __init__(self, c="", f="", l="")
  customerID = c
  firstName = f
  lastName = l
  def fullName(self):
    return self.firstName + " " + self.lastName

aleem = Customer("001", "Aleem", "Janmohamed")


# Dictionary (quick example) - used to gather different types of data into one object

def getCustomers():
	"a": "Some Guy 1",
	"b": "Some Guy 2",
	"c": "Some Guy 3",
	"d": "Some Guy 4",
	"e": Customer("001", "Aleem", "Janmohamed")    # example of using constructor within a dictionary

def getCustomer(customerID):

customers = getCustomers()
for customerID in customers:
  print(customers[customerID].fullName())



# Threading vs Multiprocessing
# Source: Engineer Man - https://www.youtube.com/watch?v=ecKWiaHCEKs

- both are trying to do the same thing: to run multiple things at the same time.

Threading:
- A new thread is spawned within the existing process
- Starting a thread is faster than starting a process
- Memory is shared between all threads
- Mutexes often necessary to control access to shared data
- One GIL (Global Interpreter Lock) for all threads

Multiprocessing:
- A new process is started independent from the first process
- Starting a process is slower than starting a thread
- Memory is not shared between processes
- Mutexes not necessary (unless threading in the new process)
- One GIL (Global Interpreter Lock) for each process



------
qualys
------

# Source: https://www.ssllabs.com/ssltest/

- a tool to check external facing certificates
- will tell you which certs are installed
- usage
	- go to the site above and type in a domain name to run an SSL scan on the target site.
	- you will get back a grade on the quality of the SSL configuration a site has



-----
quota
-----

- in Linux, "quota" displays disk usage and limits. Additional quota commands below to view more quota details



#### quotacheck - Definition and details - source: https://linux.die.net/man/8/quotacheck ####

- examines each filesystem, builds a table of current disk usage, and compares this table against that recorded in the disk quota file for the filesystem
	 (this step is ommitted if option -c is specified). 
	 - If any inconsistencies are detected, both the quota file and the current system copy of the incorrect quotas are updated (the latter only occurs if an active filesystem
	   is checked which is not advised). By default, only user quotas are checked. quotacheck expects each filesystem to be checked to have quota files named [a]quota.user and
	   [a]quota.group located at the root of the associated filesystem. If a file is not present, quotacheck will create it.

- If the quota file is corrupted, quotacheck tries to save as much data as possible. Rescuing data may need user intervention.
- With no additional options quotacheck will simply exit in such a situation. When in interactive mode (option -i) , the user is asked for advice. Advice can also be provided from command line (see option -n) , which is useful when quotacheck is run automatically (ie. from script) and failure is unacceptable.

- "quotacheck" should be run each time the system boots and mounts non-valid filesystems. This is most likely to happen after a system crash.

- It is strongly recommended to run quotacheck with quotas turned off for the filesystem. Otherwise, possible damage or loss to data in the quota files can result.
- It is also unwise to run quotacheck on a live filesystem as actual usage may change during the scan. To prevent this, quotacheck tries to remount the filesystem read-only before starting the scan. After the scan is done it remounts the filesystem read-write. You can disable this with option -m. You can also make quotacheck ignore the failure to remount the filesystem read-only with option -M.


# Enable quotacheck for the FS 

quotacheck -cug /home
	- -c option creates the quote files for each filesystem that has quota enabled
	- -u option creates these files for a user quota
	- -g option creates these files for a group quota


# List the files put into the file-table via quotacheck command above

quotacheck -avug
	- -a option checks for locally mounted filesystems that have quota enabled. 
	- -v option is for displaying verbose status.
	- -u option is for user quota
	- -g option is for group quota


# Update the quota for a user

edquota [user_id]

# Output: edit window for the target user's quota #####
Disk quotas for user [user_id] (uid @@@@):
  Filesystem      		blocks  	soft     hard     inodes     soft     hard
  /dev/sdb1                  0         0        0          0        0        0 

- you can use vi to modify the above values


repquota /dev/sdb1      # view quota report for target filesystem
quota -u [user_id]  	# view quota statistics for a user



----
RAID
----

# Sources: Powercert Animated Videos - https://www.youtube.com/watch?v=U-OCdTeZLac [RAID 0, 1, 5, 10]
#																		 - https://www.youtube.com/watch?v=UuUgfCvt9-Q [RAID 5 & 6]


- RAID stands for "Redundant Array of Independent Disks"
- used for data loss prevention in the event of hardware failure
	- data is spread across multiple disks for fault tolerance

- types:

	- RAID 0
		- not fault tolerant
		- data is 'striped' across multiple disks
			- eg. disk A would have certain parts of the data, disk B would have the other parts. If one of these disks fail, all of the data would be lost
		- advantage: speed
			- 2 disk controllers instead of one, makes data access much faster

	- RAID 1
		- fault tolerant
		- data is copied on more than 1 disk [duplicate copy of the data across each disk]
			- i.e. each disk has the same data 

	- RAID 5
		- requires 3 or more disks
		- fast and can store a large amount of data
		- can handle single disk failure, cannot handle two disk failures at the same time
		- data is not duplicated but striped across multiple disks along with "parity"
			- parity
				- an entire copy of the data across each disk
				- used to rebuild a failed disk
				- eg. an array of 4 disks totaling 4 TB
					- only 3 TB used for data storage
					- 1 TB used to store parity for rebuilding any of the 4 disks in the event of failure

	- RAID 6
		- requires 4 or more disks
		- 50% capacity used for data storage
		- like RAID 5 in terms of data striping, but parity is spread TWICE on each disk
			- can handle 2 disk failures at the same time [rare occurrence]
				- if 2 drives fail at the same time, double parity from other disks would be used to rebuild the data on the new drives
		- read performance is the same between RAID 5 and 6
		- write performance is slower because it has to write 2 independent parity blocks instead of 1


	- RAID 10
		- combines RAID 0 and 1 together
		- benefits of RAID fault tolerance from RAID 1 with the speed from RAID 0
			- downside: can only use 50% of the capacity for data storage
		- minimum of 4 disks needed
		- RAID 0 pointing to 2 RAID 1 sets of 2 disks 
			- RAID 0 -> RAID 1 -> disk 1 
												 -> disk 2
							 -> RAID 1 -> disk 3
							 					 -> disk 4





------------
Raspberry Pi
------------

# Installing Kubernetes on Raspberry Pis'
# Source - NetowrkChuck - https://www.youtube.com/watch?v=X9fSMGkjtug

- Things youll need:
	- 2 or more Raspberry Pis
	- micro SD USB adapter
	- micro SD card


1) Plug in your micro SD card into your PC and install the Rapberry Pi OS image onto it (can be headless, such as Raspberry Pi OS Lite)
	- using Rapberry Pi imager program

2) Plug the micro SD into your Rapberry Pi and let it boot up

3) Once it is up, remove the SD card, and plug it back into your computer
	- booting the Raspberry Pi for the first time creates a boot folder. Some edits are required, so next step...

4) In the "boot" folder [will show up as a drive once you plug back into your PC], open cmdline.txt and paste the below text:

		cgroup_memory=1 cgroup_enable=memory ip=x.x.x.x::y.y.y.y:255.255.255.0:somehostname:eth0:off

				- where
					- x.x.x.x is an internal IP in your network that isnt used
					- y.y.y.y is the default gateway
					- 255.255.255.0 is the subnet mask
					- somehostname is the hostname for your Raspberry Pi
					- eth0 is the network interface card
					- off means turn off auto configuration


5) Edit config.txt, scroll to the bottom and add: arm_64bit=1
	- means to use 64 bit of Raspbian [the Raspberry Pi OS] 


6) Enabling SSH on the Raspberry Pi - open Powershell and in the boot drives directory, run:

	new-item ssh
		- this will create a new blank file called ssh

	Note: on Mac or Linux, you would run touch ssh


7) Plug the SD card back into the Raspberry Pi, wait to boot

8) On your PC, ping the Raspberry Pi at the internal address you entered in cmdline.txt [and confirm the Rapberry Pi replies]
	
	ping x.x.x.x -t

		- where -t flag means continously


9) SSH into the Raspbeery Pi

	ssh pi@x.x.x.x
		- password would be "raspberry"


10) Become the root user

	sudo su -


11) Enable IPTables

	sudo iptables -F
	sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy


12) Reboot the Raspberry Pi

	sudo reboot


13) Repeat on each Raspberry Pi you want to configure into your cluster


14) SSH back into your first Raspberry Pi and switch to root


15) Install k3s on your first node [which will become your master node]

	curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s          # kubeconfig mode setting will allow to import k3s into Rancher


16) CHeck your cluster

	kubectl get nodes   				# should only show one node at this point


17) Get the master nodes token:

	cat /var/lib/rancher/k3s/server/node-token 				# use this token to register the rest of the nodes to the cluster


18) On another Raspberry Pi, install k3s, registering it as a worker node:

	curl -sfL https://get.k3s.io | K3S_TOKEN="YOURTOKEN" K3S_URL="https://x.x.x.x:6443" K3S_NODE_NAME="somehostname#" sh -s

		- where somehostname# is a unique name for your node

		- note: kubectl can only be used on the master, not the worker nodes 


19) Check your cluster again

	kubectl get nodes



20) Install Rancher on another VM or container [switch to root]

	mkdir -p /etc/rancher/rke2
	cd /etc/rancher/rke2
	vi config.yaml

	# contents of config.yaml

	token: mylittlepony
	tls-san:
		- x.x.x.x             # where x.x.x.x is the VM/container IP

  # end contents

  curl -sfL https://get.rancher.io | sh -


21) Verify Rancher was installed

	 rancherd --help


22) Enable Rancher as a service and start it

	systemctl enable rancherd-server.service
	systemctl start rancherd-server.service

	# this also sets up a K8s cluster within the VM/container with Rancher installed in it


23) Get Rancher login creds

  rancherd reset-admin   # will give you server URL + default username and password to login to the console with


24) Once you login to the Rancher console [and reset your password], add your Raspberry Pi cluster:

	- two options to choose from
		- I want to create or manage multiple clusters                  <----- choose this one
		- Im only going to use the cluster Rancher was installed on

	- agree to Terms and Conditions and continue

	- copy and save server URL thats generated


25) You will see the Rancher local cluster. Next step is to add the Raspberry Pi cluster

	- click Add Cluster
	- click Other Cluster
	- enter a name for the cluster
	- run the insecure curl command in your terminal [or secure if your K3s cluster is secure] to import the Raspberry Pi cluster into Rancher
	- click Done in the console once the command executes successfully



------
rclone
------

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI

- Linux - similar to rsync, but does support syncing to cloud provider storage mediums


#### Commands ####

cat ~/.config/rclone/rclone.conf 		# create configuration

- populate config with storage destination details
- e.g.

#### START ####
[b2test]
type = b2
account = 00XXXXXXXXXXXXXXXXXd1234ds00000000004
key = <insert_key_here>
hard_delete = true
####END

rclone config 							# verify configuration
rclone sync -P . b2test:[bucket_name]   # run sync with rclone
										# will sync your local current working dir up to the destination bucket



---
RDP
---

- remote desktop protocol - a service that gives ability to be connected to remotely (instead of directly on the device)

#### Connect to Windows machine from Linux (CentOS) ####

yum install "@X Window System" xorg-x11-xauth xorg-x11-fonts-* xorg-x11-utils -y
													# install XWindows/desktop dependencies/components
yum install freerdp -y      						# install freerdp
xfreerdp -g 800x600 -u administrator 192.168.0.1 	# connect to Windows machine (note the devices are on the same network so internal IP is used)





---------------------
reboots and runlevels
---------------------

- Linux/UNIX - prints the previous and current SysV runlevel (if known)


#### Commands ####

runlevel          		# prints runlevel  e.g. N 5, which means previous runlevel not known and current runlevel is 5
telinit 3         		# switches the runlevel of the system to 3
init 6          	    # shuts down and reboots system into runlevel 6

shutdown -h +10 	    # schedule shutdown of the system to occur in 10 minutes
shutdown -c             # cancel scheduled shutdown
shutdown -r 11:00       # reboot the system at 11am (system time).
						# With no parameter, wait time is 1 minute

reboot                  # reboots system

wall “This system will be rebooted at 11:00 AM.”       # broadcast to users system will reboot at 11:00am


service vmtoolsd.service stop      # stop the vmtoolsd service (redirects to systemctl)




---------
redirects
---------

- Linux/UNIX - e.g. redirecting command output to a file


#### Commands ####

find / -iname *.txt > find.txt              # search for all txt files under "/" and redirect findings to a file (ignore case)
find / -iname *.conf >> find.txt 			# search for all conf files under "/" and append findings to a file (ignore case, create file if not exist)
list 2>test.txt 							# redirect error output (in this case of non-existent cmd) to test.txt

ls -l /dev | more  							# list all files under /dev and use more pagination (hit spacebar to continue through results)




----------
References 
----------

# Technical
- https://bundlephobia.com/      # check cost of adding select NPM packages
- https://coolermaster.com/us/en-us/power-supply-calculator 		# CoolerMaster Power Supply Calculator - add components to find out necessary wattage
- https://crontab.guru/          # service to figure out any crontab entry
- https://crt.sh/  				 # online certificate search and log tracker tool
- https://everything.curl.dev/   # an extensive guide for all things curl
- https://explainshell.com/      # check what individual CLI lines mean [help text]
- https://groovy-lang.org        # official site for learning the Groovy programming language
- https://reqbin.com/curl        # excellent site for testing different curl commands (e.g. with POST, GET, POST JSON, PUT, DELETE, etc)
- https://www.shellcheck.net/    # shell scripting syntax checker


# Training
- boson.com  					 					# highly recommended by NetworkChuck [YT] for CCNA, CCNP, etc. certification
- https://www.youtube.com/watch?v=uEVmD6n8Il0  		# NodeJS deployment strategies [Fireship]
- https://docker-curriculum.com/ 					# Docker beginner/starter guide


# Info
- https://badssl.com   							 # DO NOT navigate to on an internal/corporate network. Bad certificate errors glossary 
- https://certificate.transparency.dev/    		 # certificate transparency. Explains how CT logging works and why we use it
- https://cpu.land/         					 # breaks down how CPU executes programs (excellent resource, written by high schoolers!)
- https://datatracker.ietf.org/wg/pkix/about/    # PKIX progress tracker. Describes the PKI X initiative with a progress tracker for different items.
- https://www.youtube.com/watch?v=nOtxRNQAKXA    # Tech Talk: ArgoCD + Tekton = Better GitOps
- https://serverfault.com/questions/660160/openssh-difference-between-internal-sftp-and-sftp-server  # Explanation on diff between internal-sftp and sftp-server





-----
Regex
-----

^[0-9]{3}-[0-9]{3}-[0-9]{4}$     # match any phone number
^[0-9]{3}-?[0-9]{3}-?[0-9]{4}$   # match three numbers before first dash, matching second and third sets are optional 
^[a-zA-Z0-9._]+@[a-zA-Z0-9]+\.[a-zA-Z]{2,3}$	 # one or more of the first set, followed by @, followed by one or more letters or numbers
^[\w\d._]+@[\w\d]+\.[\w]{2,3}$   # alternative to above, will match foreign letters and numbers as well because of \w and \d

Legend:

	Assertions/Quantifiers
		$		matches end of line
		^		matches beginning of line
		*		0 or more
		?		0 or 1
		+		1 or more
		{n}		exactly n
		{n,}	n or more
		{n,m}	between n and m


	Characters
		.		any exxcept newline
		[abc]	a, b, or c
		[a-z]	a, b, ...., z
		[^abc]	anything but a, b, c
		\d      digit
		\s      status
		\w      word character


	Special
		\n      newline
		\t      tab


# Pattern for Jenkins roles: including what jobs you want a specific group to be able to see
# e.g. jobs which start with Gemfire_DEV, have Akamai somewhere in the name, and start with Gemfire_Recycle_

"Gemfire_DEV.*|.*Akamai.*|Gemfire_Recycle_.*"



-----
redis
-----

- stands for 'Remote Dictionary Server' - an in-memory multi-model database
- RAM-based processing - data processing happens on RAM, but also stored on disk in case of restructuring/rebuilding needed
- key/value store - storing data in the form of key/value (different types include string, bitmap, hash, list, set, stream)
- secondary database - can be used as cache support for relational DBs
- **primary DB** - can also be the primary database for even a large scale application
- plugins - can install add-on module plugins to help with structuring, searching, etc.
- cli - has its own command line (naturally...)




--------
reposync
--------

- Linux (CentOS): syncs local with remote repos

mkdir -p /var/www/html/repos/{base,centosplus,extras,updates}         # create directories necessary for syncing


# Commands to sync local with packages stored in the remote repo locations

reposync -g -l -d -m --repoid=base --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=centosplus --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=extras --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=updates --newest-only --download-metadata --download_path=/var/www/html/repos/


# Verify packages are pulled down (similar for centosplus, extras and updates)

ls -l /var/www/html/repos/base/Packages/


# Create comps.xml 
touch /var/www/html/repos/centosplus/comps.xml
touch /var/www/html/repos/extras/comps.xml
touch /var/www/html/repos/updates/comps.xml
touch /var/www/html/repos/base/comps.xml


# Create new repodata for the local repos
createrepo -g comps.xml /var/www/html/repos/base/


# Create a script to do this on the regular
vi /etc/cron.daily/updaterepos

    #### content below ####
	#!/bin/bash
	LOCAL_REPOS=”base centosplus extras updates”
	for REPO in ${LOCAL_REPOS}; do
	reposync -g -l -d -m --repoid=$REPO --newest-only --download-metadata --download_path=/var/www/html/repos/
	createrepo -g comps.xml /var/www/html/repos/$REPO/
	done
    #### end content #####

chmod 755 /etc/cron.daily/updaterepos




------------
ripgrep (rg)
------------

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg

- Linux - searching your files for specific text or patterns (regex)


#### Commands ####

rg sometext 			      			# search for sometext in all files in current and subdirs
rg -e '[0-9]{2}:[0-9]{2}'    			# use -e for special patterns that contain dashes, colons (in this case, searching for timestamps)
rg --glob 'some_dir/*.txt' sometext     # search for sometext in *.txt only in the some_dir directory using --glob flag
rg --glob 'some_dir/**/*.txt' sometext  # same as above except search all sub dirs under some_dir which contain txt file extension




------
rmlint
------

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI
# documentation: https://rmlint.readthedocs.io/en/latest

- finds and removes duplicate files, empty files, broken symlinks
	- usage
		- from root of dir you want to clean, run:

		rmlint
			# will first generate a report of files that will be kept [the originals] and files that will be cleaned up
			# will create a shell script and json for you to run to perform the actual cleanup (based on the report)
		
		- from root of dir you want to clean, run:

		rmlint * // pictures          # this will designate "pictures" directory as the one that contains the originals



---
rpm
---

- Redhat Package Manager - the package manager used on RHEL OS (along with yum)

#### General commands #####

RESP_FILE=<app>-environment.response; export RESP_FILE    # set response file for an RPM - 

rpm --initdb --dbpath /path/to/rpm_db   # intialize rpmdb, which stores info about all RPMs installed. This is custom vs. rpmdb which is configured at the OS level
rpm --dbpath /path/to/rpm_db --prefix /install_path/ --nodeps -Uvh /path/to/rpm/ebm-wca-1.0.0-1.x86_64.rpm   # install RPM into prefix path
																											 # write into target RPM db and upgrade it if package already exists
																											 # skip testing any dependencies
rpm -q [pkg_name]                              # show some detail about a package
rpm -qi [rpm_name] --dbpath /path/to/rpm_db    # query package
rpm -V [rpm_name] --dbpath /path/to/rpm_db     # check pkgmap
rpm -ev pkg_name --dbpath /path/to/rpm_db      # uninstall rpm
rpm -qlpv [rpm_name.rpm]                       # list files in an RPM
rpm -ql [installed_package_name]  			   # list files in an installed RPM package
rpm -q --scripts [installed_package_name]	   # check scriptlets of an installed package
rpm -qpR mediawiki-1.4rc1-4.i586.rpm  		   # show list of dependencies for an rpm
rpm -ivh mediawiki-1.4rc1-4.i586.rpm 		   # another way of showing dependencies for an rpm except through installation attempt

rpm2cpio squid-3.5.20-12.el7.x86_64.rpm | cpio -idmv     # converts squid rpm to cpio archive



-----
rsync
-----

# source: Tech Craft - https://www.youtube.com/watch?v=szehPBOwqlI

- Linux - a fast file copying tool that can quickly sync the contents of 2 directories (including across different machines)

- benefits
	- more reliable than running from the GUI
	- faster
	- can run incrementally
- downsides
	- only supports local and rsync targets [i.e. can't send to cloud provider storage apps like Google Drive, Dropbox, etc.]

#### Commands ####

rsync -av --progress /external/volume/to/contents_dir .
	# where -a is archive mode, -v is verbose output, --progress shows syncing progress
	# -a copies directories recursively, copies timestamps, file permissions, ownership [if you're superuser]

rsync -av --progress . someuser@example.com:/volume1/dest_dir
	# sync from local to a remote (server)




---
sar
---

- Linux/UNIX - displays the average statistics for the time since the system was started (in 5 second intervals by default)
	- aka "System Activity Report"


#### Commands ####

sar                    						  # shows consolidated cpu usage stats
sar -u 2 10 		   						  # display the CPU utilization every two seconds 10 times
sar -o plab.file 10 10 >/dev/null 2>&1 &  	  # create CPU report of utilization check every 10 seconds (up to 10 checks)
sar -f plab.file 							  # view report





-------
secrets
-------

##### Password managers ######

- Dashlane - https://www.dashlane.com
		   - manage your website passwords in one place
		   - paid subscription includes monitoring the dark web to see if any of your passwords have been compromised
		   - can generate new, stronger passwords for you
		   - tells you how many times you are re-using a password

- Roboform - https://www.roboform.com
		   - similar/alternative to Dashlane


# General levels of keeping secrets
# Source: DevOps Directive [Sid Palas] - https://www.youtube.com/watch?v=7NTFZoDpzbQ

1) Hardcode directly in the code

2) Create a config file, but still stored in plaintext
	- generally ok to check into source if the secrets are for dev envs 

3) Encrypt the config file
	- can use openssl to encrypt the config

		openssl aes-256-cbc -d -a -salt -in secrets.env.enc -out secrets.env -pass pass:Where-am-I-supposed-to-store-this?!
		cat secrets.env
		
		# eg. file contents
		DB_PASS=somethingsomething


4) Use a dedicated secret manager
	- eg. in code, secret manager is called and variable referenced


5) Ephemeral or temporary credentials (eg. using Hashicorp vault)
	- rotates/renews credentials automatically, creds have an enforced expiry



# Apply secret to a K8s namespace from a file

kubectl apply -f secret.yaml


# Apply secret using K8s native CLI
	
	# passing the string directly
	
	kubectl create secret generic my-secret \
		--from-literal=APU_TOKEN=password123

	# pass from a file
	
	kubectl create secret generic my-secret \
		--from-file=cert=/path/to/cert/file`


# secret yaml example
apiVersion: v1
kind: Secret
metadata:
	name: my-secret
data:
	API_TOKEN: c4FDca23zdfA4A=


# Use secret in app (from file) - sub-section of yaml

spec:
  containers:
..
..
..
     env:
       - name: API_TOKEN
         valueFrom:
         	secretKeyRef:
         		name: my-secret
         		key: API_TOKEN


# Use secret from a volume

spec:
  containers:
  ..
  ..
  ..
     volumeMounts:
       - name: secret-volume
       - mountPath: /etc/secret-volume
  volumes:
    - name: secret-volume
      secret:
      	secretName: my-secret



# Creating a generic secret in Kubernetes
# Source: Just Me And Opensource (Venkat) - https://www.youtube.com/watch?v=CK87fP2_tDs

- three types of secrets: docker-registy, generic, or TLS

kubectl create secret generic mysecret --from-literal=username=venkat --from-literal=password=hello

kubectl get secrets                   # see all secrets in cluster

kubectl describe secret mysecret      # see info and contents of mysecret

kubectl edit secret mysecret          # Edit a secret 


# Example of secret yaml in edit mode, where you can edit user or password or both, unless you put the immutable flag as shown below
# You will get error: secrets "mysecret" is invalid, if you try to save an edited secret where the immutable flag is set

apiVersion: v1
data:
	password: aGVsbG8=
	username: dmVua2F0
kind: Secret
immutable: true
metadata:
	creationTimestamp: "2021-05-09T17:17:01Z"
	name: mysecret
	namespace: default
	resourceVersion: "2264"
	uid: c98b0109-c095-4a3d-97f3-dd8d41057752
type: Opaque


kubectl delete secret mysecret     # deletes secret - the only way to "edit" an immutable secret


---
sed
---

- Linux/UNIX - search and replace tool


#### Examples ####

sed -i -e 's/few/asd/g' hello.txt     # Simple search and replace
sed '/^#/ d ' yumtest.conf            # Remove all commented lines from yumtest.conf
sed '/^  / d' yumtest.conf            # Remove all blank lines from yumtest.conf

sed -e '/^$/ d'  -e 's/Fedora/Linux/g'  yumtest.conf     # Remove all blank lines and also replace all occurences of Fedora with Linux




--------
sendmail
--------

- Linux/UNIX - widely used mail transport agent (MTA). sends mail from one machine to another.


#### Commands ####

yum install -y sendmail sendmail-cf

alternatives --config mta  					# shows menu to switch from postfix (default mail client) to different one

make all -C /etc/mail/ 						# run anytime there's a change to sendmail.cf that needs to be rebuilt into the .cf file

systemctl restart sendmail 					# restart sendmail service

newaliases 									# load new aliases from /etc/aliases so sendmail is aware of them

mail 										# check current user's mail
mailq 										# check current user's mail queue
mail -s "Test " admin@practicelabs.com      # create an email from command prompt, send to the specified address.
											# -s flag specifies subject line


#### Files/directories/configs ####

/etc/mail  								# dir containing databases and configs for mail service

/etc/mail/sendmail.cf 					# config file for sendmail-cf. Should not be edited

/etc/mail/sendmail.mc 					# editable config for sendmail, can be used to rebuild sendmail.cf

/etc/aliases 							# specifies which account mail sent to an alias should really be delivered to.
										# eg. mail to the ftp account would be sent to root's mailbox if this config is in there [ftp: root].

$HOME/.forward   						# contains addresses to forward mail to if the account receives mail
	- \root, admin@practicelabs.com 	# eg. this entry in the file would save mail to root's home and then forward to admin@practicelabs.com




-------
SELinux
-------

- Linux/UNIX - provides general support for the enforcement of many kinds of mandatory access control policies, 
			   including those based on the concepts of Type Enforcement (reg trademark), Role-Based Access Control, and Multi-Level Security
			 - aka "Security Enhanced Linux"

- three different modes
	Enforcing: If an event occurs against the defined policy of SELinux, then the event is blocked and logged.
	Permissive: If an event occurs against the defined policy of SELinux, then the event is not blocked, however it is logged.
	Disabled: If an event occurs against the defined policy of SELinux, then the event is neither blocked nor logged.

- security context on object [ls -Z]
	SELinux user
	Role
	Type
	Level

	- eg. ls -Z on a file might yield the below [SELinux user in this case is system_u]
		-rw------- root root system_u:object_r:admin_home_t:s0 file.txt


##### Policies #####

- defines "types" for file objects and "domains" for processes

- When a user is mapped to "unconfined_u", SELinux does not block access for the user.

- "semanage" command is used to manage SELinux rules

- There are two components to the policy:
	- the binary tree - provided by the selinux-policy-<policyname> package and supplies the binary policy file.
					  - Alternatively, the binary policy can be built from source when the "selinux-policy-devel" package is installed.
	- the source tree



#### Commands ####

dnf provides */semanage 			# check if semanage package is installed
dnf install policycoreutils-python-utils  		# install semanage package (Rocky Linux)

semanage boolean –l  		# see full list of available rule settings
	- -a - adds an object
	- -d - deletes an object
	- -m - modifies an object 

semanage port -a -t http_port_t -p tcp 81  			# allow port 81 for httpd domain processes

setsebool -P httpd_can_sendmail on  		   	# allow apache server to be able to send emails
	- -P means keep the setting even after reboot

getenforce 			# check current status of SELinux policies enforcement
sestatus 			# get an overview of the SELinux configuration
setenforce 0 		# set SELinux to permissive mode [1 = Enforcing]
semanage login -l	# view a list of mappings between SELinux and Linux user accounts

ls -Z [file] 				# check SELinux security context of file
ps -eZ | grep some_process 	# check SELinux security context for a process
id -Z  						# check SELinux security context for current user


#### Files and configs ####

/etc/selinux/config  							# config file for SELinux
/etc/sysconfig/selinux  						# symlink for the above

/etc/selinux/targeted/  	    				# this is the root directory for the targeted policy, and contains the binary tree.

/etc/selinux/targeted/policy/ 					# this is the location of the the binary policy file policy.<xx>

/etc/selinux/targeted/contexts/ 				# this is the location of the security context information and configuration files
												# which are used during runtime by various applications.

/etc/selinux/targeted/contexts/files/ 			# contains the default contexts for the entire file system.
												# This is referenced by restorecon when perfoming relabeling operations.

/etc/selinux/targeted/contexts/users/ 			# in the targeted policy, only the root file is in this directory
												# These files are used for determining context when a user logs in
												# For example, for the root user, the context is user_u:system_r:unconfined_t.

/etc/selinux/targeted/modules/active/booleans*  # this is where the runtime Booleans are configured.



#### supporting tools ####

yum install policycoreutils-gui  			# install policycoreutils GUI to make managing SELinux easier
											# select "SELinux Management" from Linux GUI to use tool after installing


#### NEEDS MORE CONTENT FOR EXPLANATION ON POLICIES ####




----
sftp
----

- Linux/UNIX - secure file transfer program


#### Commands ####

sftp -vv account@server   			# sftp to server and debug while connecting




---------------
Shell scripting
---------------

read -ps    # -p is for prompt, -s is for secret text



-----
shipa
-----

Video Source: Making Kubernetes disappear with Shipa - https://www.youtube.com/watch?v=PW44JaAlI_8



----
smtp
----

- Simple Message Transfer Protocol - used for sending emails between computers/devices, usually through SMTP servers


# SMTP error codes
# Source: https://sendgrid.com/blog/smtp-server-response-codes-explained/

250 – This SMTP server response simply means everything went well and your message was delivered to the recipient server.
421 – Your message was temporarily deferred by the recipient server. This is usually a result of too many connections in a short timeframe or too many messages.
450 – Your message was not delivered because the other user mailbox was not available. This can happen if the mailbox is locked or is not routable.
451 – This response is sent when the message simply failed. Often times this is not caused by you, but rather because of a far-end server problem.
452 – This kind of response is sent back when there isn’t enough system storage to send the message. Your message is deferred until storage opens up and it can then be delivered.
550 – The message has failed because the other user’s mailbox is unavailable or because the recipient server rejected your message.
551 – The mailbox your message was intended for does not exist on the recipient server.
552 – The mailbox your message was sent to does not have enough storage to accept your message.
553 – You message was not delivered because the name of the mailbox you sent to does not exist.
554 – This is a very vague message failure response that can refer to any number of problems either on your end or with the recipient server.



--------
software
--------

# How Major Parts of Modern Software Fit Together (05/08/22)
# Source: Engineer Man - https://www.youtube.com/watch?v=2XDhad47Sjk

major pieces:
	operating systems:
		- linux
		- windows server
	backends:
		- apis
		- web servers
		- logic layer
	frontends:
		- frameworks
		desktop:
			- electron
			- qt/winforms/cocoa
			- swing
			- terminal
		web:
			- html/css/javascript
		mobile:
			- native
			- hybrid (e.g. react native)
	databases:
		- sql (rigid)
		- nosql/schemaless
	console tasks:
		- on demand scripts
	cronjobs:
		- scheduled routines
	worker/job-queue:
		- processing/fully offloading
		- fully asynchronous
	transport:
		- tcp/udp
		- http/https
		- websockets
	devops:
		- test suites
		- quality assurance
		- automated deployments
		- continuous integration
	scale concerns:
		- content delivery networks (cdns)
		- database clustering
		- server load balancing




-----
split
-----

- Linux/UNIX - divide the contents of a file into multiple files


#### Commands ####

split -2 yum.conf splityum 				# split yum.conf into multiple files, 2 lines a file




---
sql
---

##### SQL injection - quick intro #####

# Sources: NetworkChuck - https://www.youtube.com/watch?v=2OPVViV-GQk
		   Wikipedia

- sql injection - a code injection technique used to attack data-driven applications, in which malicious SQL statements are inserted into an entry field for execution
				- e.g. to dump the database contents to the attacker

- Dashlane - dashlane.com/networkchuck50 (use code networkchuck50 to get 50% off)
		   - an online tool used to monitor the dark web to see if your usernames and passwords are for sale *******
		   - will generate and store your updated passwords for you all in one place
		   - will also support MFA for logging into different sites

- reasons sql injections can be easily executed: companies or devs can be lazy with their coding.

- security measures to protect
	- use prepared statements with parameterized queries
	- use allowliist input validation
	- escape user input before putting it in a query




--------
ssh/sshd
--------

# Sources:
# Powercert Animated Videos - https://www.youtube.com/watch?v=tZop-zjYkrU
# Various/misc/experience

- Linux/UNIX - ssh - program for logging into a remote machine and for executing commands on a remote machine
				   - aka Secure Shell
				   - protects the data from being attacked or stoled as its being transferred over a network
				   - provides password and public key authentication

- Linux/UNIX - sshd - OpenSSH server daemon - listens for connections from clients (logins)



#### Commands ####

rpm -qa | grep openssh  			# check if openssh package is installed
yum install openssh   				# install it if required
systemctl status sshd  				# check if service is running or not

iptables -A INPUT -s 192.168.0.0/24 -p tcp --dport 22 -j ACCEPT 		# set rule for allowing incoming connections to port 22 from specific ip range



#### Files and configs ####

/etc/ssh/ssh_config  				# ssh client configuration
/etc/ssh/sshd_config  				# configure ssh daemon settings such as:
									# permitting root login, keys, max login retries, users and groups to allow, ListenAddress, etc



#### ssh public/private key generation for passwordless SSH ####

1) on the client side, create an ssh public/private key pair:
	ssh-keygen -t rsa -b 4096       # can use dsa encryption as well
	# do not enter a passphrase, just hit enter

2) Copy the contents of id_rsa.pub and ssh to the target server

3) cd to the users .ssh folder, edit authorized_keys, and paste the pub key entry into the file and save. You should now be able to ssh from the client to the server without a password.

4) You can also try updating authorized_keys from the client like so:
	ssh-copy-id -i ~/.ssh/id_rsa.pub user@host
	
5) notes:
	- ssh directory permissions - 700
	- authorized_keys file permission - 600




#### SSH tunneling - requires SSH server to be set up on target machine ####
# source: Tinkernut - https://www.youtube.com/watch?v=AtuAdk4MwWw

# tunnel from local machine to a remote machine
ssh -L <port_to_expose>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>




#### create SOCKS proxy ####
ssh -D <any_port> <dest_user>@<destination_SSH_server_IP>

	# after running the above, you can for example set this in your LAN settings in your browser, enter the details in the SOCKS proxy section, and any site you visit will be forwarded through the
	# SSH tunnel to the destination specified



#### Set up a SSH tunnel from the target server for a "local" PC or server to access ####
ssh -R <any_port>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>

	# enter the remote server's IP and any_port in your local browser via remote desktop and you'll be able to remote into the target machine


#### Set up chroot jail ####

- A chroot jail helps you to isolate a process and its children from the rest of the system
- You can limit a process to run in its own confined space where the process does not have access to the remaining system
- The root user, however, can break out of the chroot jail.


1] Add a specific user to group_ID [whichever group you want to configure]:

usermod -g plabFin -s /bin/false roger


2] Edit /etc/ssh/sshd_config as follows:

- change:
	From: Subsystem sftp /usr/libexec/openssh/sftp-server
	To: Subsystem sftp internal-sftp

- Add to the end of file:
	Match Group [group_ID]
	X11Forwarding no
	AllowTcpForwarding no
	ChrootDirectory [choose_any_directory]
	ForceCommand internal-sftp

3] Restart sshd service

systemctl restart sshd




---
SSL
---

# SSL handshake overview
# Source: F5 DevCentral - https://www.youtube.com/watch?v=cuR05y_2Gxc

1] client sends a request to the server to establish a connection
	- client sends a "Hello" message with:
		- TLS version
		- Cipher suites it can support

2] server sends a "Hello" message back:
	- also sends a certificate [containing its public key]
		- client uses the public key from the server for encryption
	- server chooses cipher suite to use for client based on clients list

3] client checks certificate for validity

4] server sends a "Hello" done [meaning initial communication has been completed]

5] initiating key exchange - client generates a "pre-master secret" and encrypts it with the public key to send back to the server
	- client also sends a "change cipher spec" message

6] server uses private key to decrypt pre-master secret

7] client and server calculate the "symmetric encryption key"

8] client sends a "client finished" message, telling the server it is finished create its symmetric encryption key

9] server sends a "change cipher spec" message to client so that both can alter the secret writing sent between them
	- also sends a finished message for its symmetric key generation

10] communication between server and client is now symmetric
	- AES encryption as opposed to RSA or DHEC in the first 9 steps for asymmetric communication
	- data in greater amounts can now flow between client and server



# TLS 1.3 handshake overview
# Source: F5 DevCentral - https://www.youtube.com/watch?v=yPdJVvSyMqk

1] client sends a "Hello" message with supported ciphers and key agreements
	- calculates a key share pre-emptively
		- will calculate/assume what cipher suite the server will choose based on its list

2] server will respond with "Hello" message
	- chooses cipher suite
	- generates its own key share

- Note: if server does not have any of the supported ciphers in the client list
	- server sends a "Hello" retry to client
	- if theres still no match in ciphers, server will abort the connection and let the client know

3] server sends above + signed certificate back to client

4] server generates a symmetric key, sends a finished message to client

5] client generates a symmetric key, sends a finished message to server

6] normal traffic between client and server begins




----
stat
----

- Linux/UNIX - displays detailed information about given files or file systems


#### Commands ####

stat /bin/passwd  				# show detailed info on passwd file (inlcuding permissions, size, modification time, etc)



#### Sample output ####

stat file.txt

  File: file.txt
  Size: 4030      	Blocks: 8          IO Block: 4096   regular file
Device: 801h/2049d	Inode: 13633379    Links: 1
Access: (0644/-rw-r--r--)  Uid: ( 1000/   linuxize)   Gid: ( 1000/   linuxize)
Access: 2019-11-06 09:52:17.991979701 +0100
Modify: 2019-11-06 09:52:17.971979713 +0100
Change: 2019-11-06 09:52:17.971979713 +0100
 Birth: -



-------
storage
-------

# Block vs File Storage: very traditional vs. the newer storage types

Block
	- accessed through SAN (storage area network)
	- lowest possibly latency
	- high performing
	- highly redundant


File
	- accessed via NAS (network area storage - all servers connecting to one place vs. san routing you to the right storage device)
	- highly scalable
	- accessible to multiple runtimes
	- simultaneous read/writes from multiple users


If you need:
	- boot volume - use block storage
	- lowest latency - block storage
	- mix of structured and unstructured data - file storage
	- share data with many users at once - file storage



# NAS vs SAN

NAS - Network Attached Storage
		- store data in a centralized location accessible to all devices
		- just stores data, thats it
		- will have multiple hard drives in a RAID configuration
		- will have NIC to connect to router so its network accessible, accessed as a shared drive
		- small to medium scalability
		- disadvantage:
			- single point of failures


SAN - Storage Area Network
		- special high-speed network, stores and provides access to large amounts of data (dedicated for data storage)
		- multiple disk arrays, switches, and servers
		- advantages:
			- shared, therefore fault tolerant
			- recognized as a single drive from an OS
			- interconnected with fiber channel (from 2gbps to 128gbps - ultra fast)
			- not affected by network traffic
			- highly scalable
			- very redundant
		- disadvantage
			- fiber channel expensive (alternative iSCSI, cheaper but not as fast)
			- very expensive in general


----
swap
----

# General info
# Source: https://opensource.com/article/18/9/swap-space-linux-systems

- the primary function of swap space is to substitute disk space for RAM memory when real RAM fills up and more space is needed.


#### Commands ####

swapon -s       			# shows swap info 
swapoff -a && swapon -a  	# clear swap space



----
sync
----

- Linux/UNIX - synchronize data on disk with memory. Clears the filesystem buffer.


#### Commands ####

sync; echo 1 > /proc/sys/vm/drop_caches 			# clears the PageCache 
sync; echo 2 > /proc/sys/vm/drop_caches  			# clears dentries and inodes
sync; echo 3 > /proc/sys/vm/drop_caches 			# clears PageCache, dentries and inodes

#### Notes ####

- "echo 1" clears the PageCache
- "echo 2" clears the dentries and inodes
- Avoid using the "echo 3" command as it clears PageCache, dentries, and inodes.

- dentry - [Linux] in-memory representation of a directory entry



------
sysctl
------

- Linux/UNIX - configure kernel parameters at runtime


#### Commands ####

# Set OOM killer
sysctl vm.panic_on_oom=1
sysctl kernel.panic=5
echo "vm.panic_on_oom=1" >> /etc/sysctl.conf
echo "kernel.panic=5" >> /etc/sysctl.conf

	- The kernel.panic setting defines the number of seconds before the system can reboot.


# Disable OOM
sysctl vm.overcommit_memory=2
echo "vm.overcommit_memory=2" >> /etc/sysctl.conf


#### NEEDS ADDITIONAL EXPLANATION/CONTENT #####





------
syslog
------

- Linux/UNIX - read and/or clear kernel message ring buffer; set console_loglevel

- comprises of two key processes: 'rsyslogd' and 'klogd'.
	- 'rsyslogd' process is responsible for logging events from user processes. 
	- 'klogd' process is used for logging events from the kernel.


#### /etc/rsyslog.conf - config for rsyslogd ####

- 'Modules' - contains various configuration directives that are loaded when the module is loaded.
- 'Global directives' - apply to the rsyslogd daemon and start with $. One line contains one global directive.
- 'Rules' - defines the cooperation of selector and action. A selector filters messages based on facility and priority. An action defines what needs to be done with the filtered messages.
- 'Templates' - specify the format a user may need. You can also use them for dynamic file name generation.
- 'Filter conditions' - can use one of the three types of filter conditions: RainerScript-based filters, severity and facility-based selectors, and property-based filters
- 'Output channels' - defines the type of output a user wants


#### Commands ####

ps ax | egrep -i "(syslogd|klogd)"  				# ensure both syslogd and klogd are running
netstat -anp | grep -i ":514" /etc/rsyslog.conf 	# check whether port 514 configured rsyslog to log events 

/usr/sbin/logrotate             					# the logrotate command
logrotate -s /var/log/logstatus /etc/logrotate.conf # run logrotate and write status to a file
/etc/cron.daily/logrotate                           # shell script that executes the logrotate command on a daily basis


#### logger commands ####

logger -i -p mail.err "Error Message" 				# log "Error Message" to the /var/log/maillog file with the process ID, user account, and time stamp for the error message
logger System reboot 								# log System reboot message (for tracking of when system reboots occurred)


#### journalctl commands ####

- journalctl - replacement for syslog

journalctl 						  # runs/views journal (hit q to go back to command prompt)

journalctl -b 					  # view boot events/messages only
journalctl --list-boots 		  # list details on previous boot(s)
journalctl --boot=[boot_hash_id]  # list logs on specific boot

journalctl -p crit 				# view the logs filtered based on priority
journalctl --since=2018-12-01 	# view logs starting from a certain timestamp




#### Files/directories/configs ####

/var/log  						# system log files directory
	- boot.log
	- cron
	- yum.log

/etc/logrotate.conf 			# config file for logrotate daemon
	- maintains:
		- when to rotate log files
		- what to do after rotating old log files
		- the naming convention for the rotated log files

/etc/logrotate.d/samba 			# log rotation information for the samba package

/etc/logrotate.d 				# contains the log rotation information for installed packages

/var/log/maillog 				# mail log file

/etc/systemd/journald.conf  	# contains the configuration settings for journald





-------
systemd
-------

# Basic setup
# Source: Engineer Man - https://www.youtube.com/watch?v=unIAGt5pB7A&list=WL&index=3

- organizes programs in units, uses unit files to manage their state
- systemd basically auto-manages programs you define for you by auto starting them when they go down, when the system goes down

1) Create a unit file [e.g. somename.service] to define your service:

# File contents

[Unit]
Description=SomeReallyImportantService

[Service]
Type=simple
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]
WantedBy=multi-user.target


# Or can use more complex file contents below

[Unit]
Description=SomeReallyImportantService
Wants=something.service dependedon.service
Requires=dependedon.service
After=something.service

[Service]
Type=simple
User=root
Group=root
Environment=SOMEVAR=someval
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]     ############################################## need to learn more on this section ###################################################
WantedBy=multi-user.target


# Above sections
- Description - describes the service [info]
- Wants - also starts services listed other than this service
- Requires - like Wants, except if a service listed fails to start, this service will also fail to start
- After - starts listed service[s] first before starting this service

- Type - defines how the process starts up     # different types defined here: https://www.freedesktop.org/software/systemd/man/systemd.service.html#
- User and Group - start service as specified user and group
- Environment - add variables to inject into start up
- WorkingDirectory - for temp or generated files
- ExecStart - the path to the script/program to run as part of this service
- TimeoutSec - up to specified number of seconds to start
- Restart - defines restart policy [e.g. always]
- RestartSec - how long to wait before restarting, if service goes down


2) Save the file in /etc/systemd/system/
	- note: /lib/systemd/system/ contains all the service files


3) See below the different commands to manage the service

systemctl start [service] 
systemctl stop [service]
systemctl reload [service]
systemctl restart [service]
systemctl reload-or-restart [service]
systemctl enable [service] 							
systemctl disable [service] 	  # If service is disabled, service will not start upon system boot/reboot
systemctl is-active [service]
systemctl is-enabled [service]
systemctl daemon-reload           # Reload systemctl daemon after
systemctl list-units
systemctl list unit-files         # Lists all services and their status'
								  # See following link for possible statuses: https://www.freedesktop.org/software/systemd/man/systemctl.html



# Advanced configuration of systemd
# see also : https://www.freedesktop.org/software/systemd/man/systemd.unit.html

- multi-user.target - if service is enabled, tells systemd at what run level should this service start [above example corresponds to run level "2"]


# Rough Guide on Run Levels - Linux Standard Base [LSB] 4.1.0 [see Wiki on "Runlevel"]
ID			Name																	Description
0				Off																		Turns off the device.
1				Single-user mode											Mode for administrative tasks.
2				Multi-user mode												Does not configure network interfaces and does not export networks services.
3				Multi-user mode with networking				Starts the system normally.
4				Not used/user-definable								For special purposes.
5				Full mode															Same as runlevel 3 + display manager.
6				Reboot																Reboots the device.




----
tail
----

- Linux/UNIX - view the last few lines of a file


#### Commands ####

tail -n10 yum.conf 			# view last 10 lines of yum.conf




---
tee
---

- Linux/UNIX - reads from standard input and writes to standard output or to a file


#### Examples ####

df -h | tee disk_usage.txt                       # df -h output gets written into disk_usage.txt
command | tee file1.out file2.out file3.out      # write command output to multiple files




------
telnet
------

# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=tZop-zjYkrU

- Linux/MacOS/Windows/UNIX - aka "Teletype Network"
						   - developed in 1969
						   	  - all commands are in cleartext
						   	  - no encryption
						   	  - not to be used over the internet
						   - a terminal emulation program that is used to access remote servers
						   - a simple command-line tool that will allow you to send commands remotely to a server and administer that server just as if you
	  						 were sitting in front of it


#### Usage (high-level) ####
	- remotely run programs, delete files, create folders, stop/start services, etc. on the target machine
	- manage and configure other network devices such as routers and switches
	- check if ports are open or closed on a server (common) 


#### Setup - Windows 10 ####

1] Click on Start Menu -> Windows System -> Control Panel

2] Click on "Turn Windows Features on or off"

3] Check the box next to "Telnet Client" and save



#### Fun Commands ####

telnet towel.blinkenlights.nl  			# shows ASCII version of Star Wars
telnet freechess.org 					# play telnet version of chess
telnet rainmaker.wunderground.com 		# Enter city code and you'll get the weather for that city


##### NEEDS UPDATES/MORE COMMANDS #####



-----------------
terraform - azure
-----------------

Source: https://www.youtube.com/watch?v=bHjS4xqwc9A

1) Get an Azure subscription (can be free)

2) Authenticate with Azure (best to use Azure CLI - can download from Docker Hub)
	docker run -it --rm -v ${PWD}:/work -w /work --entrypoint /bin/bash mcr.microsoft.com/azure-cli:2.6.0
	az login
	export TENANT_ID=<your_tenant_id>

3) List subscription details and set
	az account list -o table
	SUBSCRIPTION=<id>
	az account set --subscription $SUBSCRIPTION


4) Create service principal

SERVICE_PRINCIPAL_JSON=$(az ad sp create-for-rbac --skip-assignment --name aks-getting-started-sp -o json)

SERVICE_PRINCIPAL=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.appID')
SERVICE_PRINCIPAL_SECRET=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.password')    # Note: keep appID and password for later use

	# where jq is a cli which is like sed for JSON data


5) Assign a role to the newly created service principal

az role assignment create --assignee $SERVICE_PRINCIPAL \
--scope "/subscriptions/$SUBSCRIPTION" \
--role Contributor


6) Install Terraform CLI

curl -o /tmp/terraform.zip -LO https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip
unzip /tmp/terraform.zip
chmod +x terraform && mv terraform /usr/local/bin/


7) Create terraform files called main.tf and variables.tf

# main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}



8) Initialize terraform

terraform init     	# will create a .terraform folder, will have all the downloaded plugins, metadata, etc.


9) Create terraform plan with the files created in previous step

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION

# terraform will keep an in-memory record of the state of the plan that has been applied


10) Create a terraform module for your AKS cluster, with its own variables.tf file

mkdir -p <dir>/.terraform/modules/cluster


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


11) Create a cluster.tf file and define a resource group and a cluster definition

# cluster.tf file contents

resource "azurerm_resource_group" "aks-getting-started" {
	name 		 = "ask-getting-started"
	location = var.location
}

resource "azurerm_kubernetes_cluster" "aks-getting-started" {
	name 									= "aks-getting-started"
	location 							= azurerm_resource_group.aks-getting-started.location
	resource_group_name		= azurerm_resource_group.aks-getting-started.name
	dns_prefix						= "aks-getting-started"
	kubernetes_version		= var.kubernetes_version


  default_node_pool {
  	name 			    	= "default"
  	node_count    	= 1
  	vm_size					= "Standard_E4s_v3"
  	type  					= "VirtualMachineScaleSets"
  	os_disk_size_gb = 250
  }

  service_principal {
  	client_id = var.serviceprinciple_id
  	client_secret = var.serviceprinciple_key
  }

  linux_profile {
  	admin_username = "azureuser"
  	ssh_key {
  			key_data = var.ssh_key
  	}
  }

  network_profile {
  		network_plugin = "kubenet"
  		load_balancer_sku = "Standard"
  }

  addon_profile {
  	aci_connector_linux {
  		enabled = false
  	}

  	azure_policy {
  		enabled = false
  	}

  	http_application_routing {
  		enabled = false
  	}

  	kube_dashboard {
  		enabled = false
  	}

  	oms_agent {
  		enabled = false
  	}
  }
}

# see azure provider page and azurerm kubernetes cluster page on the terraform website for more details


12) Generate an SSH key for access to the cluster

ssh-keygen -t rsa -b 4096 -N "VeryStrongSecret123!" -C "your_email@example.com"
export SSH_KEY=$(cat ~/.ssh/id_rsa.pub)


13) Update the variables.tf sitting on the same level as the main.tf file (outside of the cluster folder) with the below

# Updated variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}

variable "ssh_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


14) Re-apply updated terraform plan

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


15) Update main.tf with the cluster module information

# Updated main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}

module "cluster" {
	source 								= "./modules/cluster/"
	serviceprinciple_id		= var.serviceprinciple_id
	serviceprinciple_key  = var.serviceprinciple_key
	ssh_key								= var.ssh_key
	location							= var.location
	kubernetes_version		= var.kubernetes_version
}


16) Re-initialize terraform (confirm if previous plan needs to somehow be wiped first?)

terraform init


17) Re-apply updated terraform plan (this will create 2 plans, 1 for the cluster, 1 for the resource group)

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


18) Add a new resource to demonstrate terraforms capability of updating infrastructure changes via code

# Add to above cluster.tf contents

resource "azurerm_kubernetes_cluster_node_pool" "monitoring" {
	name 										= "monitoring"
	kubernetes_cluster_id 	= azurerm_kubernetes_cluster.aks-getting-started.id
	vm_size									= "Standard_DS2_v2"
	node_count							= 1
	os_disk_size_gb					= 250
	os_type									= "Linux"
}


19) Update cluster.tf values, such as change node_count to 2 in default_node_pool

20) Re-run the same terraform apply command as in step 17 (you will be prompted to say yes or no to apply the changes)

21) Create a new k8s module (create k8s folder under modules first) and a corresponding variables.tf 

# k8s.tf file contents

provider "kubernetes" {
	load_config_file				= "false"
	host 										= var.host
	client_certificate 			= var.client_certificate
	client_key							= var.client.key
	cluster_ca_certificate  = var.cluster_ca_certificate
}


# variables.tf file contents

variable "host" {
}

variable client_certificate {
}

variable client_key {
}

variable cluster_ca_certificate {
}


22) Define a k8s deployment in the k8s.tf file

# Snippet example to put under the provider "kubernetes" section in the file

resource "kubernetes_deployment" "example" {
	metadata {
		name = "terraform-example"
		labels = {
			test = "MyExampleApp"
		}
	}

  ...
  ...
  ...

	}


23) Add a service definition to the k8s.tf file to expose the deployment in step 22

# Snippet example to put under kubernetes_deployment resource section

resource "kubernetes_service" "example" {
	metadata {
		name = "terraform-example"
	}
	spec {
		selector = {
			test = "MyExampleApp"
		}
		port {
			port         = 80
			target_port  = 80
		}

		type - "LoadBalancer"
	}
}


24) Update main.tf with module information required by the new k8s module

# Add to above main.tf file contents

module "k8s" {
	source                 = "./modules/k8s/"
	host                   = "${module.cluster.host}"
	client_certificate     = "${base64decode(module.cluster.client_certificate)}"
	client_key				     = "${base64decode(module.cluster.client_key)}"
	cluster_ca_certificate = "${base64decode(module.cluster.cluster_ca_certificate)}"
}


25) In the cluster module, create a new file called outputs.tf (which terraform uses to grab values from to match with variables we defined. In this case we spit out the contents of kube_config)

# outputs.tf file contents

output "kube_config" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config_raw
}

output "cluster_ca_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.cluster_ca_certificate
}

output "client_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_certificate
}

output "client_key" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_key
}

output "host" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.host
}


26) Go to cluster.tf and remove the monitoring node pool and update default_node_pool value back to 1

27) Re-apply the terraform plan - this will:
	- add 2 plans (the kubernetes deployment and service)
	- update 1 plan (the default_node_pool resource)
	- destroy 1 plan (the monitoring node pool resource)

Note: add a -out param to the terraform apply command to save a plan that you are applying


28) Get cluster credentials to put into kubeconfig

az aks get-credentials -n aks-getting-started -g aks-getting-started


29) Use kubectl to verify what youve set up



-------------
terminal/tty?
-------------

- Ctrl and + together - increase screen size
- Ctrl and - together - decrease screen size

#### NEEDS UPDATES ####




----
test
----

- Linux/UNIX - checks file types and compares values


#### Commands ####

test 100 -lt 99 && echo "Yes." || echo "No."  			# will print No because 100 is not less than 99
test -e /etc/.bashrc  									# check if file exists
	echo $?    											# check return code of last command (if file exists, will print 0, if not, will print 1)




------------
thread dumps
------------

# Using jstack - e.g. can be used for capturing a thread dump for:

- during a performance load test
- after or during an app crash


$JAVA_HOME/bin/jstack -l [PID]>thread-dump-$(date +%m-%d_%H%M%S)_`hostname`_`whoami`



# Using top and kill -3 in a for loop (example JIRA application running on Tomcat)

1.	Identify the process ID (pid) of the tomcat process running JIRA by executing the following command: 
2.	ps aux | grep jira
3.	Execute the following command (be sure to replace both occurrences of $JIRA_PID with the pid you identified in the previous step): 
4.	for i in $(seq 6); do top -b -H -p $JIRA_PID -n 1 > jira_cpu_usage.`date +%s`.txt; kill -3 $JIRA_PID; sleep 10; done
5.	That script will run for one minute, during which it will generate top output of the process threads to six text files in the current directory.




-----------
timedatectl
-----------

- Linux/UNIX - used to control system date and time. Can also be used to check whether NTP synchronization enabled (i.e. for automatically setting the time)


#### Commands ####


timedatectl 				# check date and time settings of the system
timedatectl set-ntp no  	# stops the time synchronization with the NTP server (which allows you to set the time manually)



#### Other ####

# localtime/timezone settings

ls /usr/share/zoneinfo/        			# list all time zones
cp America/Chicago /etc/localtime		# change the time zone to Chicago time


# ntp settings - minor updates required*******

- Network Time Protocol (NTP) is used to set and synchronize the clocks of one or more systems that are on a network
	- Synchronizes the PC clock with local or NTP servers
	- Adjusts the rate of the kernel’s clock-tick to track time accurately
	- Synchronizes time with the other NTP clients


yum install ntp                         # install ntp package (may require yum or dnf update first)
cat /etc/ntp.conf                       # ntp daemon's configuration file
ntpq -n pool.ntp.org                    # The ntpq command prompt is displayed and prints list of known peers
	peers 								# See peers list




---
top
---

- Linux/UNIX - provides a dynamic real-time view of a running system (for processes including cpu and mem usage, total number of processes running, etc)


#### Commands ####

top              # show real-time view of all running processes (updates every second)
top -u root      # show for root user only
	shift + P    # sort by CPU utilization
	c  			 # display the absolute path of listed processes
	d 			 # change refresh interval. NOTE: enter a number (in seconds)
	k  			 # kill a process while top is running. NOTE: enter a PID




--
tr
--

- Linux/UNIX - translate the format of characters of a source file into a specified format of characters in a destination file


#### Commands ####

cat yum.conf | tr a-z A-Z 			# translate all lowercase to uppercase characters in yum.conf


----------
traceroute
----------

- Linux/UNIX - a command-line utility used to show the exact route that is taken by data packets as they travel across the internet to their destination
			 - helps to find bottlenecks in routes
			 - vs. ping - tells you general connnectivity to destination and amount of time. traceroute shows more info.
			 - Linux/MacOS CLI command is "traceroute", Windows is "tracert"
			 - note: traceroute requires root, tracepath does not


- time to live [TTL] - the max number of hops a trace will do [can be modified]
					 - prevents data packets from traveling endlessly around the internet
					 - e.g. misconfigured routers tied together in a loop that receive a trace with no TTL might endlessly pass data packets and slow down a network


#### Commands ####

tracert google.com      # show trace to google.com (Windows)

	- sends 3 data packets to each router on its way to the destination
		- helps isolate false issues [instead of sending just 1 packet]
	- router sends back the 3 data packets with info about that router:
			- number of hops to router
			- time/duration of round trip
					- increases with each hop since each hop is further and further, towards destination
					- network slowness: if theres a big difference between hops, could indicate a problem or the distance between two particular hops/routers is very large
					- request timed out: indicates problem or
						- if the trace still goes all the way through and request timed out happens at one hop/router, it could be because that router wasnt configured to return info back to the source
			- IP of router

tracert -h 4 google.com    # set max hops to 4 - if destination is further than 4 hops, destination will not be reached




traceroute [ip_address]            # trace the path from host to a destination, may take multiple hops
traceroute6 [ip_address]            # trace the path from host to an IPv6 destination, may take multiple hops





---------------
troubleshooting
---------------

***Kubernetes deployments - general approach***

1. Deployments
2. Pods
For both check:
	a) status
	b) events

- Commands
	kubectl get deploy
	kubectl logs <pod_name>
	kubectl describe deploy <deployment_name>
- failure examples:
	- pod not able to pull image (e.g. its missing)
	- liveliness probe is failing (e.g. the URL doesnt exist)
- status examples and possible causes:			
	- status: ContainerCreating
		- could be stuck because config map missing
	- status: CrashLoopBack
		- pod attempts to continually restart but cannot, usually an app or image issue.
	- status: Pending
		- insufficient memory
		- image may not run (e.g. try docker run on it outside a pod)
	- pods running, but not reachable
		- check selector label in service def matches app selector label
			- kubectl get svc
	- connection refused
		- endpoint may have wrong targetPort (check yaml for port and targetPort)



***Generating Thread Dumps for troubleshooting performance issues*** (see also thread dumps section)

1.	Identify the process ID (pid) of the tomcat process running JIRA by executing the following command: 
2.	ps aux | grep jira
3.	Execute the following command (be sure to replace both occurrences of $JIRA_PID with the pid you identified in the previous step): 
4.	for i in $(seq 6); do top -b -H -p $JIRA_PID -n 1 > jira_cpu_usage.`date +%s`.txt; kill -3 $JIRA_PID; sleep 10; done
5.	That script will run for one minute, during which it will generate top output of the process threads to six text files in the current directory.
6.	Attach the files generated by the script to this issue.
7.	Generate and send a complete support zip.



***Performance issues in a web app***

- in Chrome or whichever browser, open the console and check the performance tab, see if memory usage and see if it keeps climbing
- go to memory tab and take a snapshot of the memory values of each page or call



--------
truncate
--------

- truncate -s 0 something.txt 			# truncate target file, in this case, to 0 size, removing all content




---
tty
---

- Linux/UNIX - teletypewriter - prints the name of the terminal you’re using
							  - commonly used to check if the output medium is a terminal


# details

- A user can toggle between the TTY consoles using the Ctrl+Alt+F[1-6] keys
- Even though there are six TTY consoles, they are created on the fly
- "systemd" process in CentOS and many other flavours of Linux manages the TTY consoles


#### Commands ####

tty 						# print current terminal being used [e.g. /dev/pts/0 could be the output]

Ctrl+Alt+F3  				# start TTY 3
Ctrl+Alt+F1 				# come back to original terminal
ps -ft pts/0 				# find out the process ID of TTY consoles
pkill -9 -t pts/0  			# force kill terminal process
ps -ef | grep tty 			# check for running tty processes



#### Files and configs ####

/etc/systemd/logind.conf  			#  control the use of TTY consoles in this file
	- "NAutoVTs" - Default setting is 6. You can set it to any number from 0 to 6. If set to 0, the automatic spawning of autovt services is disabled.
	- "ReserveVT" - Requires a positive integer.




----
type
----

- (Linux/UNIX) - allows you to locate special commands such as shell built-ins


#### commands ####

type type             # show type for "type" command. Output: type is a shell builtin
type yum              # yum is /usr/bin/yum




---
UFW
---

- Linux - also known as "Uncomplicated Firewall" - the users who are not familiar with firewall concepts can still use it.
 		- originated from Ubuntu
		- provides an interface to iptables, a host-based firewall. 



#### Commands ####

yum install epel-release -y  				# install epel-release on a RHEL OS
yum install --enablerepo="epel" ufw -y   	# install ufw package

ufw enable 									# enable ufw to start at system boot
ufw status 									# check status of ufw and currently set policies
ufw status verbose  						# provide more detail on status

ufw allow http  							# allow http connections
ufw allow 80   								# allow connections to port 80 (same as above)
ufw allow 80/tcp  							# add in protocol, separate rule from above
ufw deny 21   								# deny connections to port 21
ufw delete deny 21  						# remove deny rule above

ufw allow from 192.168.0.3  				# allow connections from target IP
ufw allow from 192.168.0.0/24  				# allow from range of IPs
ufw allow from 192.168.0.3 to any port 80 proto tcp 		# allow access from target IP to only port 80
ufw deny from 192.168.0.3 to any port 80 proto tcp  		# deny access from target IP to only port 80

ufw reset  									# reset back to default rules



#### Files and configs ####

/etc/ufw/before.rules   					# contains UFW rules to act on before acknowledging a request


#### before.rules config detail ####
	-A ufw-before-input -p icmp --icmp-type destination-unreachable -j ACCEPT
	-A ufw-before-input -p icmp --icmp-type source-quench -j ACCEPT
	-A ufw-before-input -p icmp --icmp-type time-exceeded -j ACCEPT
	-A ufw-before-input -p icmp --icmp-type parameter-problem -j ACCEPT
	-A ufw-before-input -p icmp --icmp-type echo-request -j ACCEPT

		- commenting the above would make it impossible for outside sources to ping the server (or any ICMP requests)



------
ulimit
------

# Sources
# https://www.thegeekdiary.com/understanding-etc-security-limits-conf-file-to-set-ulimit/
# https://ss64.com/bash/ulimit.html


- UNIX/Linux
	- set or report the resource limit of the current user.
	- user limits - limit the use of system-wide resources.


- Syntax
      ulimit [-HS] -a
      ulimit [-HS] [-bcdefiklmnpqrstuvxPRT] [limit]


Key 	limits item 	description
-S   					Set a soft limit for the given resource.
-H   					Set a hard limit for the given resource.

-a   					All current limits are reported.
-b   	?				Maximum socket buffer size.
-c   	core 			Maximum size of core files created. 
-d   	data 			Maximum size of a process'' data segment.                  
-e   	priority		Maximum scheduling priority ("nice") 
-f   	fsize 			Maximum size of files created by the shell(default option).
-i   	sigpending 		Maximum number of pending signals.
-k   	?  				Maximum number of kqueues that may be allocated.
-l   	memlock 		Maximum size that can be locked into memory. 
-m      rss 			Maximum resident set size (ignored in Linux 2.4.30 and higher)
-n   	nofile 			Maximum number of open file descriptors. 
-p      ? 				Pipe buffer size.
-P 		maxlogins		Maximum number of pseudoterminals.
-q      msgqueue 		Maximum number of bytes in POSIX message queues.
-r   	rtprio 			Maximum real-time scheduling priority.
-R      ? 				Maximum time a real-time process can run before blocking, in microseconds.
-s      stack 			Maximum stack size. 
-t      cpu 			Maximum amount of cpu time in seconds.
-T      ? 				Maximum number of threads.
-u   	noproc 			Maximum number of processes available to a single user.
-v   	? 				Maximum amount of virtual memory available to the process.
-x   	locks			Maximum number of file locks.


# Permament ulimit settings - where default settings for ulimits are kept on the OS

- in "/etc/security/limits.conf"

#[domain]        [type]  [item]  [value]

*               -       core             [value]
*               -       data             [value]
*               -       priority         [value]
*               -       fsize            [value]
*               soft    sigpending       [value] eg:57344
*               hard    sigpending       [value] eg:57444
*               -       memlock          [value]
*               -       nofile           [value] eg:1024
*               -       msgqueue         [value] eg:819200
*               -       locks            [value]
*               soft    core             [value]
*               hard    nofile           [value]
@[group]        hard    nproc            [value]
[user]          soft    nproc            [value]
%[group]        hard    nproc            [value]
[user]          hard    nproc            [value]
@[group]        -       maxlogins        [value]
[user]          hard    cpu              [value]
[user]          soft    cpu              [value]
[user]          hard    locks            [value]
some_user       soft    nproc            16384          # eg of a max no. of processes setting for some user


- [domain] can be:
	- an user name
	- a group name, with @group syntax
	- the wildcard *, for default entry
	- the wildcard %, can be also used with %group syntax, for maxlogin limit

- [type] can have the two values:
	  “soft” for enforcing the soft limits
	  “hard” for enforcing hard limits

- [item] can be one of the following:
	  core – limits the core file size (KB)
	  data – max data size (KB)
	  fsize – maximum filesize (KB)
	  memlock – max locked-in-memory address space (KB)
	  nofile – max number of open files
	  rss – max resident set size (KB)
	  stack – max stack size (KB)
	  cpu – max CPU time (MIN)
	  nproc – max number of processes
	  as – address space limit (KB)
	  maxlogins – max number of logins for this user
	  maxsyslogins – max number of logins on the system
	  priority – the priority to run user process with
	  locks – max number of file locks the user can hold
	  sigpending – max number of pending signals
	  msgqueue – max memory used by POSIX message queues (bytes)
	  nice – max nice priority allowed to raise to values: [-20, 19]
	  rtprio – max realtime priority



# Temporary settings via command line - examples

ulimit -Sn [number] 		# set soft limit for number of processes for the logged in user
ulimit -Hn [number]         # set hard limit for number of processes for the logged in user
ulimit -Sn 					# verify soft limit for number of processes for the logged in user
ulimit -Hn 					# verify hard limit for number of processes for the logged in user



-----
umask
-----

- Linux/UNIX - sets default permissions for newly created files and directories


#### Commands ####

umask  				# check umask of current directory
umask 777 			# set umask of current directory to rwx permission for user, group and other





-----
uname
-----

#### General ####

- in Linux, this command shows information about the kernel: name, version, distribution, release date, etc.


#### Commands ####

uname         # show kernel name (-s option also does this, hence it is the default option)
uname -a      # shows all kernel info based on every option that uname has available to it
uname -i      # prints the hardware platform
uname -n      # prints system hostname
uname -o      # prints OS name 
uname -p      # prints processor info
uname -r      # prints kernel release
uname -v      # prints kernel release date

# Alternatives

cat /proc/version      # also prints kernel information
dmesg | grep Linux     # also prints kernel information



----
uniq
----

- Linux/UNIX - only unique lines are displayed in the output and duplicate adjacent lines are deleted


#### Commands ####

uniq newfile.txt  				# show only unique lines in newfile.txt






-----------
update-rc.d
-----------

- Linux Ubuntu - activate, deactivate or modify a service startup.
			   - replacement for "chkconfig" utility 


#### Commands ####

sudo update-rc.d -f apache2 remove          # Remove apache2 service
sudo update-rc.d apache2 defaults           # Add apache2 service to list of programs to startup on boot
sudo update-rc.d apache2 defaults 90 90     # Define the start and kill priority (respectively) of apache2 service




---------------
user management
---------------

####### Linux ########

# Users
useradd [user_id]         			# add user to OS
passwd [user_id]          			# set password for new user
usermod  -e 12/12/2020 [user_id] 	# set expiration date for the new user's password
usermod -L [user_id]                # lock user account
chage -d 0 [user_id]  				# force user's password into expired state
userdel [user_id]					# delete user from OS

# Groups
groupadd -g 10000 plabuser  		# add group with name "plabuser" with group ID 10000
usermod -aG plabuser [user_id]      # add user to plabuser group
id [user_id]  						# show user and its primary group
groupmod -n plab plabuser           # change the name of a group (in this case from plabuser to plab)
groupdel plab 						# delete group plab


# Password info
cat /etc/passwd 					# Show all users, their groups and their passwords (encrypted)
cat /etc/shadow  					# Show all users plaintext password (viewable ONLY by root user)
cat /etc/group 						# Show passwords for groups
ls -latr /etc/skel                  # See default profiles that all new users will inherit upon creation

# Lockdown
touch /etc/nologin 					# Run this and only root will be able to login to the system. Should be a 0 byte file (no content)



# last command

last               		# displays history of logins of all users on the system
last [user_id]     		# displays history of logins of target user on the system
last -F [user_id]  		# dispalys complete timing of login timing of target user
last -f /var/log/wtmp   # retrieve last login data from wtmp file



# lastlog command

lastlog							 				# displays all users and their last logins
lastlog -b 0 -t 100  							# displays users who have logged in the last 100 days 
lastlog | grep Never | awk '{print $1}'         # users who have never logged in


# sudoers file

/etc/sudoers  						# usually a read-only file
	john ALL=NOPASSWD: /bin/vi  	# example of entry you might add. Grant john sudo access without prompting for password when invoked.


# w, who, whoami

who        		# displays the name of currently logged in user, the user's terminal, the login time of the user and host or IP from where the user is logged in
who -u     		# show list of all logged-in users
who -q -H  		# show the number of logged-in users
who -r    		# show runlevel of the system
who -a          # show complete detail of the logged-in user


w               # show complete detail of the logged-in user (alternative to who). Who they are and what they are doing (processes they are running)
w -s            # supress output for login time, JCPU and PCPU columns
w [user_id] 	# show complete detail of the target user





-------
vagrant
-------

- open-source software product for building and maintaining portable virtual software development environments
	- grouped under "Virtual Machine Management"


# Vagrantfile example for provisioning three virtual machines for a K8s cluster

# START VAGRANTFILE
# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV['VAGRANT_NO_PARALLEL'] = 'yes'

Vagrant.configure(2) do |config|

  config.vm.provision "shell", path: "bootstrap.sh"

  # Kubernetes Master Server
  config.vm.define "kmaster" do |node|

    node.vm.box               = "generic/ubuntu2004"
    node.vm.box_check_update  = false
    node.vm.box_version       = "3.3.0"
    node.vm.hostname          = "kmaster.example.com"

    node.vm.network "private_network", ip: "172.16.16.100"

    node.vm.provider :virtualbox do |v|
      v.name    = "kmaster"
      v.memory  = 2048
      v.cpus    =  2
    end

    node.vm.provider :libvirt do |v|
      v.memory  = 2048
      v.nested  = true
      v.cpus    = 2
    end

    node.vm.provision "shell", path: "bootstrap_kmaster.sh"

  end


  # Kubernetes Worker Nodes
  NodeCount = 2

  (1..NodeCount).each do |i|

    config.vm.define "kworker#{i}" do |node|

      node.vm.box               = "generic/ubuntu2004"
      node.vm.box_check_update  = false
      node.vm.box_version       = "3.3.0"
      node.vm.hostname          = "kworker#{i}.example.com"

      node.vm.network "private_network", ip: "172.16.16.10#{i}"

      node.vm.provider :virtualbox do |v|
        v.name    = "kworker#{i}"
        v.memory  = 1024
        v.cpus    = 1
      end

      node.vm.provision "shell", path: "bootstrap_kworker.sh"

    end

  end

end

# END VAGRANTFILE



--------
/var/log
--------

- Linux/UNIX - system wide log directory


#### Key subdirectories and files ####

/var/log/messages: contains general log messages
/var/log/boot: contains system boot log messages
/var/log/debug: contains debugging log messages
/var/log/auth.log: contains user login and authentication logs messages
/var/log/daemon.log: contains running services messages
/var/log/dmesg: contains Linux kernel ring buffer log messages
/var/log/dpkg.log: contains binary package log including package installation messages
/var/log/faillog: contains failed login log messages
/var/log/kern.log: contains kernel messages
/var/log/lpr.log: contains printer messages
/var/log/mail.*: contains mail server messages
/var/log/mysql.*: contains MySQL server messages
/var/log/user.log: contains userlevel logs messages
/var/log/xorg.0.log: contains X.org messages
/var/log/apache2/*: contains Apache Webserver messages
/var/log/lighttpd/*: contains Lighttpd Webserver messages
/var/log/fsck/*: contains fsck messages
/var/log/apport.log: contains application crash report messages 



--
vi
--

- [Linux] a command line editor for files

To start VI:
$ vi <filename>
Vi has 2 modes: input mode (where you just get to type into your file), and command mode. Keep reading.


### Quick guide ###

:w    # To save work

:q    # To quit

:wq   # Save and quit
ZZ    # also save and quit
:x    # also save and quit (not available on some OS')

:e! somefile.txt     # close current file without saving and open new file called "somefile.txt"

# Moving around:
h     # left
j     # down
k     # up
l     # right
0     # beginning of line
$     # end of line (shift??)
^     # start of line (shift??)
H 	  # top of screen
L  	  # bottom of screen
G 	  # bottom of file
<n>G  # the n'th line of the file
3j    # Move down 3 lines
4k    # Move up 4 lines

1 then Shift+H    # Move to beginning of file
Shift+G           # Bottom of file


# Search (note: can use regex)
/tomorrow     # Search forwards for the word tomorrow
?tomorrow     # Search backwards for the word tomorrow

# Search and replace
:%s/left/LEFT/g    # replace all occurences of left with LEFT

w forward a word, counting punctuation
b backward a word, counting punctuation
W forward a word, not counting punctuation
B backward a word, not counting punctuation

^F <ctrl>F forward a screen
^B <ctrl>B backward a screen


:noh    # shuts off highlighting


- When invoked, VI is in command mode. You can return to command mode at any time by pressing the [ESC] key. The [ESC] key is also used to cancel a
  partially-entered command.

# To enter insert mode (actually enter text):
i inserts at (before) current cursor position
I inserts at beginning of current line
a appends following current cursor position
A appends to end of current line
o opens up a new line after the current line
O opens up a new line previous to the current line
^V <ctrl>V  To quote a character (insert a non-printing character)

# To delete text:
x deletes a single character (at the cursor)
d<unit> deletes curent textual unit, taken from above. So,
dw deletes word, up to punctuation
dW deletes word, ignoring punctuation
D$           # deletes from cursor to end of line
dd or D      # delete a line
S            # delete a line and go into insert mode in the deleted line
dL deletes to bottom of screen
dG deletes from current cursor position to end of file
dd deletes current line (entirely)

# Copy and paste
y[unit]     # copy
yy          # copy (or yank) current line to buffer
yw          # copy (or yank) word
Y copy to end of line (or y$)
p paste, following the cursor.
P paste, preceding the cursor.

# To change text:
c<unit> marks unit for overwrite, puts you in insert mode. So,
cw change current word
C change to end of line
cc change current line (entirely)
r replace current (single) character
~ changes case of current character
Other useful stuff:
J joins current line to line below
/<string> searches forward for <string>
n reexecutes last search
N same thing, but other direction
?<string> searches backward for <string>
^L <ctrl>L refreshes the screen



---
vim
---

# Sources:
# Vi vs Vim - https://www.shell-tips.com/linux/vi-vs-vim/#gsc.tab=0
# Full Vi vs Vim differences - https://vimhelp.org/vi_diff.txt.html#vi-differences
# Vim Macros - https://learnbyexample.github.io/vim_reference/Macro.html


- aka "Vi IMproved"

- Linux/UNIX - an implementation of vi with many additions
			 - aka "Vi IMproved"



#### List of additions to standard Vi ####

- Multi-level undo
	- undo multiple times [just like in Windows]
	- undo cache - up to 1000 actions 

- Tabs, Multiple windows and buffers
	- display files in separate windows and tabs

- Flexible insert mode
	- allowing you to use the arrow keys to move around in a file while in insert mode

- Macros
	- record a series of commands and repeat them any number of times

- Visual mode
	- Once a section is highlighted, you can perform operations on this section only

- Horizontal Scrolling
	- When the wrap option is turned off (:set nowrap), long lines of text can be scrolled horizontally

- Unicode and internationalization improvements
	- support for Unicode encoding and uses internally an utf-8 encoding. you can set encoding on a file using set encoding=utf8
	- can also display text right to left (RTL) for languages that require it using :set rightleft. You can reverse a text using :%!rev

- Advanced features for Power-users - heavily increase your velocity, improve your development workflow, and your ability to customize Vim.

		Text formatting
		Completion in Insert mode
		Jump tags
		Automatic commands
		Viminfo
		Mouse support
		Graphical User Interface (GUI)
		Scripting language
		Plugins
		Syntax highlighting for many programming languages
		Extended regular expressions
		Integrated Spell checking
		Diff mode
		Encryption using the blowfish algorithm
		Extensive customizable
		Packages
		Edit-compile-edit speedup
		Indenting for many programming languages
		Searching for words in include files
		Advanced text objects
		Folding
		ctags and cscope integration
		Integration of several programming languages
		Asynchronous I/O support
		Timers


- Command-line editing and history [improved in Vim]
	- search through your previous commands, repeat them, or edit them before executing them again

- Command-line completion [improved in Vim]
 	- allows you to use the Tab key to complete commands, options, and filenames as needed.




#### Commands ####

vim -O file1.txt file2.txt 			# open both files in separate windows 
vim -p filename  					# open each file in separate tabs

# while editing in vim
:help  								# full help system on Vim. Navigate between sections by positioning the cursor on a |tag| and hitting Ctrl+]
:help complex-repeat  				# info on macros



--------------
virtualization
--------------

# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=UBVVq-xz5i0

- definition: the process of simulating hardware and software in a virtual (software) environment

- usage: the traditional way a business operates is by having one machine for one application
	- can consolidate them on a single server (e.g. run 3 virtual machines, running 3 different OS, each running different applications)

- hypervisor - a software that creates and runs the virtualization 
			 - it allocates and controls the sharing of a machines resources (storage, CPU, memory)

- hypervisor types
	- Type 1: installed on empty, bare metal hardware (i.e. no existing OS or software installed)
	 	- egs. VMWare ESXi, Citrix XenServer, Microsoft Hyper-V

	- High-level setup steps
		1) Install virtualization software
		2) Allocate the servers hardware resources
	 	3) Install operating systems on each VM
	 	4) Install applications on each VM
	
	- Type 2: installed on top of an existing operating system
		- typically used on personal computers
			- to test new software
			- to try out different operating systems
		- egs. Oracle VM VirtualBox, Microsoft Virtual PC, VMWare Workstation

- benefits
	- saves money on hardware and electricity
	- saves money on floor space
	- saves money on maintenance and management
	- portability (can transfer a VM from one server to another)
	- takes advantage of full computing capability of a machine
	- disaster and recovery (easy to recover a VM when VM file is spread across other machines as a backup)



------
vmstat
------

- Linux/UNIX - display various information, such as CPU, swap, and memory utilization



#### Commands ####

vmstat 						# generates reasonably good information for CPU. It generates information for CPU user time, system time, idle time, and wait time.
vmstat -a 					# show active and inactive memory
vmstat -f  					# show number of forks since last system boot
vmstat -s 					# shows same info more comprehensively
vmstat -m 					# show slab info - need root to run

vmstat 2 					# print vmstat info every 2 seconds (Ctrl+C to exit out)
vmstat 2 10 				# stop at 10 iterations
vmstat -t 2 10  			# show timestamp for each print



---
VPN
---

# Sources
# Powercert Animated Videos - https://www.youtube.com/watch?v=R-JUOpCgTZc

- Virtual Private Network - Establishes a secure and reliable network connection over an unsecure network such as the internet.
						  - protects your internet activity and disguises your identity on the internet
						  - disguises your identity by hiding your IP address 
						  - will encrypt and protect your internet data
						  - "tunneling" - encapsulates the data with an extra layer of protection
						  - enables you to access restricted internet content (e.g. in certain regions where you would normally be blocked)
						  - provides a secure connection for remote workers
						  	- coule be done using a leased [private] line
						  		- can be expensive depending on the distance







-----
watch
-----

- Linux/UNIX - execute a program periodically, showing output fullscreen


#### Commands ####
 
watch free -g  				# watch memory usage (in GB), update every 2 seconds by default



--
wc
--

- Linux/UNIX - print newline, word, and byte counts for each file


#### Commands ####

wc /proc/cpuinfo  			# prints number of lines, number of words, and number of characters in cpuinfo file
wc -l /proc/cpuinfo 		# prints only the number of lines



--------
webhooks
--------

Source: https://www.youtube.com/watch?v=rUaDIH5ZXB8&list=WL&index=2

Definition: http messages that are sent in response to an event to a third party service (e.g. you can configure a webhook in Github to send messages to Jenkins based on push events to a given repository)




-------
whereis
-------

- (Linux/UNIX) - allow you to locate more information than just the location of the command


#### commands ####

whereis ls            # shows location and more details of the "ls" command




-----
which
-----

- (Linux/UNIX) - shows the full path of shell commands
	- exception: shell builtins (see "type" section)
		- e.g. type - if you run "which type", it will not return the path since it is a shell builtin program


#### Commands ####

which yum 				 # show default path of yum
which -a yum             # show path(s) of yum  (e.g. you might see /usr/bin/yum and /bin/yum, two different locations)





-----
whois
-----

whois [domain_name]   # will return comprehensive information about that URL/domain (given that domain is publicly registered)



-------
Windows
-------

##### Windows Commands #####

# Sources:
# NetworkChuck - https://www.youtube.com/watch?v=Jfvg3CS1X3A
# NetworkChuck - https://www.youtube.com/watch?v=prVHU1fLR20

# Ensure you are in Administrator mode before executing the below

assoc 														# will tell you which file type is associated with what program
assoc .mp4=VLC.vlc 											# will associate mp4 files with VLC media player
chkdsk /f 													# will go through your disk and see if any errors (and fix them)
chkdsk /r 													# will go through your disk and check for physical sector errors (and fix them). May require reboot of machine
cls 														# wipes screen
DISM  /Online /Cleanup-Image /ScanHealth 					# Deployment Service Imaging and Management - will fix your system image if needed
DISM  /Online /Cleanup-Image /RestoreHealth  				# will restore system health if there are issues from the scan (run sfc command after to run another health check)
getmac /v 													# get MAC/physical address of machine
ipconfig 													# find your machine's IP address
ipconfig /all 												# includes MAC address, DNS server info
ipconfig /release 											# release your machine's IP address
ipconfig /renew												# reaches out to DHCP to assign a new IP to your machine
ipconfig /displaydns 										# displays all the sites your machine knows about (in your local DNS cache)
ipconfig /displaydns | clip     							# will copy the output to Clipboard so you can copy it into Notepad, Word, etc.
ipconfig /flushdns 											# will flush your DNS resolver cache and removing any old/state DNS entries
nslookup somedns.com 										# tells you which dns server you're using along with giving you the IP associated with the target address
nslookup somedns.com 8.8.8.8 								# same lookup except using a target DNS server (in this case, Google's DNS server)
nslookup -type=txt somedns.com  							# lookup other types of DNS records
netsh wlan show wlanreport									# generates pretty HTML report on your wireless network status
netsh interface show interface 								# show your network interfaces
netsh interface ip show address | findstr "IP Address" 		# show all IP addresses on your network
netsh interface ip show dnsservers 							# show all available DNS servers
netsh advfirewall set allprofiles state off 				# turn off Windows Defender firewall (wont work if youre running BitDefender)
netstat 													# check what connections are opened locally and to other servers/sites
netstat -af 												# check what ports you have opened
netstat -o													# check pids for each connection
netstat -e -t 5 											# show interface statisticss (e.g. bytes sent and received)
ping somedns.com 											# check to see if a site is up or down
ping -t somedns.com 										# keep continually pinging site
powercfg /energy 											# check if any power issues with your machine
powercfg /batteryreport 									# report to you in HTML format the state of your battery
route print  												# show routing table (routes your connections will take outbound from your machine)
route add <IP> mask <mask> <gateway_IP> 					# add a route
route delete <IP> 											# delete a route
shutdown /r /fw /f /t 0										# shutdown your PC and restart into BIOS
sfc /scannow 												# system file checker - will fix/replace system files if they are bad
tasklist | findstr <process> 								# print your task list (like Task Manager), narrow down results piping to findstr
taskkill /f /pid <PID> 										# force kill target process
tracert somedns.com 										# trace path to destination + show how long each hop towards the site took
tracert -d somedns.com 										# ignore resolving domain names


attrib +h +s +r foldername 									# hide foldername from view
attrib -h -s -r foldername 									# show or make foldername visible again
copy /b image.extension+folder.zip image.extension  		# hide a zip archive inside of an image file, e.g. copy /b untitled.jpg+myarchive.zip new_untitled.jpg
cipher /E 													# in the folder you're in, encrypt all files and directories
netsh wlan show profile 									# show every wifi network you've ever connected to
netsh wlan show profile wifinetwork key=clear				# show wifi network details AND the password to it
wmic path softwareLicensingService get OA3xOriginalProductKey 		# show original Windows product key (may not work on a system which has upgraded or re-installed Windows with a new version)

# Show wifi passwords for all wifi networks this PC has logged into
for /f "skip=9 tokens=1,2 delims=:" %i in ('netsh wlan show profiles') do @if "%j" NEQ "" (echo SSID: %j & netsh wlan show profiles %j key=clear | findstr "Key Content") >> wifipassword.txt


# Run Windows CMD as Administrator by default

1] Go to Start Menu -> search for "cmd" -> right click "Command Prompt" -> Open File Location

2] Right click Command Prompt shortcut -> Properties -> Advanced

3] Click checkbox next to "Run as administrator" and click Ok -> Apply -> Ok




#### Windows Spotlight images retrieval ####

- location: C:\Users\Buildmaster\AppData\Local\Packages\Microsoft.Windows.ContentDeliveryManager_cw5n1h2txyewy\LocalState\Assets>
	- from command prompt, cd to the above and then:

		cp * /d/new_folder     						# or some other drive/location
		cd /d/new_folder
		for i in $( ls ); do mv $i $i.jpg; done     # UNIX sh: rename all the files to jpg

		OR

		ren * *.jpg  								# rename all files to jpg files with Powershell




------
winget
------

# Sources:
# John Savill's Technical Training - https://www.youtube.com/watch?v=15fg5-I3CI4&pp=ygUndXNpbmcgd2luZ2V0IHBhY2thZ2UgbWFuYWdlciBpbiB3aW5kb3dz
# Github repo - https://github.com/microsoft/winget-pkgs


- Windows - package manager CLI tool, used to install programs into Windows environments

#### Commands ####

winget --info		 				# shows winget version plus configuration details
winget source list 					# show all remote sources for package installations (aside from list, options include add, remove, 										# update)
winget features 					# shows configuration features available/enabled/disabled for winget
winget settings 					# opens json file settings file
winget list 						# lists all installed programs (not necessarily installed by winget). Can use winget list <string> 										# to narrow the search
winget search -q code 				# search locally and in all soures for packages that include the word 'code'
winget show "microsoft powertoys" 	# shows all info about Microsoft PowerToys app
winget show "discord" --versions 	# show all avaialble versions of discord
winget install discord -v 1.0.9004  # install specified version of discord
winget upgrade 						# shows packages that have available newer versions to upgrade to
winget upgrade discord 				# downloads and upgrades the locally installed version of discord to the latest version 





----
yaml
----

- Yet Another Markup Language
	- is a data serialization language that is often used for writing configuration files.
	- has features that come from Perl, C, XML, HTML, and other programming languages. YAML is also a superset of JSON, so JSON files are valid in YAML.
	- uses Python-style indentation to indicate nesting
	- used for defining templates for different kubernetes resources (e.g. deployments, services, custom resource definitions)


# YAML mapping field definitions - taken from Marcel Dempers [That DevOps Guy]
# https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/monitoring/prometheus/kubernetes/1.18.4/prometheus-operator/crd-thanosruler.yaml

# Below are definitions of different fields and what they mean when you look at a Kubernetes template.


- apiVersion - defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values.
						 - value eg. "apiextensions.k8s.io/v1"
							- More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

- kind - a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to.
			 - Cannot be updated
			 - value is written In CamelCase, egs. "ReplicationController, CustomResourceDefinition, Pod"
			 	- More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

- spec - Specification of the desired behavior of the object, contains a complete description of the desired state, including:
			 		- configuration settings provided by the user
			 		- default values expanded by the system
			 		- properties initialized or otherwise changed after creation by other ecosystem components (e.g., schedulers, auto-scalers), and is persisted in stable storage with the API object.
          - More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status


 ##### MORE DEFINITIONS TO ADD + NOTES ON NESTING #####




---
yum
---

- package manager for RHEL7 and below Linux OS'' (or CentOS 7 and below). Can still be used on CentOS8, RHEL8, Rocky Linux

# Install a package on RHEL-based Linux (e.g. httpd)

yum install httpd        # install via yum package manager (can use dnf on CentOS 8/RHEL 8 and up, Rocky Linux). Will also install its dependencies
yum list httpd           # check httpd is installed, shows a little detail about the package
yum update  			 # check for an install updates for all installed packages (including dependencies)
yum update httpd         # check for and install updates to the http package
yum remove httpd         # remove httpd package and all its dependenies
yum info httpd           # show more details about httpd package
yum deplist httpd        # show dependencies list for http package
yum downloader squid     # download squid package RPM
yum check-update 		 # check if any updates for all installed packages
yum group list 			 # find the list of package groups on your system

yum --showduplicates list firefox    # check for multiple versions of a specific package
yum group info “System Management” 	 # find info about a specific group
yum group install kde-desktop 		 # install a specific package group
yum group update kde-desktop 		 # update a specific installed package group

reqoquery --list httpd   # list all files in httpd package (NOTE: need yum-utils package to be able to run) 


# yum configuration

cat /etc/yum.conf             # view current yum configurations
ls -latr /etc/yum.repos.d     # this is where yum repo information is kept, also referenced by yum.conf


# enable automatic security updates for yum packages

1] yum install yum-cron -y  	  # install yum-cron package

2] Edit /etc/yum/yum-cron.conf

	- from: update_cmd = default
	- to: update_cmd = security

	- from: apply_updates = no
	- to: apply_updates = yes

	- add: emit_via = email

	- add or edit: email_from = root@localhost
	- add or edit: email_to = root




------
zoxide
------

# source: Tech Craft - https://www.youtube.com/watch?v=2OHrTQVlRMg

- Linux - alternative to cd command


#### Commands ####

z ~/some_dir         # change dir to ~/some_dir

		#### NEEDS UPDATING ####