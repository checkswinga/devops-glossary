-------
Android
-------

# Starting out with an Android project [from a DevOps point of view]
# Source: Engineer Man - https://www.youtube.com/watch?v=R3eAMMBh2ng

1) Download Android Studio from the developer.android.com site and follow the instructions to install on your computer or server

2) Open Android Studio and in "Select A Project Template", choose whatever you like [e.g. Empty Activity]

3) In "Configure Your Project", youll need to configure the following:

	- Name: My Application
	- Package Name: com.hq.myapplication       # can leave as default
	- Save location: <whereever_you_like>
	- Language: Java or Kotlin
	- Minimum SDK: recommendation is Android 5.0 [Lollipop] or 6.0 [Marshmallow]       # going too low might make for problems and extra work down the road

- Side note: - designing layouts in Android [you will see the options when in Android studio]
	- purely with code
	- split between code and visual preview
	- purely drag and drop 


4) Explanations on different files and folders created:

- AndroidManifest.xml [under app/manifests]
	- metadata about the application
		- in the application block, see things like the text label, icons, activity block [screen that shows up on Android or a screen that might be switched to]


- java folder
	- contains all the java code for your application
	- other folders: one for main code, one for tests
	- example code: MainActivity.java under com.hq.myapplication


- java [generated] folder
	- never need to directly be concerned with it, though code gets generated here because of content or code you create elsewhere [such as a content xml in the layout folder]


- res folder [resources]
	- contains everything except java code itself

	- sub-folders
		- drawable folder
			- contains vector assets and images
		- layout folder
			- for layouts [e.g. menus, list items, etc.]
			- eg. activity_main.xml - eg. content xml file referenced by the MainActivity.java file


- mipmap folder
	- contains several variations of the same image [for different phone sizes]
	- in code, reference by the same name, and the phone figures out which image to use


- values folder
	- stores various constant values of different things such as colours and strings [e.g. color name="purple_200" matched to a hex value]
	- file: strings.xml - used in case app is gonna be in different languages


- themes folder
	- application wide theme [eg. light and dark themes to apply, phone can toggle depending on user choice]


- Gradle scripts
	- for building the application
		- build.gradle - has the build instructions


5) To test run your app, plug your phone into your computer and click the Play button in Android Studio



-------
Ansible
-------

#### How to automate your Mac setup ####

# Source: https://github.com/geerlingguy/mac-dev-playbook



-----------------
Apache web server
-----------------

# Install and run on CentOS Linux

yum install -y httpd           # update httpd yum package
systemctl start httpd
systemctl status httpd
systemctl enable httpd
firewall-cmd --zone=public --permanent --add-service=http
firewall-cmd --zone=public --permanent --add-service=https


### needs minor updates ###



---
apt
---

- Linux Ubuntu''s package manager - various other commands are used under apt that can be used to install packages, check details of installed packages, etc.

apt install [package]          # install package to OS (along with its dependencies). E.g. apt install apache

apt-get install [package]      # install package to OS (along with its dependencies)
apt-get remove [package]       # remove installed package from OS
apt-get autoremove [package]   # remove installed package's dependencies

apt-cache show [package]       # show package details

dpkg -S [package]              # find package details, location of installed package files



---
AWS
---

#### Top 50 List of Services ####
# Source: Fireship - https://www.youtube.com/watch?v=JIbIYCM48to&pp=sAQA

# Robots and Large Scale
- Robomaker - simulate and test your robots at scale (e.g. robot vacuums)
- IOT core - collect data from those robots, update their software, manage them remotely
- Ground Station - control satellite communications, process data and scale operations from satellites orbiting Earth
- Bracket - software used to interact with a quantum computer (i.e. to learn about the future of computing)

# Compute
- EC2 (Elastic Compute Cloud) - create a virtual computer in the cloud (OS, memory, computing power), for rent
- elastic load balancing - distribute traffic to multiple instances automatically
- Cloud Watch - collect logs and metrics from each individual instance
- Auto Scaling - create or scale down instances based on the metrics collected from Cloud Watch
- Elastic Beanstalk (PaaS) - interface for easily deploying code to AWS (including scaling, load balancing, etc.)
- Lightsail - even easier deployment tool for AWS, don[t have to worry at all about the underlying infrastructure
- Lambda (FaaS) - serverless, upload your code, decide when it should run (traffic, scaling, networking all work in background). Pay only when the app is used.
- Serverless Repo - if you dont like writing your own code, find pre-built functions to deploy with a click
- Outposts - Run AWS APIs on your own infrastructure (without tossing out your old servers)
- Snow - mini data centers that work without internet in hostile envs (e.g. the Arctic)

# Containers and Container Orchestration
- Container Registry - upload/store container images
- Elastic Container Service - run a container stored in the registry (Elastic Container Service (ECS) is the service for stopping, starting and allocating virtual machines for your containers
															and connect them to other products such as load balancers)
- EKS - Amazons Kubernetes service
- Fargate - make your containers behave like serverless functions (automatic resources)
- App Runner (2021) - deployment tool for AWS (choose your images to run, it takes care of running in AWS, including resource allocation)

# File Storage
- S3 (Simple Storage Service) - store any type of file or object
- Glacier - higher latency, lower cost storage (more for archiving, when access rate is low)
- Elastic Block Storage - fass, handle bigger throughput (more configuration required)
- Elastic File System - high performance, fully manager, much higher cost

# Database
- Simple DB - general purpose no SQL database
- Dynamo DB - document database easy to scale horizontally (cheap, scales easy, fast, but no joins and limited queries, not good at modelling relational data)
- Document DB - controversial: not mongoDB, but exactly like it to get around licensing
- Elastic Search - good for search
- Amazon RDS (Relational Database Service) - supports a variety of SQL flavours, fully manage backups, scale and caching
- Aurora - Amazons own proprietary version of SQL (compatible with Postgres/mySQL, better performance at a lower cost, easy to scale with new serverless option, only pay when in use)
- Neptune - graph database, high performance on highly connected data sets (e.g. social graph, recommendation engine)
- Elastic Cache - ultra fast database, fully managed version of Redis (in=memory database)
- Timestream - time series database
- Quantum Ledger - cryptographically signed transactions

# Analyitcs
- Redshift - data warehouse (shift away from Oracle)
- Lake Formation - tool for creating data lakes or repos that store a large amount of unstructured data (can be used in addition to DWs to query a larger variety of data sources)
- Kinesis - capture real time streams, view the captured data in a business intelligence tool
- Apache Spark (Elastic Map Reduce) - stream data from multiple platforms to a business intelligence tool for analysis
- MSK - AWS fully managed service version of Apache Kafka
- Glue - Auto ETL (Extract, Transform, Load), can connect to different kinds of databases

# Machine Learning
- Data Exchange - purchase/exchange data from third party sources for analysis
- Sagemaker - connect to data exchange, build machine learning models
- Rekognition - identify/classify images
- Lex - chatbot
- Deep Racer - actual miniature race car driven remotely with machine learning code

# Developer Essentials
- IAM - identity and access management, roles
- Cognito - enable to login with different auth methods
- Simple Notification Service (SNS)
- Simple Email Services (SES)
- Cloud Formation - templates based on infrastructure in yaml/json format (I assume like Terraform/Ansible)
- Amplify - provide SDKs to connect to infrastructure from front end apps

# Bonus
- AWS cost explorer - budgeting

# Another bonus 
- App Engine Standard - for hosting NodeJS applications, easily scalable



-------
backups
-------

Sources:
- PowerCert Animated Videos - https://www.youtube.com/watch?v=o-83E6levzM
- Jeff Geerling - https://www.youtube.com/watch?v=S0KZ5iXTkzg


#### General ####

- fault tolerance - the prevention of data loss if a component fails
- disaster recovery - the process of rebuilding an organizations data after a disaster


Comparison             Data thats backed up           Restore Procedure

- full                 All data (longest)		  	  Full backup only (fastest)

- incremental          Data thats been changed		  Full and incremetals (in the correct order) (longest)
											  		  since last full or incr.
											 		  backup (fastest)

- differential		   Data thats been changed		  Full and last differential (medium)
											 		  since the last full backup
											 		  (medium)


#### Example Strategy ####

- the "3-2-1 method"
	- 3 copies
	- 2 different media
	- 1 offsite

- Data Categories
	- photos
		- 1 on local
		- 2 on NAS
		- 3 on cloud

	- music
		- 1 on local 
		- 2 on NAS
		- 3 on cloud

	- videos
		- 1 on NAS
		- 2 on backup NAS
		- 3 on cloud (eg. AWS Glacier - cheap storage). Mostly for major emergencies only.

	- documents
		- 1 on local
		- 2 on NAS
		- 3 on dropbox

	- local files
		- 1 on local
		- 2 on NAS
		- 3 on ????
	
	- source code
		- 1 on local
		- 2 on NAS via Gickup (see https://github.com/cooperspencer/gickup)
		- 3 on Github


 #### Online tools to assist with above ####
 	- rclone - rclone.org
 	- arq - https://www.arqbackup.com
 	- restic - restic.net
 	- borg - borgbackup.readthedocs.io
 	- crashplan - https://www.crashplan.com



#### Aleem's current ####

- Data Categories
	- photos
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- music
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- videos
		- 1 on local (WD 1TB)
		- 2 on second local (Seagate 1TB)
		- 3 on OneDrive

	- documents
		- 1 on local
		- 2 on 2nd local
		- 3 on OneDrive

	- local files
		- 1 on local
		- 2 on 2nd local
		- 3 on OneDrive
	
	- source code
		- 1 on local
		- 2 on Github
		- 3 ?????


------
base64
------

# Encrypt a plaintext password

echo -n "password123" | base64 -i -     # Output: cFSzf3skacQrIrR=




----
bash
----

- UNIX/Linux - a UNIX shell and command language (replacement for Bourne shell). Runs and processes commands as text when logged into a Linux machine/server via terminal

- order of execution upon bash login to a terminal: /etc/profile > ~/.bash_profile > ~/.bash_login > ~/.profile. 
	- the bash_profile executes the bashrc file.
	- if .bash_profile does not exist, .bash_login is read. If .bash_login does not exist, .profile is read instead.

#### profiles ####

cat /etc/profile       # shows contents of system level settings and attributes for any user's profile e.g. PS1 setting, default size of command history to display, umask settings, etc.
					   # Note: bash reads /etc/profile if invoked interactively using --login option, or invoked as sh.
					   # applies to all shells, including bash

cat /etc/bashrc        # contains system-wide definitions for shell functionns and aliases, similar to /etc/profile. Applies only to bash whereas profile applies to ALL shells

cat ~/.bash_profile    # user configuration file in which the user environment can be configured. By default, some configuration is already defined, 
					   # but it can be changed or altered as per requirements.
					   


# scripts - light notes

bash ./some_script.sh     # one method used to execute a bash shell script
./some_script.sh          # another method to execute a bash shell script

------
cables
------


# Ethernet Cables
Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=_NX99ad2FUA

- unshielded twisted pair
	- most common type of ethernet cable (used more in homes)
	- 4 pairs of colour-coded wires twisted around each other to prevent electromagnetic interference (crosstalk)

- shielded twisted pair
	- same as UTP except has a foil shield that covers the wires (more protection against interference)
	- both use RJ-45 connector

- types of twisted pairs
	- straight (patch) cable
		- if both ends of a cable are using the same standard
		- allows signals to pass through from end-to-end
		- to connect computers to dissimilar devices together (e.g. from a PC to a modem)
		- most commonly used
	- crossover cable
		- if both ends of a cable are using 2 different standards
		- to connect to similar devices together

- wiring standards
	- 568A
		- order of wires: white-green, green, white-orange, blue, white-blue, orange, white-brown, brown
	- 568B (more commonly used)
		- order of wires: white-orange, orange, white-green, blue, white-blue, green, white-brown, brown
	

- categories
	- the difference being the max speed they can handle without interference/crosstalk
	- the numbers represent the tightness of the twist that are applied to the wires
		- CAT3 - 10 Mbps - obsolete
		- CAT5 - 100 Mbps - obsolete
		- CAT5e - 1 Gbps - Enhanced
		- CAT6 - 1 Gbps  - 10 Gbps (cable length under 100 meters)
		- CAT6a - 10 Gbps - Augmented
		- CAT7 - 10 Gbps - Added shielding to the wires (shielded twisted pair version of CAT6a)
		- CAT8 - 40 Gbps - ultimate copper cable, shielded (4 times faster than CA6a/CAT7)


------------
certificates
------------


### keytool and openssl ###

# Download and import website certs into JAVA store (eg. google's certs for dl.google.com/android repo):

openssl s_client -connect google.com:443 -showcerts | openssl x509 -out certfile.txt
keytool -importcert -alias globalproxy -file certfile.txt -trustcacerts -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storetype JKS

OR 

keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts
keytool -importcert -file proxy.crt -alias proxy -keystore /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security/cacerts

# check cacerts store

keytool -list -v -keystore /path/to/cacerts


# Remove certificate entry from keystore - from https://docs.oracle.com/cd/E19798-01/821-1751/ghleq/index.html

keytool -delete -noprompt -alias ${cert.alias}  -keystore ${keystore.file} -storepass ${keystore.pass}



---------
chkconfig
---------

- used to:
	- list current startup information of services or any particular service
	- updating runlevel settings of service
	- adding or removing service from management


### Commands ###

chkconfig --list              			# list running system-level services
chkconfig --del netconsole    			# remove the "netconsole" system-level service
chkconfig --add netconsole    			# add the "netconsole" system-level service
chkconfig --level 4 netconsole on       # configure "netconsole" at run level 4




=======
ciphers
=======

Cipher groups
-------------

Group 1: These cipher suites have perfect forward secrecy (ECDHE) and authenticated encryption (GCM):
	E.g.: ECDHE-RSA-AES128-GCM-SHA256, ECDHE-RSA-AES256-GCM-SHA384

Group 2:  Perfect forward secrecy, but do not have authenticated encryption.
	E.g.: ECDHE-RSA-AES128-SHA256, ECDHE-RSA-AES256-SHA384

Group 3: Authenticated encryption but no perfect forward secrecy:
	E.g. : AES128-GCM-SHA256, AES256-GCM-SHA384

Group 4: Standard algorithms but no perfect forward secrecy or authenticated encryption:
	E.g.: AES128-SHA256, AES256-SHA256
	
Group 5: Perfect forward secrecy, but use SHA-1: (Do Not Include if possible)
	E.g.: ECDHE-RSA-AES128-SHA, ECDHE-RSA-AES256-SHA

Group 6: Standard algorithms but use SHA-1: (Do Not Include if possible)
	E.g.: AES128-SHA, AES256-SHA, DES-CBC3-SHA

Group 7: RC4 gets its own special category: (Do Not Include. This is deviation)
	E.g.: RC4-SHA

Group 8: For the love of God, do not use these: (Do Not Include. This is deviation)
	E.g.: DES-CBC-SHA, RC4-MD5, IDEA-CBC-SHA



-----
CISSP
-----

# Source: boson.com CISSP 2021 curriculum (viewable online only via single user licence - $69 USD to buy the digital copy thru the site)

# Notes - these are limited based on content I felt needed to be re-emphasized for the exam. The rest of the content should be easier to absorb just going through digital copy online,
# but I will try to document more of the content that should be highlighted here.

- CISSP - Certified Information Systems Security Professional
- ISC² - International Information Systems Security Certification Consortium

- the ISC² Code of Ethics

	- Preamble

		1] The safety and welfare of society and the common good, duty to our principals, and to each other, requires that we adhere, and are seen
		to adhere, to the highest ethical standards of behaviour.

		2] Therefore, strict adherence to this Code is a condition of certification


	- The Four Canons of Ethics [in order of importance]

		1] Protect society, the common good, necessary public trust and confidence, and the infrastructure

		2] Act honourably, honestly, justly, responsibly, and legally

		3] Provide diligent and competent service to principals

		4] Advance and protect the profession


		- note: if theres ever a conflict between canons, the one that ranks the highest is the one to adhere to first


# ISC² CISSP Exam Prep Notes

- expect approx. 15% of the questions to be about security and risk management


# EU GDPR [2016]

- provisions
	- companies must inform authorities of major data breaches within 24 to 72 hours, depending on country 
	- each EU member nation must create a centralized data protection authority
	- individuals must have access to their own data
	- information regarding an individual must be transferrable to another service provider at the individuals request
	- individuals retain the "right to be forgotten" and have their information deleted if it is no longer required


# Import/Export Law

- ITAR - International Traffic in Arms Regulations - limits the export of military and defense items, including information regarding these items.
- EAR - Export Administration Regulations - applies to civilian goods and services that could have military or information security applications.


# Intellectual Property

- IP rights were first recognized by 2 treaties:
	- Paris Convention for the Protection of Industrial Property [1883]
	- Berne Convention for the Protection of Literary and Artistic Works [1886]

- IP protections are managed by World Intellectual Property Organization [WIPO]
	- established 1967, an agency of the U.N.

- Digital Millenium Copyright Act [DMCA], passed in 1998 by U.S. Congress to comply with WIPO treaties
	- prohibits the circumvention of copyright protections on digital media


# Intellectual Property Protections

- patents - protects the holders exclusive right to use, create and sell an invention.
		  - terms are typically 20 years

- trademarks - typically protect brandings, logos, slogans, anything that creates a distinction between one product and a similar product

- copyrights - protect art, music, literature, source code created by an individual or organization
			 - infringement is when the above is used without the permission of the owner
			 - copyright usually includes the year the work was copyrighted
			 - licenses enable a copyright owner to grant specific uses of the material to others 

- trade secrets - the protection of confidential information about how a product is created.


# Intellectual Property Attacks 


-------------------------------
CompTIA A+ Certifcation 220-801
-------------------------------

- see "CompTIA_Aplus_Certification_220-801_notes.txt"



----
cpio
----

- (Linux/UNIX) - a tool for creating and extracting archives, or copying files from one place to another


#### Commands ####

ls | cpio -ov > text.cpio        # List files in current dir and put result into cpio archive
cpio -iv < text.cpio             # Extract cpio archive into current directory. Note if the current dir contains
								 # the same file names as in the archive, the files will not be extracted.



-------
crontab
-------

# Sources:
- https://crontab.guru - translate any cron entry and tells you what its timing interval is
- https://www.youtube.com/watch?v=QEdHAwHfGPc - Engineer Mans cron video

# Of the 5 stars:
- 1st - minute (0 - 59)
- 2nd - hour (0 - 23)
- 3rd - day of the month (1 - 31)
- 4th - month (1 - 12)
- 5th - day of the week (0 - 6) (Sunday to Saturday; 7 is also Sunday on some systems)

# Format
* * * * * [user] [command]

# Quick examples

# every minute
* * * * * /path/to/script

# every minute as user tim
* * * * * tim /path/to/script

# every hour at the top of the hour
0 * * * * /path/to/script

# nightly at 11pm
0 23 * * * /path/to/script

# first of the month at 3pm, every month
0 15 1 * * /path/to/script

# every day at 8am, 10am, 12pm and 2pm
0 8,10,12,14 * * * /path/to/script

# every half hour
*/30 * * * * /path/to/script

# every tuesday at 8am
0 8 * * 2 /path/to/scropt

# first wednesday of each month
0 23 1-7 * 3 /path/to/script


# Edit crontab
crontab -e

# List crontab
crontab -l

# Run crontab (Debian/Ubuntu)
service cron start
ps -ef | grep cron



---------
curl/wget
---------

- (UNIX/Linux) a tool for transferring data to or from a server. Supports multiple protocols


#### Commands ####

# Download a file (eg. Artifactory, using credentials)
curl -u <user>:<password> http://artifactory_url:8081/artifactory/path/to/package/something.rpm -o ./something.rpm


# Upload a file (eg. to Artifactory w/ creds)
curl -X PUT -u <user>:<password> -T something.rpm "http://artifactory_url:8081/artifactory/path/to/package/something.rpm"


# recursive download with wget
wget --no-parent -r http://WEBSITE.com/DIRECTORY


# set insecure
echo insecure >> $HOME/.curlrc


# curl with telnet (in case telnet not available)
curl -v telnet://someurl.com:[port]



#### References and tools ####

https://reqbin.com/curl - excellent site for testing different curl commands (e.g. with POST, GET, POST JSON, PUT, DELETE, etc)



----
date
----

- (Linux) used to display system date and time


### Commands ###

date                		# default format: Tue Oct 10 22:55:01 PDT 2017
date "+%Y-%m-%d"    		# format displays as "YY-MM-DD". Good for putting into filenames.
date +%D -s 2019-12-20		# set the date to Dec 20 2019
date +%T -s 23:00:00 -u     # set the time to 11pm, using UTC format (-u flag)




---------------
default gateway
---------------

Source: https://www.youtube.com/watch?v=pCcJFdYNamc

- definition: a device that forwards data from one network to another [usually a router]
	- lets devices from one network communicate with devices on another network
	- "default" means the designated device is the first option thats looked upon when data needs to exit the network
	- to connect out to the internet from a local network, it has to go through a gateway, usually the default
	- not required for internetwork communication

	- an IP address consists of two parts: network address and host address
	- subnet mask: reveals how many bits in the IP address are used for the network by masking the network portion of the address

		- eg.
			- IP address  - 192.168.0.2   -   11000000.10101000.00000000.00000010
			- Subnet mask - 255.255.255.0 -   11111111.11111111.11111111.00000000
																				network  network network     host

			  - for each of the 4 digits, the sections with all 1s indicate the network portion, and these "mask" the corresponding parts of the IP address of the network, telling it
			  	that it does NOT have to go through the default gateway to communicate with the device on that subnet
			  	- 192.168.0 is the subnet in this case
			  	- see 3:39 in source vid for examples of communication


----
deno
----

- What is it? A secure runtime for javascript and typescript

***to be continued***




----
DHCP
----

# Additional notes not covered in detail in CompTIA notes

- DHCP is a service that runs on a server, such as a Microsoft server or a Linux server
	- also a service that runs on routers


- scope - the range of IP addresses a DHCP server can hand out [from a start IP address to an end IP address]
				- customizable


- lease - DHCP assigns the IP address to a computer for a certain amount of time
				- to make sure DHCP server does not run out of IP addresses in its scope
				- renewal - lease expires if a computer does not automatically renew its lease [e.g. if a computer is not on the network, in which case lease expires]


- reservation - ensures that a specific computer or device [identified by its MAC address] will always be given the same IP address
						  - can configure this in DHCP settings on a DHCP server
						  - typically given to special devices such as network printers, servers, routers, etc.




---
DNS
---

# Sources:
# Powercert Animated Videos - https://www.youtube.com/watch?v=mpQZVYPuDGU
# https://support.dnsimple.com/articles/differences-between-a-cname-alias-url/

- Domain Name Service - resolves names to numbers [domain names to IP addresses]


- The "A record" maps a name to one or more IP addresses when the IP are known and stable.
- The "CNAME" record maps a name to another name. It should only be used when there are no other records on that name.
- The "ALIAS" record maps a name to another name, but can coexist with other records on that name.
- The "URL record" redirects the name to the target name using the HTTP 301 status code.



---
DMZ
---

- Demilitarized Zone
	- a.k.a a perimeter network
	- used to improve the security of an organizations network by segregating devices such as computers and servers on opposite sides of a firewall

	- the servers are behind a companys firewall and are inside the companys private network
	 	- the company is letting in people from an untrusted network [the internet] and are given access behind the companys firewall

	- divides a network into two parts by taking devices from inside the firewall and putting them outside
		- a more secure network would use 2 [or more] firewalls for extra layers of protection

	- in the real world, a DMZ is an area where the military is forbidden
	- in the computing world, a DMZ is where firewall protection is forbidden


------
docker
------

# sharing namespaces: https://www.guidodiepen.nl/2017/04/accessing-container-contents-from-another-container/

# exposing docker port with bobrik/socat container:
docker run -d -v /var/run/docker.sock:/var/run/docker.sock -p 2376:2375 bobrik/socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock

# bobrik/socat container as a service:
docker service create --mode=global --name socat \
--publish 2376:2375 \
--mount "type=bind,source=/var/run/docker.sock,destination=/var/run/docker.sock" \
--entrypoint "socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock" \
bobrik/socat

# docker login troubleshooting
	- docker login not working (user interaction not allowed msg)
   		- remove credsStore from config.json
   		- if above doesn't work, uncheck 'Securely store Docker logins in macOS keychain'
   		- if above doesn't work, rm /usr/local/bin/docker-credential-osxkeychain

# checking container logs

docker run -it -v /var/lib/docker:/var/lib/docker <image_ID> bash - check /var/lib/docker/containers/<ID>/<container-id>-json.log

# persistent container, ensure auto restarts if machine goes down
docker update --restart=always <container_id>


# security scan tools

- docker bench security - ### needs updating from internal Confluence ###

- dockscan - https://github.com/kost/dockscan
	- needs ruby 2.0
	- installation via gem install



# sidecar pattern - https://www.magalix.com/blog/the-sidecar-pattern

# spin up Jenkins in docker
docker run \
    --name dcct-mobile-callisto-jenkins \
    --detach \
    --network jenkins \
    --env DOCKER_HOST=tcp://docker:2376 \
    --env DOCKER_CERT_PATH=/certs/client \
    --env DOCKER_TLS_VERIFY=1 \
    --publish 80:8080 \
    --publish 50000:50000 \
    --volume jenkins-data:/var/jenkins_home \
    --volume jenkins-docker-certs:/certs/client:ro \
    <ARTIFACTORY_URL>/local-docker-dcct/mobile/dev-jenkins-lts-2.263.1:1.0

# spin up cntlm on docker
docker run --restart always --name cntlm \
  -e "USERNAME=username" \
  -e "DOMAIN=mydomain" \
  -e "PASSNTLMV2=640937B847F8C6439D87155508FA8479" \
  -e "PROXY=123.123.123.123:8080" \
  -p 3128:3128 \
  robertdebock/docker-cntlm



# Multi-stage docker image build - Building an image with multi-stage (cut down the size) - example Dockerfile  (2021/04/08)

#### First stage

FROM ubuntu:18.04 as builder					 # notice the 'as builder' in the first stage
RUN apt-get update
RUN apt-get install -y make yasm nasm as31 binutils
COPY . .
RUN make release


#### Second stage

FROM scratch            					     # can use alpine here if scratch is too empty
COPY --from=builder /asttpd /asmttpd             # this is where the executable produced from stage 1 gets copied over to the second stage
COPY /web_root/index.html /web_root/index.html

CMD ["/asmttpd", "/web_root", "8080"]



# Multi-stage docker image build - 2nd example of a python Dockerfile where you can build either a debugger or regular image, depending on the scenario
# Source: DevOps Directive - https://www.youtube.com/watch?v=qCCj7qy72Bg

###############
# base
###############

FROM python:3.8.4-slim AS base

RUN pip install pytz

WORKDIR /src
COPY . ./


######################
# debugger
######################

FROM base AS debugger

RUN pip install debuggy

ENTRYPOINT ["python","-m", "debugger", "--listen", "0.0.0.0:5678", "--wait-for-client", "-m"]


######################
# primary
######################

FROM base as primary

ENTRYPOINT [ "python", "-m"]


# END DOCKERFILE

- to build the debugger:
	docker build -t debugger:<version> --target=debugger .

- to run debugger:
	docker run -p 5678:5678 debugger:<version> unittest         # where unittest is the py module used in the video example

- to build the primary image:
  docker build -t primary:<version> --target=primary .




--------------
docker compose
--------------

# Source: Marcel Dempers (That DevOps Guy) - https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/messaging/kafka/docker-compose.yaml

- side notes
	- docker compose is a separate installer package on Linux, but is included in Docker Desktop for Mac and Windows and does not need to be installed separately in this case.
	- docker compose allows you to describe the building and runninig of multiple container images all in one file [in YAML format]


# Example of docker-compose.yaml from Marcel's Kafka with docker compose video
# Note: you would still need to create the network like in the docker example for building/running the containers individually

version: "3.8"
services:
  zookeeper-1:
    container_name: zookeeper-1
    image: aimvector/zookeeper:2.7.0
    build:
      context: ./zookeeper
    volumes:
    - ./config/zookeeper-1/zookeeper.properties:/kafka/config/zookeeper.properties
    - ./data/zookeeper-1/:/tmp/zookeeper/
    networks:
    - kafka
  kafka-1:
    container_name: kafka-1
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-1/server.properties:/kafka/config/server.properties
    - ./data/kafka-1/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-2:
    container_name: kafka-2
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-2/server.properties:/kafka/config/server.properties
    - ./data/kafka-2/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-3:
    container_name: kafka-3
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    volumes:
    - ./config/kafka-3/server.properties:/kafka/config/server.properties
    - ./data/kafka-3/:/tmp/kafka-logs/
    networks:
    - kafka
  kafka-producer:
    container_name: kafka-producer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer:
    container_name: kafka-consumer
    image: aimvector/kafka:2.7.0
    build: 
      context: .
    working_dir: /kafka
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    networks:
    - kafka
  kafka-consumer-go:
    container_name: kafka-consumer-go
    image: aimvector/kafka-consumer-go:1.0.0
    build: 
      context: ./applications/consumer
    environment:
    - "KAFKA_PEERS=kafka-1:9092,kafka-2:9092,kafka-3:9092"
    - "KAFKA_TOPIC=Orders"
    - "KAFKA_VERSION=2.7.0"
    - "KAFKA_GROUP=orders"
    networks:
    - kafka
networks: 
  kafka:
    name: kafka


# How to Use the above file

docker network create kafka   # pre-requisite to running the containers
docker compose build       # will build all of the container images described in the file. Notice the different context or folder paths for each image shown in the build context section
docker compose up          # will run all of the container images that were built from this docker compose file



------------------------------
fail2ban - for blocking access
------------------------------

- Source article: https://www.digitalocean.com/community/tutorials/how-to-protect-ssh-with-fail2ban-on-centos-7

To install on CentOS [for Rocky Linux, you would use dnf, but need to test it]:

yum install epel-release
yum install fail2ban
systemctl enable fail2ban

- Edit /etc/fail2ban/jail.local (see jail.local.txt in this repo for details)

systemctl restart fail2ban
fail2ban-client status
fail2ban-client status sshd




-----
fdisk
-----

#### General ####

- Linux command: partition table manipulator
	- hard disks can be divided into one or more logical disks called partitions. This division is described in the partition table found in sector 0 of the disk.


#### Commands ####

fdisk -l 				 # lists all partitions
fdisk -l /dev/sda        # lists the partitions on the /dev/sda device

# Remove and add partition
fdisk /dev/sdb           # displays list of options for the device, type "m" and ENTER to see complete list, e.g. delete a partition, change a partition type, etc.
	d                    # Delete a partition, will ask you which one
	2                    # Delete partition #2 for example
	p                    # Add a primary partition
	[Enter]				 # Hit Enter to keep the default for the starting disk sector of the partition
	[number_then_enter]  # Put in an end sector number, example: 35000000 and hit enter
	Y or N               # for removing a particular signature on the sector, ext4 for example. In the boson labs I chose yes
	w                    # write the newly updated partition table and exit the fdisk utility prompt


# Example of fdisk menu

[root@server ~]# fdisk /dev/sda
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table
Building a new DOS disklabel with disk identifier 0x031547c5.

Command (m for help): m
Command action
   a   toggle a bootable flag
   b   edit bsd disklabel
   c   toggle the dos compatibility flag
   d   delete a partition
   g   create a new empty GPT partition table
   G   create an IRIX (SGI) partition table
   l   list known partition types
   m   print this menu
   n   add a new partition
   o   create a new empty DOS partition table
   p   print the partition table
   q   quit without saving changes
   s   create a new empty Sun disklabel
   t   change a partitions system id
   u   change display/entry units
   v   verify the partition table
   w   write table to disk and exit
   x   extra functionality (experts only)

Command (m for help):



--------------
File transfers
--------------

# Linux large tar transfer w/ untar at the same time, nohup the log into tmp:
nohup bash -c "cat somearchive.tar | ssh -t linuxuser@x.x.x.x 'tar -C /path/to/dest -xvf - /path/to/tarup'" > /tmp/filexfer.out &

# using netcat

tar -cvf - ~/var | nc -vv -l 127.0.0.1 -p 1234    # better to use the localhost IP for security

# then on client
cd /tmp
nc 127.0.0.1 1234 | tar -xvf -


-----------
Filesystems
-----------

# General

- file systems divide the storage space on a drive into virtual compartments known as "clusters"
- maintain an index of where individual files are located

# Linux file system directory assignment meanings

# Sources:
# Engineer Man: https://www.youtube.com/watch?v=UFIoRLqhFpo
# Fireship: https://www.youtube.com/watch?v=42iQKuQodW4

/bin - stores common/essential executables available for everyone, egs. cp rm ls

/boot - kernel and boot configuration needed to boot the system
	  - you may see the boot mount point by running "mount" command, e.g. /dev/sda1

/dev - device files: files which point to both physical and pseudo devices

/etc - editable text config: system and program configuration files

/home - non-root user home directories for user data

/lib
/lib32
/lib64 - library files used by the system, includes .so files and others

/lost+found - saved files due to failure

/media - auto-mounting place for certain external devices on some distros

/mnt - place to mount various file systems

/opt - optional/add-on software: various software

/proc - virtual filesystem for resources, processes and more

/root - root user home directory

/sbin - essential executables for super user (root)

/tmp - temporary files not persisted between system reboots

/usr/bin - non-essential installed binaries

/usr/local/bin - locally compiled binaries

/var - this directory contains files which may change in size, such as spool and log files.

/var/lib - Variable state information for programs.



# Types

auto - this is a special one. It will try to guess the fs type when you use this.

exFAT - Extended file allocation table
	- file system optimized for high-capacity USB flash drives and memory cards
	- maximum file size = 16EB
	- default FS for SDXC mem cards
	- broader non-Windows OS support than NTFS [including read/write on MacOS]


ext - Extended File System
	- launched in 1992 for Linux

ext2 - launched in 1993
	- default FS in many Linux systems for years

ext3
	- launched in 2001
	- this is the most common Linux fs type from a couple years back
	- introduced journaling to protect against corruption in the event of crashes or power failures

ext4
	- launched 2008
	- this is probably the most common Linux fs type of the last few years
	- maximum file size of 16TB, max volume size of 1EB
	- no native Windows or MacOS support


FAT - File Allocation Table
	- major variants: FAT12, FAT16, FAT32
		- each has increasing number of clusters, maximum file and volume sizes
		- FAT32 still widely used for removeable media and popular due to wide OS compatibility

		- max file size
			- FAT12 - 32MB [8KB clusters] or 16MB w/ 4KB clusters
			- FAT16 - 2GB/4GB
			- FAT32 - 4GB

		- max volume size
			- FAT12 - 32MB [8KB clusters]
			- FAT16 - 16GB [256KB clusters]
			- FAT32 - 32GB [Windows format]
						  - 2TB [other OS]
						  - 16TB [theoretical]


glusterfs - for Gluster


HFS - Hierarchical File System [for MacOS]
	- aka MacOS Standard
	- was introduced w/ journaling as HSF+ [HSF Extended]
	- files and volumes up to 8EB [as of MacOS 10.4]
	- 2017 - APFS launched [Apple file system]
	- no native Windows or Linux support



NTFS - New Technology File System

	- this is the most common Windows fs type or larger external hard drives
	- file size limit of 16 exabytes
	- journaling file system [maintains a record of changes in case of failures]
	- supports file permissions and encryption
	- all Windows must be installed on NTFS
		- downside: limited non-Windows OS compatibility - eg. read-only in MacOS and older Linux distros


vfat - this is the most common fs type used for smaller external hard drives


ZFS - zed file system
- created by Sun Microsystems, now developed by OpenZFS project
- an advanced fs fype that pools disk storage
- integrated volume manager to control storage hardware
	- provides increased data protection
- available for Linux, FreeBSD and TrueOS



# General commands

- in Linux,
	- "blkid" shows attributes of block devices (such as filesystem type)
	- "lsblk" shows partition information, including the type of FS on each partition
	- "mkfs" is for creating a file system on a partition
	- "mkswap" is used to create swap space on a disk/partition


# Creating swap space on a partition, followed by creation of an FS
umount /dev/sdb2               # unmounts a partition [in this case, /dev/sdb2, to prepare for creating swap space on it]
mkswap /dev/sdb2               # creates swap space on a partition. NOTE: this will wipe the file system signature as well
mkfs -t <fs_type> /dev/sdb2    # creates a file system, where fs_type can be ext3 for example, on the /dev/sdb2 partition
lsblk -f                       # verify the partition type


#### Command options ####

# lsblk
lsblk -a                      # include empty devices in the list
lsblk -d                      # show only the main devices, e.g. /dev/sdb, but don't show /dev/sdb1 and sdb2
lsblk -m                      # show ownerm group and mode of each device
lsblk -o NAME,SIZE            # show only name and partition size of each device

# blkid
blkid -i /dev/sdb1            # display I/O limits on /dev/sdb1
blkid -p /dev/sdb1            # show additional information about the /dev/sdb1 block device
blkid -o list                 
   OR
cat /proc/mounts 			  # show filesystem type and mount points of each block device


# df
df -h                         # show general info about mounted filesystems (e.g. available and used space)


# fsck - file system checker for errors, can use to repair as well

umount /dev/sdb1 			  # unmount sdb1 fs to run next command
fsck /dev/sdb1                # check for errors on the /dev/sdb1 fs (note: can be used on any unmounted fs)
fsck -a /dev/sdb1             # check and auto-repair if errors. can use -y option as well if available on OS
fsck -M /dev/sdb1             # check mounted file system - not recommended
ls /sbin/fsck*                # check for fs specific command options to run with fsck
fsck -t [fs_type] [fs] 		  # check specific fs type
fsck /dev/sdb1 -f             # force fs check and generate report


# tune2fs - allows the system administrator to adjust various tunable filesystem parameters on Linux ext2, ext3, or ext4 filesystems.

tune2fs -l /dev/sdb1
tune2fs -l /dev/sdb1 | grep -i mount     # check when FS was mounted, options used, max # the fs can be mounted, etc.
tune2fs -c 20 /dev/sdb1                  # change max number of times fs can be mounted
tune2fs -l /dev/sdb1 | grep -i name      # see volume label
tune2fs -l /dev/sdb1 | grep -i check     # when was fs was checked last
tune2fs -i 5d /dev/sdb1                  # check fs every 5 days


# e2label - set a label for a volume

e2label /dev/sdb1         # check for label of a volume or partition
e2label /dev/sdb1 Data    # set name "Data" for sdb1 device




----
find
----

- in Linux, used to find [recursively] files and directories that match a specific pattern or attribute

# find jax2b cached artifacts in maven local repo
find . | grep jaxb2-plugin | grep 0.8.1 | grep jar


# using find with grep

find . -type f -exec grep -iH "rbac" {} \;      # searches for the string "rbac" in current directory and all subdirectories.
												# using -type flag so the search doesn't print unnecessary "this is a directory" output
										        # It will also print name of the file matching the string, remove -H flag if this is not necessary

# using find to search for all csv files in current folder (without going into subfolders) and removing them if they are older than 1 day (1440 minutes = 1 day)
find . -maxdepth 1 -type f -mmin 1440 -name "*.csv.*" -exec rm -f {} \;



---------------------------
firewalld (on CentOS/RHEL7)
---------------------------

# configuration file location
/etc/firewalld/firewalld.conf


# check firewalld status
systemctl status firewalld
	# or
firewall-cmd --state


# check default configuration
firewall-cmd --list-all


# check zones
firewall-cmd --get-zones   		# output: block dmz drop external home internal public trusted work

# or get default zone
firewall-cmd --get-default-zone


# check firewalld services, see all the apps which firewalld can be configured for
firewall-cmd --get-services


# allow a specific port
firewall-cmd --add-port=3306/tcp


# change default zone to dmz and open up incoming connections (this is supposed to make it internally accessible only, but you can also limit the source address entries instead of putting 0.0.0.0 as shown below)
firewall-cmd --set-default-zone=dmz
firewall-cmd --zone=dmz --add-rich-rule='rule family="ipv4" source address="0.0.0.0/0" accept'


# restart firewalld service
systemctl restart firewalld
   # or
firewall-cmd --reload


# add/remove a specific service(s)
firewall-cmd --add-service=mysql --permanent    # the permanent flag will make this setting survive a reload or restart
  # or
firewall-cmd --add-service={mysql,http,https,ldap} --permanent
  # or remove
firewall-cmd --remove-service={mysql,http,https,ldap} --permanent
firewall-cmd --reload


# port forwarding
firewall-cmd --add-forward-port=port=8080:proto=tcp:toport=80   # can add toaddr= if you want to redirect to another host


# add traffic "rich rule"
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.102" accept' # will accept all traffic from this IP 
firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.122.103" drop'   # will not accept traffic from this IP


# set all temporary or runtime firewalld configurations permanently ***
firewall-cmd --runtime-to-permanent



# Zones list

drop: The lowest level of trust. All incoming connections are dropped without reply and only outgoing connections are possible.

block: Similar to the above, but instead of simply dropping connections, incoming requests are rejected with an icmp-host-prohibited or icmp6-adm-prohibited message.

public: Represents public, untrusted networks. You don’t trust other computers but may allow selected incoming connections on a case-by-case basis.

external: External networks in the event that you are using the firewall as your gateway. It is configured for NAT masquerading so that your internal network remains private but reachable.

internal: The other side of the external zone, used for the internal portion of a gateway. The computers are fairly trustworthy and some additional services are available.

dmz: Used for computers located in a DMZ (isolated computers that will not have access to the rest of your network). Only certain incoming connections are allowed.

work: Used for work machines. Trust most of the computers in the network. A few more services might be allowed.

home: A home environment. It generally implies that you trust most of the other computers and that a few more services will be accepted.

trusted: Trust all of the machines in the network. The most open of the available options and should be used sparingly.



------------
font flipper
------------

Site: https://fontflipper.com

- a website that helps you choose/experiment with different fonts for your designs with a Tinder-style UI
- can download the font you like from Google fonts


-------
Fortify
-------

# upload command:
./fortifyclient -url https://fortify_url.com/ssc -authtoken xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx uploadFPR -file ../something.fpr -project "project_name" -version "latest"



---------------
getfacl/setfacl
---------------

# Sources: https://www.computerhope.com/unix/usetfacl.htm

- Linux/UNIX - getfacl - displays the file name, owner, the group, and the ACL (Access Control List)
			 - setfacl - modifies the access control list of a file or files.


#### Commands ####

getfacl /some/path          								# get access control list for target directory
getfacl /path/to/file 										# get access control list for target file
setfacl -Rm g:logfsg:r-x d:g:logfsg:r-x /some/path 			# set default ACL for specific directory (recursively) to a particular group (logfsg)



---
GIT
---


### Commands and configs ###

# Sources
# Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o
# git-scm - https://git-scm.com
# General experience :D


# General usage - once you install git onto your Linux, Mac or Windows machine, below is the typical process when working with the git client to
# update and check-in files to a source code repository hosted on Github, Bitbucket or any other remote Git-based source control application

# 1) Getting Started
mkdir git_workspace             # Create a workspace on your local or server machine
git clone [git_repo_url]        # Clones a entire copy of the target remote repository onto your local
cd [repp_name]                  # Go into locally cloned copy of the repo
git remote -v 					# Shows remote repo URL (this will sohw the fetch and pull remote repo URL, which sometimes can be different)


# 2) Everyday commands

git pull                        # Syncs your local repository with the remote repository. Note: ALWAYS run git pull before you start making updates or are about to commit/push updates
git status -s                   # check to see which files have been added/modified/deleted
git branch                      # Shows the current working branch + lists previous working branches

git checkout [branch_name]      # Switches your working branch to a different one
OR
git checkout -b [branch_name]   # Create new branch based on current working branch and switches to it

git diff                        # See differences made between latest version committed and local updates (all files)
OR
git diff [filename]             # See differences made between latest version committed and local updates (single file or folder)

git add [filename(s)]           # prepare filename(s) to be committed to repo
OR
git add .                       # prepare all files and folders in parent and subfolders to be committed

git commit -m "Your msg" 		# commit files into repo with your commit message on what was updated
git push -u origin [branch_name]    # push files into remote to the target branch (which matches your working branch locally).


# 3) Other useful commnads

git log                        # see the repo's commit history
git log [filemame/foldername]  # see a target file or folder's commit history

git reset                      # reverts the "git add" command. Will un-prep the files originally prepped to be committed. Useful if you need to make more changes or do not want
							   # to add specific files for committing.

git branch -m [current_branch_name] [new_branch_name]        # Rename a branch
git merge [branch_to_merge_files_from]                       # Merge the target branch into your working branch

git stash save [stash_name]   								 # Saves all files currently in added/modified/deleted state, which are not yet prepped for committing, into a stash,
															 # which can later be restored
git stash pop [stash_name]									 # Restores all files from the target stash

git config --global user.name "John Doe"                     # Globally configure the username you will use for updates to any GIT repo
git config --global user.email johndoe@example.com           # Globally configure the email address you will use for updates to any GIT repo



# .gitattributes - use .gitattributes within a repo to define file behaviours/attributes. Eg below of contents within a .gitattributes file:

*           text=auto
*.txt		text
*.vcproj	text eol=crlf
*.sh		text eol=lf
*.jpg		-text


# Alias - creating command alias and storing within git configuration

git config --global alias.ac "commit -am"    # where ac is the new alias command
git ac "Your commit message"								 # usage for above alias



# Bisect - do a binary search through commits to figure out where a bug was introduced
# Source: git-scm - https://git-scm.com/docs/git-bisect

1] As an example, suppose you are trying to find the commit that broke a feature that was known to work in version v2.6.13-rc2 of your project. You start a bisect session as follows:

git bisect start
git bisect bad                 # Current version is bad
git bisect good v2.6.13-rc2    # v2.6.13-rc2 is known to be good


2] Once you have specified at least one bad and one good commit, git bisect selects a commit in the middle of that range of history, checks it out, and outputs something similar to the following:

# Output
      Bisecting: 675 revisions left to test after this (roughly 10 steps)
  

3] You should now compile the checked-out version and test it. If that version works correctly, type:

git bisect good


4] If that version is broken, type

git bisect bad

- then git bisect will respond with something like

# Output
	Bisecting: 337 revisions left to test after this (roughly 9 steps)


5] Keep repeating the process: compile the tree, test it, and depending on whether it is good or bad run git bisect good or git bisect bad to ask for the next commit that needs testing. Eventually
there will be no more revisions left to inspect, and the command will print out a description of the first bad commit. The reference refs/bisect/bad will be left pointing at that commit.



# Clean - cleans up hanging or loose file references

git clean -df


# Combine add and commit in one command (add applies to working directory like using "git add .")

git commit -am "Your commit message"

	# the long equivalent of the above is:
	
	git add [files]
	# or
	git add .                               # for prepping all files and folders recursively to be committed to the branch/repo

	git commit -m "Your message here"       # this will commit all the "staged" or prepped files to your repo

    
    # and then to push to your remote site

    git push -u origin [branch_name]        # where -u = upstream, origin = the default short name for the remote location




# Commit message update (of an existing commit)

git commit --amend -m "Your new commit message"



# Commit additional files to your last commit (without updating the commit message) - only works locally, if you haven't pushed your commit to the remote

git commit --amend --no-edit



# Credentials storing - set up storing creds in OSX
git config --global credential.helper store



# Hooks - tell GIT to do stuff upon certain CLI actions

- in local repo, look under .git/hooks folder, you can create shell scripts for different things like pre-commit actions, post-commit actions, etc.
- husky - an npm package that makes implementing GIT hooks easier [go to Github repo and "npm install husky -D" locally to find out more]


# Log - an advanced command for an easier look at git commit history locally

git log --graph --oneline --decorate


# Migrate repos from one remote to another
git clone --bare <repo_url>
cd <cloned_repo>.git
git push --mirror  https://github.com/<new-repo>


# Merge - locally merge the contents of some branch into your own branch

git checkout <target_branch>
git pull
git checkout <your_branch>
git pull
git merge <target_branch>



# Squash - take two or more commits in a branch and "squash" them together into a single commit
# See medium.com for more details - https://medium.com/@slamflipstrom/a-beginners-guide-to-squashing-commits-with-git-rebase-8185cf6e62ec

git rebase <branch_name> --interactive

	# the above opens a file that shows all of the commits for that branch
	# then you would "pick" the commit you want to use, and "squash" the commits you want to go into the picked commit.


# Squash automatically or auto-squash - tell GIT in advance that you will be squashing commits

git commit --fixup fb2f677       # fixup is like squash except it ignores the commit message when squashing occurs
git commit --squash fc2f55       # preps commit for squashing upon commit
git rebase -i --autosquash       



# Stash - used to set local changes aside for later

git stash        
git pop          # you can use stash and pop if you only plan to use stash for one set of changes to put away


# Stash - commands below are for saving multiple stashes

git stash save <name_of_stash>
git stash list                                  # list all stashes
git stash apply <index_of_stash_name>           # pop the selected stash from the index list


# Status - check your local repo to see added, deleted or modified files

git status -s       # show added/deleted/modified/renamed files in short list.
					# green status - ready to be commited
					# red status - unstaged. Running 'git commit' successfully will not commit a file with this status

	# types of status
		' ' = unmodified
		M = modified
		A = added
		D = deleted
		R = renamed
		C = copied
		U = updated but unmerged
		DD = two files together which are unmerged, and both deleted
		UU = two files unmerged and both modified. This usually happens when you attempt to merge two branches and the same file from both branches have conflicting lines between them


# yum install git - install newer version on CentOS7
yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm
yum install git



------
Github
------

# Commands

- sudo reboot - reboot Linux server [which will ultimately reboot the Github service - note needs updating]
- sudo systemctl start elasticsearch - restart search service
- sudo systemctl status elasticsearch - check search services status


# Cool Stuff
# Source: Fireship - https://www.youtube.com/watch?v=ecK3EnyGD8o

- to use VSCode in a browser to work on a specific repo in Github, go to the repo you want to work on, and hit the "." key
	- you can use a terminal if you set up Github codespace, option can be found within the VSCode browser editor




---
GPT
---

#### General ####

- GPT - GUID Partition Table
	- the latest standard for defining the partitions on a hard disk
	- uses globally unique identifiers (GUID) to define the partition, and you can create theoretically unlimited partitions on the hard disk. 

- gdisk - note there are a lot more options for partition types with the gdisk utility than with fdisk [including creating partitions other than just Linux file system]


#### Commands ####

sudo gdisk /dev/sdb            # displays partition table scan results of /dev/sdb via gdisk utility
	l                          # type 'l' to see full list of partition types


# Example of gdisk menu

[root@li1961-156 ~]# gdisk /dev/sdb
GPT fdisk (gdisk) version 0.8.10

Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries.

Command (? for help): ?
b       back up GPT data to a file
c       change a partitions name
d       delete a partition
i       show detailed information on a partition
l       list known partition types
n       add a new partition
o       create a new empty GUID partition table (GPT)
p       print the partition table
q       quit without saving changes
r       recovery and transformation options (experts only)
s       sort partitions
t       change a partitions type code
v       verify disk
w       write table to disk and exit
x       extra functionality (experts only)
?       print this menu



----
grep
----

- Linux/UNIX - search for and print lines matching a pattern


#### Command examples ####

# grep

grep yum /etc/yum.conf               # Search for the word  "yum" in yum.conf
grep -n "old" /etc/yum.conf          # Search for the word "old" in yum.conf. Print the line number where the word is found.
grep -c "old" /etc/yum.conf          # Print the number of occurrences only of "old". Good (maybe better) alternative to wc -l
grep -o "y" /etc/yum.conf | wc -w    # Count the number of words that start with "y" in yum.conf
grep -v "yum" /etc/yum.conf          # NON-matching: find/print all lines that do not have the word "yum"
grep -n "^$" /etc/yum.conf           # List all blank lines with their line numbers


# egrep - extended grep

ls | egrep "yum" /etc/yum.conf       # egrep is needed when using pipe (|). egrep is deprecated, can also use grep -E
egrep -c '^1|01$' /etc/yum.conf      # Count number of lines that start with 1 and end in 01 in yum.conf
egrep 'Fedora|yum' /etc/yum.conf     # Search for both Fedora and yum simultaneously in yum.conf


# fgrep - fixed grep, does not interpret regex special characters

fgrep -c 'yum' /etc/yum.conf            # Will return same result as grep or egrep
fgrep -c 'yum|Fedora' /etc/yum.conf     # This will not return a result because it reads | literally


# multiple grep
grep "some_text" file.txt | grep "some_more_text" | egrep "this|that|theother"


### Misc ###

- GREP_COLORS - "https://linuxaria.com/pills/coloring-grep-to-easier-research"




------
groovy
------

# General
# Source: Derek Banas - https://www.youtube.com/watch?v=B98jc8hdu9g


# Example class

class GroovyTut {
	static void main(String[] args) {

		println("Hello World")

		def age = "Dog";
		age = 40;
		def name = "Derek";
		def multString = '''I am
		a String that goes on
		for many lines''';

		println("5 + 4 = " + (5 + 4));
		println("5 - 4 = " + (5 - 4));
		println("5 / 4 = " + (5.intdiv(4)));

		println("5.2 + 4.4 = " + (5.2.plus(4.4)));
		println("(3 + 2) * 5 = " + ((3 + 2) * 5));

		println("age++ = " + (age++));              				# prints 40, then increments
		println("++age = " + (++age));              				# prints 42 - increment the value of age then print the value [age being 41 at this line]

		println("Biggest Int " + Integer.MAX_VALUE);        # Biggest Int 2147483647
		println("Smallest Int " + Integer.MIN_VALUE);				# Smallest Int -2147483647

		println("Biggest Float " + Float.MAX_VALUE);        # Biggest Float 3.4028235E38

		println ('I am ${name}\n');													# Single quotes: string is taken literally, newline is not ignored. Output: I am name
		println ("I am ${name}\n");													# Double quotes: prints content of variable, newline is not ignored. Output: I am Derek
		println (multString);																# Triple quotes: allows to print on multiple lines

		println("3rd index of name " + name[3]);						# name[3] would resolve to 'r'
		println("Index of r " + name.indexOf('r'))					# name.indexOf('r') would resolve to '2'

		println("Derek == Derek " + ('Derek'.equalsIgnoreCase(Derek)));

	}

}


# Analysis of example 

- static - method or function that belongs to the class
- void - doesnt return anything when function runs
- main - main executable function that runs everytime you call the file this code is in
- String[] args - a string array called 'args' - pass in a bunch of numbers or strings, all of them would be stored in this array
- def - for declaring variables
- println - print whatever is in parenthesis to the screen/console

- arithmetic operations - see examples above [and more in video]
	- (Math.xxx(y)) - use Math class xxx function on a number y
		- eg. Math.abs() - absolute value of a number

- var.substring(x) - get substring of the contents of var starting with the x index
- var.split('x') - split everything in var with the occurrence of x [separated by commas]
- var.toList() - split var into a list of every character [separated by commas]




# Notes

- everything in groovy is an object


# Command-line

groovy groovytut.groovy       # execute groovytut.groovy script





----
helm
----

Definition: a package manager for kubernetes, defines package specifications in the form of custom YAMLs called 'charts'

Directory structure:

folder/
	Chart.yaml     			# contains info about the chart
	LICENSE 			 			# OPTIONAL: license for the chart
	README.md 		 			# OPTIONAL: human readable readme
	values.yaml    			# Default config values for the chart
	values.schema.json  # OPTIONAL: impose a structure on the values file with a JSON schema
	charts/           	# A directory containing any charts upon which this chart depends
	crds/								# Custom Resource Definitions
	templates/					# A dir of templates when combined with values generate valid K8s manifest files
	templates/NOTES.txt # OPTIONAL: plan text file containing short usage notes  




--------
hostname
--------

- Linux/UNIX - identifies the name/domain name of a system on a network



#### Commands ###

hostname            # displays hostname
hostname -s         # displays short version of hostname
hostname -f         # displays fully qualified domain name (FQDN)

cat /etc/hostname   # you can see the hostname configured here

hostnamectl         # view detailed info about the host/hostname

hostnamectl set-hostname plablinux2 			# change the hostname to plablinux2. Requires reboot of system




------
httpie
------

# Sources
- https://pypi.org/project/httpie/ - homepage, lots of examples

# on Ubuntu, the httpie CLI utility for interacting with websites via CLI
apt-get install python3-pip
pip3 install --upgrade httpie
http <website_URL>


### Examples ###

# Custom HTTP method, HTTP headers and JSON data:

http PUT pie.dev/put X-API-Token:123 name=John


# Submitting forms:

http -f POST pie.dev/post hello=World


# See the request that is being sent using one of the output options:

http -v pie.dev/get


# Build and print a request without sending it using offline mode:

http --offline pie.dev/post hello=offline


# Use GitHub API to post a comment on an issue with authentication:

http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body='HTTPie is awesome! :heart:'


# Upload a file using redirected input:

http pie.dev/post < files/data.json


# Download a file and save it via redirected output:

http pie.dev/image/png > image.png


# Download a file wget style:

http --download pie.dev/image/png




-------
hwclock
-------

- Linux/UNIX - displays/modifies the current time that the hardware clock on the computer shows


#### Commands ####

hwclock             							# display current time on hardware clock (same output with -r or --show flag)
hwclock -w 										# set the hwclock to the current system time
hwclock --set --date 12/19/2018  				# set the hwclock date manually
hwclock --set --date "12/20/2018 23:10:00"		# set the hwclock date and time manually
hwclock -s 										# copy hardware time to system time




------------
if/else/case
------------

#### shell scripting ####

# case statements - if some value, do this; if another value, do this instead

#!/bin/sh
CAR="Honda"
case "$CAR" in
   "Ferrari") echo "Ferrari is quite expansive."
   ;;
   "Jaguar") echo "I like Jaguar."
   ;;
   "Honda") echo "Honda is the car of the year."
   ;;
esac



# if statements (using elif and else)
# if - if a certain statement is true, do something
# elif - or if another statement is true, do somethin else
# else - otherwise if nothing else is true, do this

#!/bin/sh
x=10
z=20
if [ $x == $z ]
then
   echo "x is same as z"
elif [ $x -gt $z ]
then
   echo "x is more than z"
elif [ $x -lt $z ]
then
   echo "x is less than z"
else
   echo "None of them match"
fi


-------
ingress
-------

# Responsible for
	- accept/deny incoming requests
	- SSL termination
	- routing
	- load balancing


------
init.d
------

- /etc/init.d - contains a number of start/stop scripts for various services on your system.

- usage:
	/etc/init.d/[command] OPTION

	- where OPTION can be any of the following:
		- start
		- stop
		- reload
		- restart
		- force-reload


### related ####

cat /etc/inittab           # view runlevels of different programs. Is no longer used by systemd, which uses targets instead of runlevels



--------
ipconfig
--------

Source: PowerCert Animated Videos - https://www.youtube.com/watch?v=ZKhorleA5aA

- ipconfig command line tool displays TCP/IP network configuration of the network adapters on a Windows computer


# command variations/switches

ipconfig /all                # displays full TCP/IP configuration of your network adapters (eg. additional info such as hostname, DHCP enabled, physical address, DHCP server address, etc.)
ipconfig /flushdns           # flushes the DNS resolver cache on the computer (side note: computers only understand numbers, not names, hence DNS translation)
	- DNS resolver cache contains a list of DNS names matched up with IPs that youve visited, so its faster on subsequent visits to those websites
		- this means less load on DNS server for lookups
	- reasons to flush DNS
		- if a website changes its IP and youre not able to resolve
		- if your cache has been hacked, redirecting you to malicious websites
			- helps hide website search behaviour

ipconfig /displaydns         # displays the contents of the DNS resolver cache
	- normally shows a bunch of records
	- Time To Live - amount of time before entry is erased from the cached





--------
IPTables
--------

# To block 116.10.191.* addresses:
sudo iptables -A INPUT -s 116.10.191.0/24 -j DROP

# To block 116.10.*.* addresses:
sudo iptables -A INPUT -s 116.10.0.0/16 -j DROP

# To block 116.*.*.* addresses:
sudo iptables -A INPUT -s 116.0.0.0/8 -j DROP

# Open a port (RHEL6)
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 5667 -j ACCEPT
service iptables save



-----
istio
-----

Video source: Istio Service Mesh Explained - https://www.youtube.com/watch?v=KUHzxTCe5Uc

- istiod - control plane of the Istio service mesh, responsible for injecting sidecar proxies into pods (which happen when apps "opt-in" to the service mesh)
 - components
    - pilot - traffic management, injecting/managing lifecycle of sidecar proxies
    - citadel - certificate authority, helps achieve mutual TLS between services within the mesh
    - galley - translates kubernetes YAML into format for Istio to process


# install Istio

curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.12 TARGET_ARCH=x86_64 bash -
mv istio-1.6.12/bin/istioctl /usr/local/bin
chmod +x /usr/local/bin/istioctl
mv istio-1.6.12 /tmp/


# Pre-flight check for compatibility with a cluster (k8s api, version, if istio already installed, can setup within cluster, auto sidecar injector)

istioctl x precheck


# Istio profile check and install with default profile

istioctl profile list
istioctl install --set profile=default
kubectl -n istio-system get pods
istioctl proxy-status


- two ways to opt-in
 	- label a namespace (all pods within the namespace will join the service mesh)
		kubectl label namespace/default istio-injection=enabled    # takes effect for new pods, you'll see an additional container within each pod
 
	- use istioctl to grab the deployment YAML and inject the sidecar proxy using istioctl
		kubectl -n ingress-nginx get deploy nginx-ingress-controller -o yaml | istioctl kube-inject -f - | kubectl apply -f -    # takes effect for new pods
  

- add-ons
	- comes pre-shipped with a Grafana and Kiali dashboard add-ons for rich metrics/telemetry
	- to install Grafana (to the Istio namespace):
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/prometheus.yaml
		kubectl apply -f /tmp/istio-1.6.12/samples/addons/grafana.yaml
		kubectl get pods --namespace istio-system

	- to expose Grafana dashboard
		kubectl -b istio-system port-forward svc/grafana 3000

    - see something wrong in Grafana? Command below to check logs within the cluster of pods throwing errors:
    	kubectl logs <pod_name> -c <container_name> --tail 50

    - need to buy dev some time to fix something? Implement virtual service in Istio (i.e. automated retries, canary deploys, traffic splitting)
    	- see https://www.youtube.com/watch?v=KUHzxTCe5Uc @ 26:10




-------
Jenkins
-------

# when login is broken
	- Stop Jenkins (the easiest way to do this is to kill the servlet container. Best way is to run a stop script, either a custom shell or stopping the service if its running that way)
	- Go to $JENKINS_HOME in the file system and find config.xml file.
	- Open this file in the editor.
	- Look for the false element in this file.
	- Replace true with false
	- Remove the elements authorizationStrategy and securityRealm
	- Start Jenkins


# Decrypt secret from Credentials

- In your Jenkins instance
	- go to Credentials -> Your_secret -> Global credentials (unrestricted) -> Update (Your_secret)
	- right click inside "Secret" field -> Inspect
	- copy the value inside the _.secret input name entry (e.g. should be like this: {AQAAABAAAAAwJABqfeWZl6KviKrG/uq1JcSwL1ru1/82OSGQkS3JreDRp54r7fBoAwEiO/+D8uCASaD1UTH0GAyet8z+YZIzZQ==} )
    - open a new tab and go to: <your Jenkins instance URL>/script
	- Paste the following script with your secret value into the console and click Run:
		- println hudson.util.Secret.decrypt("{AQAAABAAAAAwJABqfeWZl6KviKrG/uq1JcSwL1ru1/82OSGQkS3JreDRp54r7fBoAwEiO/+D8uCASaD1UTH0GAyet8z+YZIzZQ==}")
	- you will see the decrypted value printed if it ran successfully



-----
kafka
-----

# Introduction
# Source: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=heR3I3Wxgro
#																					 - https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/messaging/kafka  [source code + README with instructions]

- Apache Kafka - an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration,
								 and mission-critical applications.

- zookeeper - centralized service for maintaining configuration information [naming and providing distributed synchronization for apache services such as kafka]
						- keeps track of status of kafka cluster nodes
						- also keeps track of partitions and topics


- logistics
	- client [producer] ----> kafka broker	----> application server[s]

		- clients are referred to as "producers"
		- messages go to "brokers" which are kafka instances

		- servers that consume messages are called "consumers"
			- consumer can subscribe to a topic and receive messages, in order, indexing what it has received so far
				- if a consumer dies/shuts down, once its back online it can use the index number to pick up consuming messages from the index it left off on

		- messages are stored on the brokers in "topics"
			- topics can be divided into partitions, messages/copies of messages go into the partitions
				- if a broker dies, messages are not lost as a result of this replication

		- in source vid and repo, the example for testing will make use of pre-packed scripts that come with the Kafka installation to mock producers and consumers for testing purposes


 - testing

	- the Github repo above has a Dockerfile to build a Debian/Java11 container with Kafka installed [see dockerfile under messaging/kafka folder]
 		- can spin up multiple instances of this image for having more than one broker to replicate topics/partitions/messages
 			- minor configuration updates requires for server.properties, such as broker ID, which has to be unique for each container
 		- for zookeeper, its the same dockerfile except you use the start-zookeeper.sh script as the entrypoint. For testing, one zookeeper container is sufficient
 			- see zookeeper dockerfile and start script under messaging/kafka/zookeeper




----------
kubernetes
----------


# General

- control nodes and control plane - the administrative node[s] of a cluster which instruct and manage the worker nodes which run workloads

	- kube-api server - main component which communicates instructions to/from all cluster components, and validates/configures data for api objects such as pods, services, RCs, etc.
	- etcd - key/value store for a cluster
	- scheduler - responsible for telling api server where a pod needs to be scheduled and when
	- kubelet - the agent which sits on the worker node and communicates with the api server
	- kube-proxy - intercommunication mechanism in a cluster between nodes
	- controller-manager - responsible for telling the api server to match the desired state with the actual state of pods within a cluster
	- daemonset - ensures that all or some nodes run a copy of a pod [e.g. kube-proxy daemonset would ensure kube-proxy pod runs on every available node in the cluster]



# Installing a Kubernetes cluster using kubeadm [Centos7]
# Source: Just Me And Opensource - https://www.youtube.com/watch?v=v6Jh2sZClDY
#								 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-centos-7.md
# 								 - https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-ubuntu-20.md [for install on Ubuntu 20.04]         


For Centos 7 install:

On both master and worker,

1. Disable firewalld

systemctl disable firewalld; systemctl stop firewalld


2. Disable swap

swapoff -a; sed -i '/swap/d' /etc/fstab


3. Disable SELinux

setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux


4. Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


5. Install docker  #### note will have to check how this works for containerd since docker is deprecated in K8s as of v1.21

yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-19.03.12 
systemctl enable --now docker


6. Add yum repo for Kubernetes install

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


7. Install Kubernetes

yum install -y kubeadm-1.18.5-0 kubelet-1.18.5-0 kubectl-1.18.5-0


8. Enable and start kubelet service

systemctl enable --now kubelet



On master node,

9. Initialize Kubernetes cluster

kubeadm init --apiserver-advertise-address=172.16.16.100 --pod-network-cidr=192.168.0.0/16


10. Deploy Calico network

kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml


11. Cluster join command [generating token for worker nodes]

kubeadm token create --print-join-command


Side note: if you want to be able to run kubectl commands as non-root user, then as a non-root user perform these:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


12. On the worker node[s], use the output from the "kubeadm token create" command and run it.


13. Verify cluster

kubectl get nodes
kubectl get cs         # get component status



####### kind - a software for running kubernetes clusters within docker containers ########
# Sources
- https://www.youtube.com/watch?v=m-IlbCgSzkc&list=WL&index=1

# Prerequisites
- docker
- go (1.11+)
- kubectl

# Install kind
- can download binary from Github, place in any PATH (e.g. /usr/local/bin/kind)

1] Create kind kubernetes cluster

kind create cluster --name <cluster_name>
	- this will pull the kind docker image to spin up for creating a single master node cluster

kind get clusters
	- show running clusters

kind delete cluster
	- to remove the created cluster

2] Check running nodes
kubectl get nodes

3] set kubeconfig to point to your new cluster (can run more than once to add clusters)
export KUBECONFIG="$(kind get kubeconfig-path --name="<cluster_name>"


# Multi-node cluster - its possible to create a kind cluster with worker nodes added

1] create a yaml with the following def
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker


2] create cluster with this yaml definition

kind create cluster --config /path/to/kind.yaml


3] verify

kind get clusters
docker ps 					                   # will show running kind containers, one which is your master/control plane as in the first example, and one or more as your worker nodes
kubectl cluster-info                   # show info about cluster, including the URL to hit it
kubectl get nodes -o wide  					   # see the nodes not as docker containers but as kind nodes

docker exec -ti <container_ID> bash    # log into any of the kind containers
which ctr 														 # container runtime binary similar to docker command line
ctr namespaces list  									 # can see namespaces within the cluster
ctr namespace k8s.io containers list   # can see all the containers that make up the cluster



########## k0S - a frictionless, easy-to-install-and-use kubernetes distro ##########

# General website - https://k0sproject.io/
# Getting started steps - https://docs.k0sproject.io/v1.22.2+k0s.0/install/
# Youtube getting started guide from Just Me and Opensource - https://www.youtube.com/watch?v=aHPGf6FsY7Y



########## kubectl ##########


# Random useful kubectl commands

kubectl expose deploy <pod> --port <port> --dry-run -o yaml > somefile.yaml    # Expose a deployment as a service, dry run to check yaml definition, and can output to a file
kubectl -n <namespace> port-forward <pod_name> <port> 			               # Port forward to expose an app as a service
kubectl scale deploy <deployment_name> --replicas=<number>                     # Manually scale a deployment
kubectl rollout restart sts/<sts_name>                                         # Rolling restart of stateful set pods of name sts_name
kubectl rollout restart deployments/<deploy_name>                              # Rolling restart of deployments of name deploy_name
kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod  # Delete all pods in namespace which are in Evicted state




########## kURL - a custom Kubernetes distro creator ############

# Website - https://kurl.sh/

# Demo for installation: Just Me and Opensource - https://www.youtube.com/watch?v=_4edisHDWzs

- the kURL tool helps you to create a Kubernetes cluster which can be customized off the bat with different tools that are more typical/standard that are not provided out of the box right away
	with a vanilla K8s installation such as monitoring tools [Prometheus], cluster backup and restore mechanism [Velero], certificate management [CertManager], and more. See kurl.sh for more details.






#### Webhooks ####
# Source - Marcel Dempers (That DevOps Guy) - https://www.youtube.com/watch?v=1mNYSn2KMZk
#											- https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/kubernetes/admissioncontrollers/introduction


- admission webhooks
	- types
		- mutating - intercepts object/YAML before it hits API server, allows us to make changes first
			- egs. inject CPU limits, labels, ulimits, go to a specific node, etc.

		- validation - accept or reject request
			- egs.
				- policy enforcement - if CPU/mem limits not set, do not create pod
														 - only allow pods if the image is part of a specific image registry
				- notfiications when events occur


- steps to create a webhook from scratch

	1. Create a K8s cluster [can use kind or k3s for ease]

	2. Create a TLS certificate for the webhook [can use Cloudflare SSL utility as shown below, the swiss army knife for dealing with TLS certs]

		cd kubernetes/admissioncontrollers/introduction      # within the Github repo in the source above
		docker run -it --rm -v ${PWD}}:/work -w /work debian bash


		# a. Install Cloudflare SSL and json utilities

		apt-get update && apt-get install -y curl &&
		curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl && \
		curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson && \
		chmod +x /usr/local/bin/cfssl && \
		chmod +x /usr/local/bin/cfssljson


		# b. Generate a CA certificate in /tmp
		cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca


		# c. Generate webhook certificate

    cfssl gencert \
    	-ca=/tmp/ca.pem \
    	-ca-key=/tmp/ca-key.pem \
    	-config=./tls/ca-config.json \
  	  -hostname="example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1" \
  		-profile=default \
  		./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook



		# d. Make a secret

cat <<EOF > ./tls/example-webhook-tls.yaml
apiVersion: v1
kind: Secret
metadata:
	name: example-webhook-tls
type: Opaque
data:
	tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\n')
  tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\n') 
EOF


		# e. Generate CA Bundle + inject into template
		
		ca_pem_b64="$(openssl base64 -A <"/tmp/ca.pem")"         # where -A means no line breaks so the full certificate string can be stored in the variable

		sed -e 's@${CA_PEM_B64}@'"$ca_pem_b64"'@g' <"webhook-template.yaml" \
    > webhook.yaml


  3. Create the webhook with the generated CA

  # a. YAML templated definition - see https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/kubernetes/admissioncontrollers/introduction/webhook-template.yaml

  apiVersion: admissionregistration.k8s.io/v1
  kind: MutatingWebhookConfiguration
  metadata:
    name: example-webhook
  webhooks:
    - name: example-webhook.default.svc.cluster.local
      admissionReviewVersions:
        - "v1beta1"
      sideEffects: "None"
      timeoutSeconds: 30
      objectSelector:
        matchLabels:
          example-webhook-enabled: "true"
      clientConfig:
        service:
          name: example-webhook
          namespace: default
          path: "/mutate"
        caBundle: "${CA_PEM_B64}"
      rules:
        - operations: [ "CREATE" ]
          apiGroups: [""]
          apiVersions: ["v1"]
          resources: ["pods"]

   # End YAML
 
     - note the CA_PEM_B64 variable, referencing step 2e. Running that will replace the variable with the actual value in webhook.yaml

   #### INCOMPLETE ####





---
ldd
---

# Linux - checking library dependencies
ldd -d 



--------------
Linux [system]
--------------

#### Command-line - special/"heroic" commands ####
# Source: Engineer Man - https://www.youtube.com/watch?v=Zuwa8zlfXSY

# 1. redo last command but as root
sudo !!

# 2. open an editor to run a command
ctrl+x+e

# 3. create a super fast ram disk
mkdir -p /mnt/ram
mount -t tmpfs tmpfs /mnt/ram -o size=8192M

# 4. don't add command to history (note the leading space)
 ls -l

# 5. fix a really long command that you messed up
fc

# 6. tunnel with ssh (local port 3337 -> remote host's 127.0.0.1 on port 6379)
ssh -L 3337:127.0.0.1:6379 root@emkc.org -N

# 7. quickly create folders
mkdir -p folder/{sub1,sub2}/{sub1,sub2,sub3}

# 8. intercept stdout and log to file
cat file | tee -a log | cat > /dev/null

# bonus: exit terminal but leave all processes running
disown -a && exit



#### Kernel commands ####

lsmod       # shows the kernel modules loaded into the kernel

modinfo     								# shows information about a specific kernel module
modinfo -d [kernel_module_name]     		# prints generic info about the module
modinfo -a [kernel_module_name]     		# prints the author of the module
modinfo -n [kernel_module_name]     		# prints location of the module
modinfo -F depends [kernel_module_name]     # prints dependencies of the target module

rmmod [kernel_module_name]                  # removes target module from the kernel
modprobe [kernel_module_name]               # add module to the running kernel. Note the module should exist under /lib/modules/`uname -r`



#### Kernel - General info ####
#### Sources: Engineer Man - https://www.youtube.com/watch?v=CWihl19mJig

- privilege rings in Linux
	- Ring 3 - most outer ring
			 - least privilege
			 - user space/applications - cannot talk directly to hardware

	- Ring 2 - between Ring 3 and 1
			 - device drivers
			 - more privilege than Ring 3

	- Ring 1 - device drivers [pseudo hardware]

	- Ring 0 - the kernel space
			 - inner most ring
			 - most privilege
			 - kernel modules/apps connect user space applications to physical and pseudo hardware
			 - kernel programs [unlike user space applications] run reactionally, responding to events [such as user applications]

# How to build a Linux loadable kernel module [that Rickrolls people]
# Source: Engineer Man - https://www.youtube.com/watch?v=CWihl19mJig


1] Re-creating the example of kernel module called "rickroll.c", which prints a message when the module has been loaded and unloaded

# START
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>

static int __init rickroll_init(void) {
	printk(KERN_INFO "Rickroll module has been loaded\n");
	return 0;
}

static void __exit rickroll_exit(void) {
	printk(KERN_INFO "Rickroll module has been unloaded\n");
}


module_init(rickroll_init);
module_exit(rickroll_exit);

# END


2] Create a Makefile

obj-m += rickroll.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules

clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean



3] Build the module

make     	

- this will create a bunch of files
	- modules.order
	- Module.symvers
	- rickroll.c
	- rickroll.ko     # ko stands for "kernel object"
	- rickroll.mod.c
	- rickroll.mod.o
	- rickroll.o

4] Tail the kernel log in a different terminal

tail -f /var/log/kern.log


5] Inject the kernel module

sudo insmod rickroll.ko 

- the log should print that the module has been loaded


6] Test removing the module

sudo rmmod rickroll

- the log should print that the module has been removed


7] Expand the module to build a device to "Rickroll" people if they read from the device

# Updated module code below for rickroll.c

#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/uaccess.h>

#define DEVICE_NAME "rickroll"

static int dev_open(struct inode*, struct file*);
static int dev_release(struct inode*, struct file*);
static ssize_t dev_read(struct file*, char*, size_t, loff_t*);
static ssize_t dev_write(struct file*, const char*, size_t, loff_t*);

static struct file_operations fops = {
	.open = dev_open,
	.read = dev_read,
	.write = dev_write,
	.release = dev_release,
};

static int major;


static int __init rickroll_init(void) {
	major = register_chrdev(0, DEVICE_NAME, &fops);

	if (major < 0) {
		printk(KERN_ALERT "Rickroll load has failed\n");
		return major;
	}

	printk(KERN_INFO "Rickroll module has been loaded\n");
	return 0;
}


static void __exit rickroll_exit(void) {
	unregister_chrdev(major, DEVICE_NAME);
	printk(KERN_INFO "Rickroll module has been unloaded\n");
}


static int dev_open(struct inode *inodep, struct file *filep) {
	printk(KERN_INFO "Rickroll device opened\n");
	return 0;
}


static ssize_t dev_write(struct file *filep, const char *buffer, size_t len, loff_t *offset) {
	printl(KERN_INFO "Sorry, rickroll is read only\n");
	return -EFAULT;
}


static int dev_release(struct inode *inodep, struct file filep*) {
	printk(KERN_INFO "Rickroll device closed");
	return 0;
}


static ssize_t dev_read(struct file *filep, char *buffer, size_t len, loff_t *offset) {
	int errors = 0;
	char *message = "never gonna give you up, never gonna let you down...;      # missing end double quote on purpose, make sure to add back
	int message_len = strlen(message);

	errors = copy_to_user(buffer, message, message_len);

	return errors == 0 ? message_len : -EFAULT;
}

module_init(rickroll_init);
module_exit(rickroll_exit);

# END 


- Notes on updated rickroll.c
	- dev_open - when the device is open
	- dev_release - when the device is released or closed
	- dev_read - when the device is read from
	- dev_write - when the device is written to
	- register_chrdev - chrdev stands for character device
		- the 0 in this function means dynamically generate a major number
		- the major number is a small integer that serves as the index into a static array of char driver
			- i.e. every device has a major number associated with it
	- return 0 if successful
	- char *buffer is from user space to copy error message into
	- on the return errors line, the ? message_len mean to return it if no errors, or return -EFAULT if neither are true

 #### SOME MORE INFO NEEDED ON OTHER FUNCTIONS IN THE CODE ####


8] Repeat step 5 and make note of the major number from the log

9] Register the new "device" [into /dev directory] and point it to the rickroll module

cd /dev
sudo mknod rickroll c <major_number> 0
ls -l | grep rick


10] Test the new "device"

cat /dev/rickroll

- should print the char *message a lot
- Ctrl ^C to close the device, and the log should print that the device is closed



#### Processes ####

# Sources
	# Engineer Man - https://www.youtube.com/watch?v=TJzltwv7jJs
	# boson.com CompTIA Linux labs

- fork/exec - when executing any command
				- Linux "forks" the shell terminal [or parent] process into two: itself and a copy of itself.
				- then it "execs" by replacing the second [child] process with the command that was entered


#### process commands ####

^C [or Ctrl ^C] - if the child process is still running [i.e. your terminal is still executing a command], this will send a SIGINT to terminate the process
^Z [or Ctrl ^Z] - sends a TSTP signal to Linux to "suspend" the process [equivalent of "kill -20" on a process]


# jobs

jobs        	  # see the jobs table for the user (background processes). You will also see stopped processes that can be restarted.


# fg

fg          	  # execute in the foreground - if there's one stopped job in the jobs table, this will run it by default
fg %3       	  # this will run the 3rd job in the jobs table
fg 2  			  # this will bring the 2nd job in the jobs list (background) to the foreground


# bg

bg                # background
bg %3             # this will run the 3rd job in the jobs table in the background


# kill

kill -15 [pid]    # sends sigterm to terminate the process cleanly
kill -2 [pid]     # sends sigint to terminate process 
kill -1 [pid]     # sends sighup to terminate process
kill -9 [pid]     # sends sigkill to kill the process ungracefully or the "dirty" way
kill -20 [pid]    # stop or suspend a running process
kill -18 [pid]    # restart a suspended process. An example of how kill sends terminal signals and not necessarily all process killers


# pidof

pidof firefox          # lists all process IDs containing firefox in the command


# killall

killall -9 firefox     # kill all processes which contain firefox in the command


# ps (see ps section)



### Reboot commands ###

init 6                  # Reboots and starts back up to run level 6
systemctl reboot        # Reboots via systemctl
shutdown -r 			# -r flag denotes reboot



---------
localtime 
---------

# change time zone

rm -f /etc/localtime
ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime
date



------
locate
------

- (Linux/UNIX) - searches the location of files from a specific database
			   - e.g. it can display all the directies on the system that has a certain name as part of the filepath

#### commands ####

locate /etc            		# find all directories that has /etc in the filepath
locate -S              		# show the database that locate uses to find files and directories
				       		# e.g. /var/lib/mlocate/mlocate.db
				       		# some db names show as mlocate or slocate, depending on the Linux distro

updatedb               		# updates the locate database
cat /etc/updatedb.conf      # show contents of updatedb conf file





--
ln
--

- in Linux, creates a hard or soft link to an existing file or directory
- Symbolic links: It is a pointer to the source file. It can point to a source file on the local or remote filesystem.
- Hard links: It is another directory entry for the source file and carries those same properties, such as file permissions, of the source file.
			  If you delete one file, the other file remains intact. A hard link must exist in the same local filesystem.


#### Commands ####

ln -s <real_path> <soft_link_to_be_created>
ln <real_path> <hard_link_to_be_created>



-----
loops
-----

#### shell scripting ####

# for loop (example) - iterating through all files under /etc/ directory and printing to stdout

#!/bin/bash
echo "Files in the /etc directory are:"
for f in $(ls /etc/*)
do
  echo $f
done


# until loop - do something until certain criteria is met

#!/bin/sh
a=1
until [ $a -gt 6 ]
do
  echo $a
  a=$(( a+1 ))
done


# while loop - do something while something is true

#!/bin/bash
a=0
while [ $a -le 10 ]
do
  echo "Welcome $a times."
  a=$(( a+1 ))  
done



--
ls
--

- (Linux/UNIX) - list files in current directory


#### Commands ####

ls [!Aa]*         # list all files that don't start with an 'a' or 'A'
ls D[eo]*         # list all files that start with 'D' and with second letter 'e' or 'o'




--------
lscolors
--------

# Colourize text in terminal:
export LSCOLORS=gxBxhxDxfxhxhxhxhxcxcx
eval "$(dircolors /etc/DIR_COLORS)"




----
lsof
----

lsof <filename>       # check which processes have this file open
lsof -p <PID>         # check a specific pids open files [could be multiple]
lsof -u <userid>      # check list of open files for specific user
lsof -i <port>        # check which processes are listening on specific port
lsof -i <protocol>    # check which processes are listening on specific protocol [i.e. tcp]



---
Lua
---

# Source: Fireship - https://www.youtube.com/watch?v=jUuqBZwwkQw

- Lua is a lightweight dynamic scripting language often embedded into other programs like World of Warcraft and Roblox.
- Its minimal syntax makes it easier to learn than Python, while being much more performant than other interpreted languages. 


	

---
lxd
---

# Source: https://linuxcontainers.org/

- a next generation system container and virtual machine manager
- the lxc cli that lxd uses should not be confused with lxc itself [which is OS-level virtualization for running multiple Linux containers on a single host]. lxd is really just
  an alternative to lxc, and makes use of lxc tools via liblxc, such as the cli
	


# Commands - the below are lxc cli commands that can be executed within an lxd environment
# Can also try online without installing locally as well at https://linuxcontainers.org/lxd/try-it/


# Basics for listing and running container images

lxc image list        							      # list images stored locally (docker equivalent command: docker images)
lxc image list ubuntu: | less  			    		  # list of images from the ubuntu: registry
lxc launch images:ubuntu/18.04 [container_name]       # run a container called container_name based on the ubuntu 18.04 image
lxc list  											  # check running containers
lxc info [container_name]               			  # inspect running container named container_name
lxc config show [container_name]					  # show configuration details of container_name
lxc stop [container name] 							  # stop running container container_name
lxc delete [container_name]  						  # remove running container container_name
lxc start [container_name]							  # start running container container_name
lxc delete --force [container_name]                   # force stop and delete container_name


# Container interaction commands

lxc exec [container_name] -- free -m                	        # execute free -m within container_name
lxc config set [container_name] limits.memory 128MB   			# set memory limit on container_name
lxc exec [container_name] -- apt-get update           			# run apt-get update within container_name
lxc exec [container_name] -- apt-get install sl -y    			# install sl package within container_name
lxc exec [container_name] -- /usr/games/sl            			# run sl program within container_name [sl stands for 'Steam Locomotive' - displays a train on-screen when you mistype ls command :D ]
lxc snapshot [container_name] clean  							# take a snapshot called clean of running container_name, which can be used for restoration if something messes up in the container
lxc restore [container_name] clean  							# restore the clean snapshot of container_name
lxc file pull [container_name]/etc/hosts .            			# copy a file from container_name to local
lxc file push hosts [container_name]/etc/hosts        			# copy a file from local to container_name
lxc file pull [container_name]/var/log/syslog - | less          # check log files in the container
lxc exec lxdserver:[container_3] bash  				 			# log into remote container on remote lxdserver
lxc copy lxdserver:[container_3] lxdserver:[container_4]        # create a copy of that container on that same remote
lxc move lxdserver:[container_4] [container_5]                  # move the copied remote container to local and rename it to container_5
lxc start [container_5]                                         # and then start container_5


# Image commands

lxc publish [container_name]/clean --alias clean-ubuntu     	# publish the snapshot "clean" of container_name to the default registry
lxc launch clean-ubuntu [container_2]							# launch a new container based on the published image
lxc remote list                                                 # list all remote registries
lxc list lxdserver:  											# list all remote containers on lxdserver
lxc image list lxdserver:	 									# list all images on remote lxdserver
lxc launch clean-ubuntu lxdserver:[container_3]       			# create new container called container_3 on remote lxdserver
lxc image delete clean-ubuntu                                   # delete the published image



---------
Mac OS X
---------

   # check RAM:
   		system_profiler SPHardwareDataType | grep "Memory:"

   # check # of CPU cores:
   		sysctl -a | grep cpu.core_count

   # check GPU:
	 	  system_profiler SPDisplaysDataType

   # stop and start Jenkins
   		sudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist
		  sudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plist

   # update Jenkins URL
   	    sudo defaults write /Library/Preferences/org.jenkins-ci httpPort 8082
   	    sudo defaults write /Library/Preferences/org.jenkins-ci prefix /jenkins
   	    # for brew:
   	       /usr/local/Cellar/jenkins/2.x.x/homebrew.mxcl.jenkins.plist
   	       		update:
   	       			  <string>--httpPort=8082</string>
  					  <string>--prefix=/jenkins</string>
   	       brew services restart jenkins-lts

   # update Jenkins heap size/permGen
        sudo defaults write /Library/Preferences/org.jenkins-ci minPermGen 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci permGen 2048m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci minHeapSize 512m
  		  sudo defaults write /Library/Preferences/org.jenkins-ci heapSize 2048m


   # check installed java versions:
   		  /usr/libexec/java_home -V



--
mc
--

- midnight commander - graphical file manager for UNIX/Linux OS.


# Install

yum install mc      # RHEL-based OS'


# Navigating the GUI - see https://linuxcommand.org/lc3_adv_mc.php

- type 'mc' and ENTER to enter mc graphical interface
- can still type out UNIX/Linux commands and at the same time, use mouse to click directly on files in the directory you are in
- click on file and press 'F3' to view contents 
- for an RPM file, "double click" on RPM, click the cpio file and press "F3" to view the contents



-----
mkdir
-----

- Linux: creates a directory

mkdir -p /var/www/html/repos/{base,centosplus,extras,updates}      # create directories under var (-p flag creates if any directories in command do not exist).
																   # base, centosplus, extras and updates will all be created under repos directory




-----
mount
-----


#### General command line in Linux/UNIX ####

mount               # shows all partitions on a hard disk (possibly all hard disks attached to a Linux server?) and their mount points
umount [fs_path]    # unmount a file system e.g. umount /dev/sdb1


#### fstab ####
cat /etc/fstab      # show the list of partitions detected at boot time. Also shows fs which is mounted on the root partition or /

- format of fstab: <file system>   <dir>   <type>  <options>   <dump>  <pass>
			   eg. UUID="14314872-abd5-24e7-a850-db36fab2c6a1" /lpo/sda ext4 defaults,noatime 0 0


#### RAM disk creation ####
# Source: Engineer Man - https://www.youtube.com/watch?v=Zuwa8zlfXSY

mkdir -p /mnt/ram
mount -t tmpfs tmpfs /mnt/ram -o size=8192M         # mount tmpfs device onto /mnt/ram directory, which will create a new FS
cd /mnt/ram
dd if=/dev/zero of=test.iso bs=1M count=8000

	- "dd" copies a file and converts the data.
		- In the case above, its being used to test the write speed on the new RAM disk
	- "if" - input file
	- "of" - output file
	- "bs" - block size
	- "/dev/zero" - produces a continuous stream of NULL (zero value) bytes.



##### Volume mounting in Linux #####
# Source article: https://www.linode.com/docs/platform/block-storage/how-to-use-block-storage-with-your-linode/#add-a-volume-from-the-linode-detail-page

1] One-time step for defining the FS type of the volume
mkfs.ext4 $volume_path   # check the file system path of the volume in your cloud console, it will show you the device drive location


2] Steps for mounting the volume
mkdir /mnt/my-volume
mount $volume_path /mnt/my-volume
df -h


3] Create entry in /etc/fstab:
FILE_SYSTEM_PATH /mnt/my-volume ext4 defaults,noatime,nofail 0 2

* noatime - This will save space and time by preventing writes made to the filesystem for data being read on the volume.
* nofail - If the volume is not attached, this will allow your server to boot/reboot normally without hanging at dependency failures if the volume is not attached.


4] if you need to unmount it or reboot your Linode without affecting the volume
umount /mnt/my-volume
Remove /etc/fstab entry



#### Re-mount existing volume after fstab update ####

- If you add an entry in fstab for an existing volume that''s already mounted, ensure you unmount the volume after adding to fstab.

1] Entry to add in fstab:

/dev/sdb1  /home  ext4  defaults,usrquota  1  1


2] Commands to remount the FS

umount /dev/sdb1
df -h
mount /dev/sdb1
mount -o remount /path       # remount the file system path previously mounted before fstab update. E.g. "mount -o remount /home" if its part of /dev/sdb1


NOTE: see quotacheck section for managing disk quotas


--------
netgroup
--------

# Check netgroup list (Linux):
> ldaplist -l netgroup <netgroup_name> 



---------
netcat/nc
---------

### SOURCE page: https://www.linode.com/docs/guides/netcat/ ###


# make netcat act as the telnet utility (TCP protocol is the default, use -u for UDP procotol)

nc localhost 22


# make netcat act as a server, accept incoming connection on a given port

nc -l -p 1234


# get more info from remote server (eg. for connectivity issues)

nc -v localhost 1234  [or nc -vv]


# port scanning (eg. on localhost, from ports 1-30)

nc -z -vv -n 127.0.0.1 1-30


# transferring files (a client connects to port 4567 below to receive the access.log contents)

cat access.log | nc -vv -l -p 4567

# and then

nc -vv localhost 4567 > fileToGet
^C [to close the connection]


# turn a process into a server (when client connects, nc will execute /bin/bash, to give shell access to machine)

nc -vv -l -p 12345 -e /bin/bash


# executing a command after connecting

nc -vv -c "ls -l" -l 127.0.0.1 -p 1234


# act as simple web server (eg. can use curl or wget to connect to this port to get the index.html contents)

nc -vv -l 127.0.0.1 -p 4567 < index.html
wget -qO- http://localhost:4567/


# get data from web servers

nc www.linode.com 80

# OR

echo -en "GET / HTTP/1.0\n\n\n" | netcat www.linode.com 80



# Create a chat server

nc -vv -l 127.0.0.1 -p 1234

# and from the client

nc -vv 127.0.0.1 1234

# then type in terminal





-------
netstat
-------

netstat -tupln   # display network connections, tcp and udp, show which processes are using which sockets (need root for p option), -l is for listening sockets

-n - show only numbers, not names [i.e. DNS names, which takes time to calculate when not using the -n switch]
-a - show active connections, where TCP and UDP are listening
-b - show which program is creating the connection [e.g. you'll see something like chrome.exe]
-f - shows FQDN in foreign address column
-? - show all options




--------------------
networking - general
--------------------

- An example LAN [NetworkChucks setup]: PC -> Switch -> Router -> Firewall -> Modem -> Fiber optic -> Internet


# ARP - Address Resolution Protocol
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=cn8Zxh9bPio

- definition - used to resolve IP addresses to MAC addresses [the physical address of a device, eg. 00-04-5A-63-AI-66]

- info
	- devices need the MAC address for communication on a LAN
	- devices use ARP to acquire MAC address for a device
		- IP address is used to locate device on a network
		- MAC address is used to identify the actual device

- process
	1. device checks its ARP cache table, the internal list of all IP/MAC address combinations

		arp -a  				# Windows cmd to check list

	2. device sends a broadcast message to every device on the network for a specific destination IP address it wants to talk to
		- asks for MAC address
	3. receiving device with the destination IP responds with MAC address
	4. stores IP/MAC address info in arp cache [which can be checked with arp -a command]
	5. communication between devices can begin
	6. subsequent communication to that device, it can check the ARP cache instead of broadcasting request to whole network


- two types of ARP entries

	- dynamic - entry is created automatically when a device sends out broadcast message requesting a MAC address
						- entries are not permanent, flushed periodically
	- static - a manual entry of a IP/MAC address into the ARP cache
					 - often used to reduce any unnecessary ARP broadcast traffic on a network

		arp -s x.x.x.x  yy-yy-yy-yy-yy-yy     # command for entering static entry into ARP cache, with IP followed by MAC address




# Bluetooth vs. Wi-Fi
# Source: Powercert Animated Videos - https://www.youtube.com/watch?v=mPMGRILsOVk&list=WL&index=1

- both are radio frequency technlogies for wirelessly connecting electronic devices

- Bluetooth
	- a low power wireless technology that uses a short-range radio that provides a way to connect nearby devices to each other
	- has a computer chip that will broadcast a signal for other devices to connect and exchange data - known as "pairing"

	- most common usages:
		- wireless audio streaming [e.g. wireless earbuds to iPhone]
		- headphones to a TV
		- wireless keyboard/mouse to a computer
		- cellphone to car audio for hands free

	- range: 30ft or 10m


- Wi-Fi
	- a wireless technology that uses radio waves that allows devices to be able to connect to the internet
	- most common method: via Wi-Fi router in a network, for devices to connect to
	- range: 100 - 300ft [30 - 91m]


- differences
	- Bluetooth
		- used to connect devices to each other
		- less power
		- slower transfer rate
		- shorter range
		- longer battery life
		- smaller battery
		- simpler to connect to than Wi-Fi

	
	- Wi-Fi
		- used to connect devices to the internet
		- faster than Bluetooth
		- 10x longer ranger
		- often requires a password

	
- both operate at 2.4GHz
	- other devices also operate at this frequency. Bluetooth is less vulernerable/highly resistant to interference because it uses "Frequency Hopping Spread Spectrum" [FHSS]
		- trasmits signals in a specific pattern that only the transmitting and receiving devices know
		- hops between 79 different channels
		- changes channels 1600 times per second




###### Hubs vs. Switches vs. Routers ######
# Source: https://www.youtube.com/watch?v=1z0ULvg_pW8

- hub - connect all network devices together
			- not intelligent, only knows when something is connected to it
			- data is copied/transmitted to all ports which have a connection to it
				- can be a security concern if you dont want every device to see data being transmitted between one device on the network and another

- switch - like a hub except intelligent, only broadcasts between communicating devices, not every device on network
			   - can detect specific devices that are connected to it
			   - keeps track of mac addresses of the devices connected
			   

- both of the above
			- for internal communication within a network
			- cannot read IP addresses
			- used to create networks


- router - route/fwds data based on the IP address
				 - inspects packet for the IP address before it routes it
				 - the gateway of a network
				 	 - accepts only packets fron the internet intended for its own network
				 - used to connect networks


##### IPv4 and IPv6 Dual Stack #####
- IPv4 and IPv6 together is referred to as "dual stack"
	- IPv4 addresses are running out and are being migrated to IPv6
	- dual stack connectivity allows your ISP to process IPv4 and IPv6 data simultaneously



#### Linux commands ####
 
route -n        # find routing table of server
nestat -rn      # find routing table of server
ip route        # find routing table of server



#### Misc ####

- default gateway - IP address of your router or modem/router combo that your computer connects to
- subnet mask - defines which parts of the IP address refers to the network and host



##### Port Forwarding #####
# Source: https://www.youtube.com/watch?v=2G1ueMDgwxw

- definition: allows computers over the internet to connect to a specfic computer or service within a private network

- info
	- port - a logical connection that is used by programs and services to exchange information
				 - is always associated with an IP address
				 - identified by a unique number
				 - IP address purpose is determined by the port [e.g. some_ip:80 usually means a http webpage, 22 for SSH/SFTP, 443 for HTTPS]
				 - "Well Known Ports" - priviledged category of ports ranging from "0 - 1023"

- eg. Remote Desktop Connection from a public computer to a destination computer on a private network behind a router

	- the router will receive the RDP request and needs to know where to forward for port 3389, the default RDP port
	
		- need to configure the destination computers private IP into the routers webpage
			- under single port forwarding -> app name: RDP, external port: 3389, internal port: 3389, protocol: BOTH, To IP Address: <dest_pc_ip>, Enabled: True
			- source computer will try to hit the router IP:port -> router forwards to internal IP:port 

	

#### TCP wrapper (Linux) ####

- For access control, "inetd" and "xinetd" files use the TCP_WRAPPER service. 
- The xinetd binary file has built-in support for the TCP_WRAPPERS. TCP_WRAPPERS is configured in two different files. These files are:

	/etc/hosts.allow
	/etc/hosts.deny

#######updates needed to above section#######



-----
nginx
-----

# Install nginx on Linux CentOS

yum install -y epel-release       # install epel repo (includes gpg keys for package signing and repository information. Gives OS access to a wider range of available packages to install)
yum install -y nginx              # install nginx (epel repo above is a dependency)


# Enable nginx as a service
systemctl start nginx             # start nginx service
systemctl enable nginx            # enable auto restart if system reboots
systemctl status nginx            # check status of nginx service


# firewall rules to allow http/https traffic
firewall-cmd --zone=public --permanent --add-service=http
firewall-cmd --zone=public --permanent --add-service=https
firewall-cmd --reload

- start firefox within Ubuntu and in address bar, type "localhost" and hit Enter to see default webpage running from nginx


# stop nginx (manual)
  nginx -s stop

# start nginx (manual)
  ./nginx


# Concept in K8s
- ingress.yaml -> service.yaml -> application-deploy.yaml
	- ingress has the routing rules, service pods act as the load balancers to the application pods/nodes



-----------
nice/renice
-----------

# definition: can run a program with a modified scheduling priority
# range: from -20 to 19. Default is 0, the lower the value, the higher the priority (so -20 is highest priority)


# start process with nice value (10 by default)
nice [command]

# start process with a specific nice value (2 in this case)
nice -n 2 [command]

# change nice value of a running process
renice -n 2 -p [pid]

# change nice value for all running processes for a user
renice -n 2 -u [user] 



-----
nmcli
-----

- Linux/UNIX - network manager cli. Controls NetworkManager program and reports on network status.
			 - can also be used to manage hostname of system


#### Commands ####

nmcli general hostname           			# View current hostname of system
sudo nmcli general hostname plablinux02     # Change hostname to plablinux02




------
nodejs
------

# NodeJS Debugging in Docker
# Source Video: Marcel Dempers [That DevOps Guy] - https://www.youtube.com/watch?v=ktvgr9VZ4dc
# Source Code: https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/nodejs    [for docker stuff and source code]
# 						 https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/.vscode   [for vscode add-on launch.json which runs the debugger]
# Source Video for Debugger example: DevOps Directive - https://www.youtube.com/watch?v=5JQlFK6MdVQ


# Prerequisites
	- VScode
	- Docker


# Components

- dockerfile [in the nodejs folder in the Github repo]
	- image: the debugger image can be built using docker compose [YAML can be found at the top of the repo]
	- port exposing: need to expose the server port of the nodejs app and another port for the debugger
	- hot reloading: the source code itself is mounted into the container, technically not needing nodejs installed on the local since its in the container,
		so you just update the code locally and it automatically reflects in the container
		- rebuilding the image: the only time you need to rebuild the actual image is when package.json gets updated since you may need to npm install any updated dependencies
	- debugger: in server.js, inside the request/response function, you can put a "debugger;" line and execution will stop there so you can see whats going on in the execution at that point in time



---
npm
---

# Set auth token in npmrc for a target registry
npm config set '//artifactory_url/artifactory/api/npm/npm/:_authToken' 'zsASDKLNvasdfoloi3w123lk1jhof8jaspodfj'


# Set node-sass binary site for pulling sass bindings during an npm install (if required)
npm config set sass_binary_site=http://artifactory_url:8081/artifactory/node-sass


# Set npm registry for resolving dependencies, publishing
npm config set registry http://artifactory_url:8081/artifactory/api/npm/npm/


# Set root user (in case installs fail with permission denied, only for root or sudo installs)
npm -g config set user root


# Set SSL check to false (use only temporarily as a workaround for certificate errors)
npm config set strict-ssl false


# Remove all node_modules folders on an FS in multiple locations
npx npkill





------
parted
------

- Linux command: allows you to easily manage hard disk partition

parted         					# check parted version and enter parted prompt
parted -l 					   # list partiions


# while in parted menu
	select /dev/sdb 			# choose /dev/sdb device for parted utility to use
	print                       # print partitions on /dev/sdb
	rm                          # remove a partition, will prompt to choose one. NOTE: make sure to unmount a partition before removing it


parted /dev/sda u s p          # display partition table, where:
									# u = abbrev. for unit command
									# s = unit sector [in bytes]
									# p = abbrev. for print command




----------
partitions
----------

*** sections to see also: fdisk, GPT ***


#### General - notes about partitions in Linux ####

- Primary partition - a bootable partition and it contains the operating system/s of the computer
					- generally assigned the first letters in alphabet as the drive letters [e.g. C, D]
- Extended partition - a partition that is not bootable
					 - typically contains multiple logical partitions
	                 - used to store data
					 - a disk drive can contain multiple primary partitions, but it can contain only a single extended partition

- multiple partitions – a boot system can be created using several primary partitions



#### Partitions file ####
# Sources:
	# boson
	# https://www.networkworld.com/article/3297916/examining-partitions-on-linux-systems.html


cat /proc/partitions 			# see table of partitioned devices and their major minor numbers
								# Major number: defines device type
								# Minor number: defines unique instance of the device type

# Eg. /proc/partitions
major minor  #blocks  name
   8        0  117220824 sda
   8        1  117219328 sda1
  11        0    1048575 sr0
   8       16  488386584 sdb
   8       17     512000 sdb1
   8       18  487872512 sdb2


# List of major numbers for devices - see https://www.kernel.org/doc/Documentation/admin-guide/devices.txt


#### Other partition utilities ####

# Commands to bring up editors or display/modify partitions

cfdisk           
sfdisk -l -uM 
fdisk -l 
df                       # shows disk usage
lsscsi                   # lists SCSI devices on a system
smartctl -i [device]     # print drive info about a device
ls /dev/disk/by-id       # determine model and serial number of a disk



--------------------
performance analysis
--------------------

# Sources
- https://www.slideshare.net/brendangregg/container-performance-analysis

# To review
- Brendan Greggs ftrace repo - https://github.com/brendangregg/perf-tools
- BGs eBPF blog post - http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html
- BGs eBPF article - http://www.brendangregg.com/ebpf.html
- eBPF [aka BPF] - https://github.com/iovisor/bcc
- Intel snap - a metric collector used by monitoring GUIs: https://github.com/intelsdi-x/snap
- Collectd plugin - https://github.com/bobrik/collectd-docker


# Basic commands

dmesg | tail    		# shows kernel errors
free -m [or -g]			# memory usage
iostat -xz 1				# disk I/O
mpstat -P ALL 1			# CPU balance
netstat -s 					# shows statistics by protocol (network)
pidstat 1						# process usage
sar -n DEV 1				# network I/O
sar -n TCP,ETCP, 1  # TCP stats
top 								# overview of processes (hit 1 to see CPU usage). Note: does not show container ID
uptime     					# shows time passed since last reboot of server
vmstat 1						# overall stats by time


# Advanced tools

iosnoop					# disk I/O events w/ latency
btrfsdist				# latency histogram
zfsslower 1			# file system latency is a better pain indicator than disk latency


# Namespaces - limit what you can see, whereas cgroups limit what you can use

cat /proc/<PID>/cgroup 						 					 # events for target container within the cgroup
docker stats 								 								 # a top for containers
dockerpsns.sh [or docker ps --namespaces]    # an initial check before deep dive. More info at https://github.com/docker/docker/issues/32501
grep <PID> /sys/fs/cgroup/cpu,cpuacct/docker/*/tasks | cut -d/ -f7      # will show container associated with PID
htop 										 # can show cgroup (unlike top), but may truncate important info
ls -l /proc/<PID>/ns/*						 # shows ns info about a process
nsenter -t <PID> -u hostname				 # check which host target PID is running on
	-m, -u, -i, -n, -p, -U 					 # mount, uts, ipc, net, pid, user
nsenter -t <PID> -n netstat -i 				 # container netstat
nsenter -t <PID> -m -p df -h 				 # container file system usage
nsenter -t <PID> -p top 					 # container top
nsenter -t <PID> -m -p top 					 # -m for more?
	grep NSpid /proc/<PID>/status 			 # an alternative to above command
systemd-cgtop								 # a top for cgroups



# cgroup metrics cont'd

cd /sys/fs/cgroup/cpu,cpuacct/docker/<container_ID>
ls
cat cpuacct.usage
cat cpu.stat 								 # will show total time throttled



# CPU profiling

perf record -F 49 -a -g -- sleep 30

	# Limitations
	- perf wont be able to find /tmp/perf-PID.map files on the host, PID would be different as well
	- perf cant find container binaries in host paths [i.e. what /usr/bin/java?]

	# Other notes
	- Can copy files to the host, map PIDs, then run perf script/report:
		- https://blog.alicegoldfuss.com/making-flamegraphs-with-containerized-java/
		- http://batey.info/docker-jvm-flamegraphs.html
	- Can nsenter (-m -u -i -n -p) a "power" shell, and then run perf -p PID
	- perf should be fixed to be namespace aware


# CPU Flame Graphs

git clone --depth 1 https://github.com/brendangregg/FlameGraph
cd FlameGraph
perf record -F 40 -a -g -- sleep 30
perf script | ./stackcollapse-perf.pl | ./flamegraph.pl > perf.avg

	# Notes
	- see CPU Profiling section for getting perf symbols to work
	- from the host, can study all containers, as well as container overheads

strace -fp <PID>							 # trace/debugging target PID


# ftrace

funccount '*ovl*'							 # show kernel function calls (overlay)
kprobe -s 'p:ovl_fill_merge ctx=%di name=+0(%si):string'   # look into more


# BPF (Berkeley Packet Filter)
runqlat -p <PID> 10 1						 # show queue latency, 10 lines, update every second
runqlat --pidnss -m 						 # show per namespace


#### Summary ####

Identify bottlenecks:
1. in the host vs. container, using system metrics
2. in application code on contaienrs, using CPU flame graphs
3. deeper in the kernel, using tracing tools



------
Piston
------

# Source: Engineer Man - https://www.youtube.com/watch?v=SD4KgwdjmdI
#									     - https://github.com/engineer-man/piston

- This is a code execution engine built by Engineer Man, originally running on Docker but was moved to LXC for better performance [and incidentally, better security]
	- you can take any source code and put it into the engine, and it will execute it for you
	- it even runs untrusted and possibly malicious code without fear of any harmful effects


### To be filled in with more details at a later date if a useful place to apply it is found ###


--
ps
--

ps eww -p [PID]          		# Check full process args on Linux
ps -auwwwx | grep [anything]    # Alternative to above
									# -a lists all running processes
									# -u: Lists the user who is running the process
									# -x: Lists all running processes even if they are not part of the current terminal session
									# grep for specific criteria
ps -ef                          # full format listing of all running processes the user has access to
ps f 1                          # list init process




------
python
------

# Constructors (quick example below) - used to enter values to variables at the time of instantiating an object

class Customer:
  def __init__(self, c="", f="", l="")
  customerID = c
  firstName = f
  lastName = l
  def fullName(self):
    return self.firstName + " " + self.lastName

aleem = Customer("001", "Aleem", "Janmohamed")


# Dictionary (quick example) - used to gather different types of data into one object

def getCustomers():
	"a": "Some Guy 1",
	"b": "Some Guy 2",
	"c": "Some Guy 3",
	"d": "Some Guy 4",
	"e": Customer("001", "Aleem", "Janmohamed")    # example of using constructor within a dictionary

def getCustomer(customerID):

customers = getCustomers()
for customerID in customers:
  print(customers[customerID].fullName())



# Threading vs Multiprocessing
# Source: Engineer Man - https://www.youtube.com/watch?v=ecKWiaHCEKs

- both are trying to do the same thing: to run multiple things at the same time.

Threading:
- A new thread is spawned within the existing process
- Starting a thread is faster than starting a process
- Memory is shared between all threads
- Mutexes often necessary to control access to shared data
- One GIL (Global Interpreter Lock) for all threads

Multiprocessing:
- A new process is started independent from the first process
- Starting a process is slower than starting a thread
- Memory is not shared between processes
- Mutexes not necessary (unless threading in the new process)
- One GIL (Global Interpreter Lock) for each process



-----
quota
-----

- in Linux, "quota" displays disk usage and limits. Additional quota commands below to view more quota details



#### quotacheck - Definition and details - source: https://linux.die.net/man/8/quotacheck ####

- examines each filesystem, builds a table of current disk usage, and compares this table against that recorded in the disk quota file for the filesystem
	 (this step is ommitted if option -c is specified). 
	 - If any inconsistencies are detected, both the quota file and the current system copy of the incorrect quotas are updated (the latter only occurs if an active filesystem
	   is checked which is not advised). By default, only user quotas are checked. quotacheck expects each filesystem to be checked to have quota files named [a]quota.user and
	   [a]quota.group located at the root of the associated filesystem. If a file is not present, quotacheck will create it.

- If the quota file is corrupted, quotacheck tries to save as much data as possible. Rescuing data may need user intervention.
- With no additional options quotacheck will simply exit in such a situation. When in interactive mode (option -i) , the user is asked for advice. Advice can also be provided from command line (see option -n) , which is useful when quotacheck is run automatically (ie. from script) and failure is unacceptable.

- "quotacheck" should be run each time the system boots and mounts non-valid filesystems. This is most likely to happen after a system crash.

- It is strongly recommended to run quotacheck with quotas turned off for the filesystem. Otherwise, possible damage or loss to data in the quota files can result.
- It is also unwise to run quotacheck on a live filesystem as actual usage may change during the scan. To prevent this, quotacheck tries to remount the filesystem read-only before starting the scan. After the scan is done it remounts the filesystem read-write. You can disable this with option -m. You can also make quotacheck ignore the failure to remount the filesystem read-only with option -M.


# Enable quotacheck for the FS 

quotacheck -cug /home
	- -c option creates the quote files for each filesystem that has quota enabled
	- -u option creates these files for a user quota
	- -g option creates these files for a group quota


# List the files put into the file-table via quotacheck command above

quotacheck -avug
	- -a option checks for locally mounted filesystems that have quota enabled. 
	- -v option is for displaying verbose status.
	- -u option is for user quota
	- -g option is for group quota


# Update the quota for a user

edquota [user_id]

# Output: edit window for the target user's quota #####
Disk quotas for user [user_id] (uid @@@@):
  Filesystem      		blocks  	soft     hard     inodes     soft     hard
  /dev/sdb1                  0         0        0          0        0        0 

- you can use vi to modify the above values


repquota /dev/sdb1      # view quota report for target filesystem
quota -u [user_id]  	# view quota statistics for a user



----
RAID
----

# Sources: Powercert Animated Videos - https://www.youtube.com/watch?v=U-OCdTeZLac [RAID 0, 1, 5, 10]
#																		 - https://www.youtube.com/watch?v=UuUgfCvt9-Q [RAID 5 & 6]


- RAID stands for "Redundant Array of Independent Disks"
- used for data loss prevention in the event of hardware failure
	- data is spread across multiple disks for fault tolerance

- types:

	- RAID 0
		- not fault tolerant
		- data is 'striped' across multiple disks
			- eg. disk A would have certain parts of the data, disk B would have the other parts. If one of these disks fail, all of the data would be lost
		- advantage: speed
			- 2 disk controllers instead of one, makes data access much faster

	- RAID 1
		- fault tolerant
		- data is copied on more than 1 disk [duplicate copy of the data across each disk]
			- i.e. each disk has the same data 

	- RAID 5
		- requires 3 or more disks
		- fast and can store a large amount of data
		- can handle single disk failure, cannot handle two disk failures at the same time
		- data is not duplicated but striped across multiple disks along with "parity"
			- parity
				- an entire copy of the data across each disk
				- used to rebuild a failed disk
				- eg. an array of 4 disks totaling 4 TB
					- only 3 TB used for data storage
					- 1 TB used to store parity for rebuilding any of the 4 disks in the event of failure

	- RAID 6
		- requires 4 or more disks
		- 50% capacity used for data storage
		- like RAID 5 in terms of data striping, but parity is spread TWICE on each disk
			- can handle 2 disk failures at the same time [rare occurrence]
				- if 2 drives fail at the same time, double parity from other disks would be used to rebuild the data on the new drives
		- read performance is the same between RAID 5 and 6
		- write performance is slower because it has to write 2 independent parity blocks instead of 1


	- RAID 10
		- combines RAID 0 and 1 together
		- benefits of RAID fault tolerance from RAID 1 with the speed from RAID 0
			- downside: can only use 50% of the capacity for data storage
		- minimum of 4 disks needed
		- RAID 0 pointing to 2 RAID 1 sets of 2 disks 
			- RAID 0 -> RAID 1 -> disk 1 
												 -> disk 2
							 -> RAID 1 -> disk 3
							 					 -> disk 4





------------
Raspberry Pi
------------

# Installing Kubernetes on Raspberry Pis'
# Source - NetowrkChuck - https://www.youtube.com/watch?v=X9fSMGkjtug

- Things youll need:
	- 2 or more Raspberry Pis
	- micro SD USB adapter
	- micro SD card


1) Plug in your micro SD card into your PC and install the Rapberry Pi OS image onto it (can be headless, such as Raspberry Pi OS Lite)
	- using Rapberry Pi imager program

2) Plug the micro SD into your Rapberry Pi and let it boot up

3) Once it is up, remove the SD card, and plug it back into your computer
	- booting the Raspberry Pi for the first time creates a boot folder. Some edits are required, so next step...

4) In the "boot" folder [will show up as a drive once you plug back into your PC], open cmdline.txt and paste the below text:

		cgroup_memory=1 cgroup_enable=memory ip=x.x.x.x::y.y.y.y:255.255.255.0:somehostname:eth0:off

				- where
					- x.x.x.x is an internal IP in your network that isnt used
					- y.y.y.y is the default gateway
					- 255.255.255.0 is the subnet mask
					- somehostname is the hostname for your Raspberry Pi
					- eth0 is the network interface card
					- off means turn off auto configuration


5) Edit config.txt, scroll to the bottom and add: arm_64bit=1
	- means to use 64 bit of Raspbian [the Raspberry Pi OS] 


6) Enabling SSH on the Raspberry Pi - open Powershell and in the boot drives directory, run:

	new-item ssh
		- this will create a new blank file called ssh

	Note: on Mac or Linux, you would run touch ssh


7) Plug the SD card back into the Raspberry Pi, wait to boot

8) On your PC, ping the Raspberry Pi at the internal address you entered in cmdline.txt [and confirm the Rapberry Pi replies]
	
	ping x.x.x.x -t

		- where -t flag means continously


9) SSH into the Raspbeery Pi

	ssh pi@x.x.x.x
		- password would be "raspberry"


10) Become the root user

	sudo su -


11) Enable IPTables

	sudo iptables -F
	sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy


12) Reboot the Raspberry Pi

	sudo reboot


13) Repeat on each Raspberry Pi you want to configure into your cluster


14) SSH back into your first Raspberry Pi and switch to root


15) Install k3s on your first node [which will become your master node]

	curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s          # kubeconfig mode setting will allow to import k3s into Rancher


16) CHeck your cluster

	kubectl get nodes   				# should only show one node at this point


17) Get the master nodes token:

	cat /var/lib/rancher/k3s/server/node-token 				# use this token to register the rest of the nodes to the cluster


18) On another Raspberry Pi, install k3s, registering it as a worker node:

	curl -sfL https://get.k3s.io | K3S_TOKEN="YOURTOKEN" K3S_URL="https://x.x.x.x:6443" K3S_NODE_NAME="somehostname#" sh -s

		- where somehostname# is a unique name for your node

		- note: kubectl can only be used on the master, not the worker nodes 


19) Check your cluster again

	kubectl get nodes



20) Install Rancher on another VM or container [switch to root]

	mkdir -p /etc/rancher/rke2
	cd /etc/rancher/rke2
	vi config.yaml

	# contents of config.yaml

	token: mylittlepony
	tls-san:
		- x.x.x.x             # where x.x.x.x is the VM/container IP

  # end contents

  curl -sfL https://get.rancher.io | sh -


21) Verify Rancher was installed

	 rancherd --help


22) Enable Rancher as a service and start it

	systemctl enable rancherd-server.service
	systemctl start rancherd-server.service

	# this also sets up a K8s cluster within the VM/container with Rancher installed in it


23) Get Rancher login creds

  rancherd reset-admin   # will give you server URL + default username and password to login to the console with


24) Once you login to the Rancher console [and reset your password], add your Raspberry Pi cluster:

	- two options to choose from
		- I want to create or manage multiple clusters                  <----- choose this one
		- Im only going to use the cluster Rancher was installed on

	- agree to Terms and Conditions and continue

	- copy and save server URL thats generated


25) You will see the Rancher local cluster. Next step is to add the Raspberry Pi cluster

	- click Add Cluster
	- click Other Cluster
	- enter a name for the cluster
	- run the insecure curl command in your terminal [or secure if your K3s cluster is secure] to import the Raspberry Pi cluster into Rancher
	- click Done in the console once the command executes successfully





---------------------
reboots and runlevels
---------------------

- Linux/UNIX - prints the previous and current SysV runlevel (if known)


#### Commands ####

runlevel          		# prints runlevel  e.g. N 5, which means previous runlevel not known and current runlevel is 5
telinit 3         		# switches the runlevel of the system to 3
init 6          	    # shuts down and reboots system into runlevel 6

shutdown -h +10 	    # schedule shutdown of the system to occur in 10 minutes
shutdown -c             # cancel scheduled shutdown
shutdown -r 11:00       # reboot the system at 11am (system time).
						# With no parameter, wait time is 1 minute

reboot                  # reboots system

wall “This system will be rebooted at 11:00 AM.”       # broadcast to users system will reboot at 11:00am


service vmtoolsd.service stop      # stop the vmtoolsd service (redirects to systemctl)




----------
References 
----------

# Technical
- https://crontab.guru/          # service to figure out any crontab entry
- https://bundlephobia.com/      # check cost of adding select NPM packages
- https://www.shellcheck.net/    # shell scripting syntax checker
- https://explainshell.com/      # check what individual CLI lines mean [help text]
- https://groovy-lang.org        # official site for learning the Groovy programming language


# Training
- boson.com - highly recommended by NetworkChuck [YT] for CCNA, CCNP, etc. certification
- https://www.youtube.com/watch?v=uEVmD6n8Il0 - NodeJS deployment strategies [Fireship]



-----
Regex
-----

^[0-9]{3}-[0-9]{3}-[0-9]{4}$     # match any phone number
^[0-9]{3}-?[0-9]{3}-?[0-9]{4}$   # match three numbers before first dash, matching second and third sets are optional 
^[a-zA-Z0-9._]+@[a-zA-Z0-9]+\.[a-zA-Z]{2,3}$	 # one or more of the first set, followed by @, followed by one or more letters or numbers
^[\w\d._]+@[\w\d]+\.[\w]{2,3}$   # alternative to above, will match foreign letters and numbers as well because of \w and \d

Legend:

	Assertions/Quantifiers
		$		matches end of line
		^		matches beginning of line
		*		0 or more
		?		0 or 1
		+		1 or more
		{n}		exactly n
		{n,}	n or more
		{n,m}	between n and m


	Characters
		.		any exxcept newline
		[abc]	a, b, or c
		[a-z]	a, b, ...., z
		[^abc]	anything but a, b, c
		\d      digit
		\s      status
		\w      word character


	Special
		\n      newline
		\t      tab



-----
redis
-----

- stands for 'Remote Dictionary Server' - an in-memory multi-model database
- RAM-based processing - data processing happens on RAM, but also stored on disk in case of restructuring/rebuilding needed
- key/value store - storing data in the form of key/value (different types include string, bitmap, hash, list, set, stream)
- secondary database - can be used as cache support for relational DBs
- **primary DB** - can also be the primary database for even a large scale application
- plugins - can install add-on module plugins to help with structuring, searching, etc.
- cli - has its own command line (naturally...)




--------
reposync
--------

- Linux (CentOS): syncs local with remote repos

mkdir -p /var/www/html/repos/{base,centosplus,extras,updates}         # create directories necessary for syncing


# Commands to sync local with packages stored in the remote repo locations

reposync -g -l -d -m --repoid=base --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=centosplus --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=extras --newest-only --download-metadata --download_path=/var/www/html/repos/
reposync -g -l -d -m --repoid=updates --newest-only --download-metadata --download_path=/var/www/html/repos/


# Verify packages are pulled down (similar for centosplus, extras and updates)

ls -l /var/www/html/repos/base/Packages/


# Create comps.xml 
touch /var/www/html/repos/centosplus/comps.xml
touch /var/www/html/repos/extras/comps.xml
touch /var/www/html/repos/updates/comps.xml
touch /var/www/html/repos/base/comps.xml


# Create new repodata for the local repos
createrepo -g comps.xml /var/www/html/repos/base/


# Create a script to do this on the regular
vi /etc/cron.daily/updaterepos

    #### content below ####
	#!/bin/bash
	LOCAL_REPOS=”base centosplus extras updates”
	for REPO in ${LOCAL_REPOS}; do
	reposync -g -l -d -m --repoid=$REPO --newest-only --download-metadata --download_path=/var/www/html/repos/
	createrepo -g comps.xml /var/www/html/repos/$REPO/
	done
    #### end content #####

chmod 755 /etc/cron.daily/updaterepos



---
rpm
---

- Redhat Package Manager - the package manager used on RHEL OS (along with yum)

#### General commands #####

RESP_FILE=<app>-environment.response; export RESP_FILE    # set response file for an RPM - 

rpm --initdb --dbpath /path/to/rpm_db   # intialize rpmdb, which stores info about all RPMs installed. This is custom vs. rpmdb which is configured at the OS level
rpm --dbpath /path/to/rpm_db --prefix /install_path/ --nodeps -Uvh /path/to/rpm/ebm-wca-1.0.0-1.x86_64.rpm   # install RPM into prefix path
																											 # write into target RPM db and upgrade it if package already exists
																											 # skip testing any dependencies
rpm -q [pkg_name]                              # show some detail about a package
rpm -qi [rpm_name] --dbpath /path/to/rpm_db    # query package
rpm -V [rpm_name] --dbpath /path/to/rpm_db     # check pkgmap
rpm -ev pkg_name --dbpath /path/to/rpm_db      # uninstall rpm
rpm -qlpv [rpm_name.rpm]                       # list files in an RPM
rpm -ql [installed_package_name]  			   # list files in an installed RPM package
rpm -q --scripts [installed_package_name]	   # check scriptlets of an installed package

rpm2cpio squid-3.5.20-12.el7.x86_64.rpm | cpio -idmv     # converts squid rpm to cpio archive





-------
secrets
-------

# General levels of keeping secrets
# Source: DevOps Directive [Sid Palas] - https://www.youtube.com/watch?v=7NTFZoDpzbQ

1) Hardcode directly in the code

2) Create a config file, but still stored in plaintext
	- generally ok to check into source if the secrets are for dev envs 

3) Encrypt the config file
	- can use openssl to encrypt the config

		openssl aes-256-cbc -d -a -salt -in secrets.env.enc -out secrets.env -pass pass:Where-am-I-supposed-to-store-this?!
		cat secrets.env
		
		# eg. file contents
		DB_PASS=somethingsomething


4) Use a dedicated secret manager
	- eg. in code, secret manager is called and variable referenced


5) Ephemeral or temporary credentials (eg. using Hashicorp vault)
	- rotates/renews credentials automatically, creds have an enforced expiry



# Apply secret to a K8s namespace from a file

kubectl apply -f secret.yaml


# Apply secret using K8s native CLI
	
	# passing the string directly
	
	kubectl create secret generic my-secret \
		--from-literal=APU_TOKEN=password123

	# pass from a file
	
	kubectl create secret generic my-secret \
		--from-file=cert=/path/to/cert/file`


# secret yaml example
apiVersion: v1
kind: Secret
metadata:
	name: my-secret
data:
	API_TOKEN: c4FDca23zdfA4A=


# Use secret in app (from file) - sub-section of yaml

spec:
  containers:
..
..
..
     env:
       - name: API_TOKEN
         valueFrom:
         	secretKeyRef:
         		name: my-secret
         		key: API_TOKEN


# Use secret from a volume

spec:
  containers:
  ..
  ..
  ..
     volumeMounts:
       - name: secret-volume
       - mountPath: /etc/secret-volume
  volumes:
    - name: secret-volume
      secret:
      	secretName: my-secret



# Creating a generic secret in Kubernetes
# Source: Just Me And Opensource (Venkat) - https://www.youtube.com/watch?v=CK87fP2_tDs

- three types of secrets: docker-registy, generic, or TLS

kubectl create secret generic mysecret --from-literal=username=venkat --from-literal=password=hello

kubectl get secrets                   # see all secrets in cluster

kubectl describe secret mysecret      # see info and contents of mysecret

kubectl edit secret mysecret          # Edit a secret 


# Example of secret yaml in edit mode, where you can edit user or password or both, unless you put the immutable flag as shown below
# You will get error: secrets "mysecret" is invalid, if you try to save an edited secret where the immutable flag is set

apiVersion: v1
data:
	password: aGVsbG8=
	username: dmVua2F0
kind: Secret
immutable: true
metadata:
	creationTimestamp: "2021-05-09T17:17:01Z"
	name: mysecret
	namespace: default
	resourceVersion: "2264"
	uid: c98b0109-c095-4a3d-97f3-dd8d41057752
type: Opaque


kubectl delete secret mysecret     # deletes secret - the only way to "edit" an immutable secret


---
sed
---

- Linux/UNIX - search and replace tool


#### Examples ####

sed -i -e 's/few/asd/g' hello.txt     # Simple search and replace
sed '/^#/ d ' yumtest.conf            # Remove all commented lines from yumtest.conf
sed '/^  / d' yumtest.conf            # Remove all blank lines from yumtest.conf

sed -e '/^$/ d'  -e 's/Fedora/Linux/g'  yumtest.conf     # Remove all blank lines and also replace all occurences of Fedora with Linux




----
sftp
----

# SFTP debug:
sftp -vv account@server




---------------
Shell scripting
---------------

read -ps    # -p is for prompt, -s is for secret text



-----
shipa
-----

Video Source: Making Kubernetes disappear with Shipa - https://www.youtube.com/watch?v=PW44JaAlI_8



----
smtp
----

- Simple Message Transfer Protocol - used for sending emails between computers/devices, usually through SMTP servers


# SMTP error codes
# Source: https://sendgrid.com/blog/smtp-server-response-codes-explained/

250 – This SMTP server response simply means everything went well and your message was delivered to the recipient server.
421 – Your message was temporarily deferred by the recipient server. This is usually a result of too many connections in a short timeframe or too many messages.
450 – Your message was not delivered because the other user mailbox was not available. This can happen if the mailbox is locked or is not routable.
451 – This response is sent when the message simply failed. Often times this is not caused by you, but rather because of a far-end server problem.
452 – This kind of response is sent back when there isn’t enough system storage to send the message. Your message is deferred until storage opens up and it can then be delivered.
550 – The message has failed because the other user’s mailbox is unavailable or because the recipient server rejected your message.
551 – The mailbox your message was intended for does not exist on the recipient server.
552 – The mailbox your message was sent to does not have enough storage to accept your message.
553 – You message was not delivered because the name of the mailbox you sent to does not exist.
554 – This is a very vague message failure response that can refer to any number of problems either on your end or with the recipient server.




------------------------------------------------------
ssh public/private key generation for passwordless SSH
------------------------------------------------------

1) on the client side, create an ssh public/private key pair:
	ssh-keygen -t rsa -b 4096
	# do not enter a passphrase, just hit enter

2) Copy the contents of id_rsa.pub and ssh to the target server, cd to the users .ssh folder, edit authorized_keys, and paste the pub key entry into the file and save. You should now be able to ssh from the client to the server without a password.

3) You can also try updating authorized_keys from the client like so:
	ssh-copy-id -i ~/.ssh/id_rsa.pub user@host
	

# SSH tunneling - requires SSH server to be set up on target machine
# source: https://www.youtube.com/watch?v=AtuAdk4MwWw&list=WL&index=9

# tunnel from local machine to a remote machine
ssh -L <port_to_expose>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>


# create SOCKS proxy
ssh -D <any_port> <dest_user>@<destination_SSH_server_IP>

	# after running the above, you can for example set this in your LAN settings in your browser, enter the details in the SOCKS proxy section, and any site you visit will be forwarded through the
	# SSH tunnel to the destination specified


# Set up a SSH tunnel from the target server for a "local" PC or server to access
ssh -R <any_port>:<destination_IP>:<port_to_forward> <dest_user>@<destination_IP>

	# enter the remote server's IP and any_port in your local browser via remote desktop and you'll be able to remote into the target machine



---
SSL
---

# SSL handshake overview
# Source: F5 DevCentral - https://www.youtube.com/watch?v=cuR05y_2Gxc

1] client sends a request to the server to establish a connection
	- client sends a "Hello" message with:
		- TLS version
		- Cipher suites it can support

2] server sends a "Hello" message back:
	- also sends a certificate [containing its public key]
		- client uses the public key from the server for encryption
	- server chooses cipher suite to use for client based on clients list

3] client checks certificate for validity

4] server sends a "Hello" done [meaning initial communication has been completed]

5] initiating key exchange - client generates a "pre-master secret" and encrypts it with the public key to send back to the server
	- client also sends a "change cipher spec" message

6] server uses private key to decrypt pre-master secret

7] client and server calculate the "symmetric encryption key"

8] client sends a "client finished" message, telling the server it is finished create its symmetric encryption key

9] server sends a "change cipher spec" message to client so that both can alter the secret writing sent between them
	- also sends a finished message for its symmetric key generation

10] communication between server and client is now symmetric
	- AES encryption as opposed to RSA or DHEC in the first 9 steps for asymmetric communication
	- data in greater amounts can now flow between client and server



# TLS 1.3 handshake overview
# Source: F5 DevCentral - https://www.youtube.com/watch?v=yPdJVvSyMqk

1] client sends a "Hello" message with supported ciphers and key agreements
	- calculates a key share pre-emptively
		- will calculate/assume what cipher suite the server will choose based on its list

2] server will respond with "Hello" message
	- chooses cipher suite
	- generates its own key share

- Note: if server does not have any of the supported ciphers in the client list
	- server sends a "Hello" retry to client
	- if theres still no match in ciphers, server will abort the connection and let the client know

3] server sends above + signed certificate back to client

4] server generates a symmetric key, sends a finished message to client

5] client generates a symmetric key, sends a finished message to server

6] normal traffic between client and server begins



-------
Storage
-------

# Block vs File Storage: very traditional vs. the newer storage types

Block
	- accessed through SAN (storage area network)
	- lowest possibly latency
	- high performing
	- highly redundant


File
	- accessed via NAS (network area storage - all servers connecting to one place vs. san routing you to the right storage device)
	- highly scalable
	- accessible to multiple runtimes
	- simultaneous read/writes from multiple users


If you need:
	- boot volume - use block storage
	- lowest latency - block storage
	- mix of structured and unstructured data - file storage
	- share data with many users at once - file storage



# NAS vs SAN

NAS - Network Attached Storage
		- store data in a centralized location accessible to all devices
		- just stores data, thats it
		- will have multiple hard drives in a RAID configuration
		- will have NIC to connect to router so its network accessible, accessed as a shared drive
		- small to medium scalability
		- disadvantage:
			- single point of failures


SAN - Storage Area Network
		- special high-speed network, stores and provides access to large amounts of data (dedicated for data storage)
		- multiple disk arrays, switches, and servers
		- advantages:
			- shared, therefore fault tolerant
			- recognized as a single drive from an OS
			- interconnected with fiber channel (from 2gbps to 128gbps - ultra fast)
			- not affected by network traffic
			- highly scalable
			- very redundant
		- disadvantage
			- fiber channel expensive (alternative iSCSI, cheaper but not as fast)
			- very expensive in general


----
swap
----

# General info
# Source: https://opensource.com/article/18/9/swap-space-linux-systems

- the primary function of swap space is to substitute disk space for RAM memory when real RAM fills up and more space is needed.


# Commands

swapon -s       # shows swap info 




-------
systemd
-------

# Basic setup
# Source: Engineer Man - https://www.youtube.com/watch?v=unIAGt5pB7A&list=WL&index=3

- organizes programs in units, uses unit files to manage their state
- systemd basically auto-manages programs you define for you by auto starting them when they go down, when the system goes down

1) Create a unit file [e.g. somename.service] to define your service:

# File contents

[Unit]
Description=SomeReallyImportantService

[Service]
Type=simple
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]
WantedBy=multi-user.target


# Or can use more complex file contents below

[Unit]
Description=SomeReallyImportantService
Wants=something.service dependedon.service
Requires=dependedon.service
After=something.service

[Service]
Type=simple
User=root
Group=root
Environment=SOMEVAR=someval
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]     ############################################## need to learn more on this section ###################################################
WantedBy=multi-user.target


# Above sections
- Description - describes the service [info]
- Wants - also starts services listed other than this service
- Requires - like Wants, except if a service listed fails to start, this service will also fail to start
- After - starts listed service[s] first before starting this service

- Type - defines how the process starts up     # different types defined here: https://www.freedesktop.org/software/systemd/man/systemd.service.html#
- User and Group - start service as specified user and group
- Environment - add variables to inject into start up
- WorkingDirectory - for temp or generated files
- ExecStart - the path to the script/program to run as part of this service
- TimeoutSec - up to specified number of seconds to start
- Restart - defines restart policy [e.g. always]
- RestartSec - how long to wait before restarting, if service goes down


2) Save the file in /etc/systemd/system/
	- note: /lib/systemd/system/ contains all the service files


3) See below the different commands to manage the service

systemctl start [service] 
systemctl stop [service]
systemctl reload [service]
systemctl restart [service]
systemctl reload-or-restart [service]
systemctl enable [service] 							
systemctl disable [service] 	  # If service is disabled, service will not start upon system boot/reboot
systemctl is-active [service]
systemctl is-enabled [service]
systemctl daemon-reload           # Reload systemctl daemon after
systemctl list-units
systemctl list unit-files         # Lists all services and their status'
								  # See following link for possible statuses: https://www.freedesktop.org/software/systemd/man/systemctl.html



# Advanced configuration of systemd
# see also : https://www.freedesktop.org/software/systemd/man/systemd.unit.html

- multi-user.target - if service is enabled, tells systemd at what run level should this service start [above example corresponds to run level "2"]


# Rough Guide on Run Levels - Linux Standard Base [LSB] 4.1.0 [see Wiki on "Runlevel"]
ID			Name																	Description
0				Off																		Turns off the device.
1				Single-user mode											Mode for administrative tasks.
2				Multi-user mode												Does not configure network interfaces and does not export networks services.
3				Multi-user mode with networking				Starts the system normally.
4				Not used/user-definable								For special purposes.
5				Full mode															Same as runlevel 3 + display manager.
6				Reboot																Reboots the device.




---
tee
---

- Linux/UNIX - reads from standard input and writes to standard output or to a file


#### Examples ####

df -h | tee disk_usage.txt                       # df -h output gets written into disk_usage.txt
command | tee file1.out file2.out file3.out      # write command output to multiple files




-----------------
terraform - azure
-----------------

Source: https://www.youtube.com/watch?v=bHjS4xqwc9A

1) Get an Azure subscription (can be free)

2) Authenticate with Azure (best to use Azure CLI - can download from Docker Hub)
	docker run -it --rm -v ${PWD}:/work -w /work --entrypoint /bin/bash mcr.microsoft.com/azure-cli:2.6.0
	az login
	export TENANT_ID=<your_tenant_id>

3) List subscription details and set
	az account list -o table
	SUBSCRIPTION=<id>
	az account set --subscription $SUBSCRIPTION


4) Create service principal

SERVICE_PRINCIPAL_JSON=$(az ad sp create-for-rbac --skip-assignment --name aks-getting-started-sp -o json)

SERVICE_PRINCIPAL=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.appID')
SERVICE_PRINCIPAL_SECRET=$(echo $SERVICE_PRINCIPAL_JSON | jq -r '.password')    # Note: keep appID and password for later use

	# where jq is a cli which is like sed for JSON data


5) Assign a role to the newly created service principal

az role assignment create --assignee $SERVICE_PRINCIPAL \
--scope "/subscriptions/$SUBSCRIPTION" \
--role Contributor


6) Install Terraform CLI

curl -o /tmp/terraform.zip -LO https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip
unzip /tmp/terraform.zip
chmod +x terraform && mv terraform /usr/local/bin/


7) Create terraform files called main.tf and variables.tf

# main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}



8) Initialize terraform

terraform init     	# will create a .terraform folder, will have all the downloaded plugins, metadata, etc.


9) Create terraform plan with the files created in previous step

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION

# terraform will keep an in-memory record of the state of the plan that has been applied


10) Create a terraform module for your AKS cluster, with its own variables.tf file

mkdir -p <dir>/.terraform/modules/cluster


# variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


11) Create a cluster.tf file and define a resource group and a cluster definition

# cluster.tf file contents

resource "azurerm_resource_group" "aks-getting-started" {
	name 		 = "ask-getting-started"
	location = var.location
}

resource "azurerm_kubernetes_cluster" "aks-getting-started" {
	name 									= "aks-getting-started"
	location 							= azurerm_resource_group.aks-getting-started.location
	resource_group_name		= azurerm_resource_group.aks-getting-started.name
	dns_prefix						= "aks-getting-started"
	kubernetes_version		= var.kubernetes_version


  default_node_pool {
  	name 			    	= "default"
  	node_count    	= 1
  	vm_size					= "Standard_E4s_v3"
  	type  					= "VirtualMachineScaleSets"
  	os_disk_size_gb = 250
  }

  service_principal {
  	client_id = var.serviceprinciple_id
  	client_secret = var.serviceprinciple_key
  }

  linux_profile {
  	admin_username = "azureuser"
  	ssh_key {
  			key_data = var.ssh_key
  	}
  }

  network_profile {
  		network_plugin = "kubenet"
  		load_balancer_sku = "Standard"
  }

  addon_profile {
  	aci_connector_linux {
  		enabled = false
  	}

  	azure_policy {
  		enabled = false
  	}

  	http_application_routing {
  		enabled = false
  	}

  	kube_dashboard {
  		enabled = false
  	}

  	oms_agent {
  		enabled = false
  	}
  }
}

# see azure provider page and azurerm kubernetes cluster page on the terraform website for more details


12) Generate an SSH key for access to the cluster

ssh-keygen -t rsa -b 4096 -N "VeryStrongSecret123!" -C "your_email@example.com"
export SSH_KEY=$(cat ~/.ssh/id_rsa.pub)


13) Update the variables.tf sitting on the same level as the main.tf file (outside of the cluster folder) with the below

# Updated variables.tf file contents

variable "serviceprinciple_id" {
}

variable "serviceprinciple_key" {
}

variable "tenant_id" {
}

variable "subscription_id" {
}

variable "ssh_key" {
}

variable "location" {
	default = "australiaeast"
}

variable "kubernetes_version" {
	default = "1.16.10"
}


14) Re-apply updated terraform plan

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


15) Update main.tf with the cluster module information

# Updated main.tf file contents

provider "azurerm" {
	version = "=2.5.0"

	subscription_id = var.subscription_id
	client_id				= var.serviceprinciple_id
	client_secret		= var.serviceprinciple_key
	tenant_id 			= var.tenant_id

	features {}
}

module "cluster" {
	source 								= "./modules/cluster/"
	serviceprinciple_id		= var.serviceprinciple_id
	serviceprinciple_key  = var.serviceprinciple_key
	ssh_key								= var.ssh_key
	location							= var.location
	kubernetes_version		= var.kubernetes_version
}


16) Re-initialize terraform (confirm if previous plan needs to somehow be wiped first?)

terraform init


17) Re-apply updated terraform plan (this will create 2 plans, 1 for the cluster, 1 for the resource group)

terraform plan -var serviceprinciple_id=$SERVICE_PRINCIPAL \
	-var serviceprinciple_key="$SERVICE_PRINCIPAL_SECRET" \
	-var tenant_id=$TENANT_ID \
	-var subscription_id=$SUBSCRIPTION \
	-var ssh_key="$SSH_KEY"


18) Add a new resource to demonstrate terraforms capability of updating infrastructure changes via code

# Add to above cluster.tf contents

resource "azurerm_kubernetes_cluster_node_pool" "monitoring" {
	name 										= "monitoring"
	kubernetes_cluster_id 	= azurerm_kubernetes_cluster.aks-getting-started.id
	vm_size									= "Standard_DS2_v2"
	node_count							= 1
	os_disk_size_gb					= 250
	os_type									= "Linux"
}


19) Update cluster.tf values, such as change node_count to 2 in default_node_pool

20) Re-run the same terraform apply command as in step 17 (you will be prompted to say yes or no to apply the changes)

21) Create a new k8s module (create k8s folder under modules first) and a corresponding variables.tf 

# k8s.tf file contents

provider "kubernetes" {
	load_config_file				= "false"
	host 										= var.host
	client_certificate 			= var.client_certificate
	client_key							= var.client.key
	cluster_ca_certificate  = var.cluster_ca_certificate
}


# variables.tf file contents

variable "host" {
}

variable client_certificate {
}

variable client_key {
}

variable cluster_ca_certificate {
}


22) Define a k8s deployment in the k8s.tf file

# Snippet example to put under the provider "kubernetes" section in the file

resource "kubernetes_deployment" "example" {
	metadata {
		name = "terraform-example"
		labels = {
			test = "MyExampleApp"
		}
	}

  ...
  ...
  ...

	}


23) Add a service definition to the k8s.tf file to expose the deployment in step 22

# Snippet example to put under kubernetes_deployment resource section

resource "kubernetes_service" "example" {
	metadata {
		name = "terraform-example"
	}
	spec {
		selector = {
			test = "MyExampleApp"
		}
		port {
			port         = 80
			target_port  = 80
		}

		type - "LoadBalancer"
	}
}


24) Update main.tf with module information required by the new k8s module

# Add to above main.tf file contents

module "k8s" {
	source                 = "./modules/k8s/"
	host                   = "${module.cluster.host}"
	client_certificate     = "${base64decode(module.cluster.client_certificate)}"
	client_key				     = "${base64decode(module.cluster.client_key)}"
	cluster_ca_certificate = "${base64decode(module.cluster.cluster_ca_certificate)}"
}


25) In the cluster module, create a new file called outputs.tf (which terraform uses to grab values from to match with variables we defined. In this case we spit out the contents of kube_config)

# outputs.tf file contents

output "kube_config" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config_raw
}

output "cluster_ca_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.cluster_ca_certificate
}

output "client_certificate" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_certificate
}

output "client_key" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.client_key
}

output "host" {
	value = azurerm_kubernetes_cluster.aks-getting-started.kube_config.0.host
}


26) Go to cluster.tf and remove the monitoring node pool and update default_node_pool value back to 1

27) Re-apply the terraform plan - this will:
	- add 2 plans (the kubernetes deployment and service)
	- update 1 plan (the default_node_pool resource)
	- destroy 1 plan (the monitoring node pool resource)

Note: add a -out param to the terraform apply command to save a plan that you are applying


28) Get cluster credentials to put into kubeconfig

az aks get-credentials -n aks-getting-started -g aks-getting-started


29) Use kubectl to verify what youve set up




-----------
timedatectl
-----------

- Linux/UNIX - used to control system date and time. Can also be used to check whether NTP synchronization enabled (i.e. for automatically setting the time)


#### Commands ####


timedatectl 				# check date and time settings of the system
timedatectl set-ntp no  	# stops the time synchronization with the NTP server (which allows you to set the time manually)



#### Other ####

# localtime/timezone settings

ls /usr/share/zoneinfo/        			# list all time zones
cp America/Chicago /etc/localtime		# change the time zone to Chicago time


# ntp settings - minor updates required*******

- Network Time Protocol (NTP) is used to set and synchronize the clocks of one or more systems that are on a network
	- Synchronizes the PC clock with local or NTP servers
	- Adjusts the rate of the kernel’s clock-tick to track time accurately
	- Synchronizes time with the other NTP clients


yum install ntp                         # install ntp package (may require yum or dnf update first)
cat /etc/ntp.conf                       # ntp daemon's configuration file
ntpq -n pool.ntp.org                    # The ntpq command prompt is displayed and prints list of known peers
	peers 								# See peers list





----------
traceroute
----------

- definition: a command-line utility used to show the exact route that is taken by data packets as they travel across the internet to their desiination
	- helps to find bottlenecks in routes

- vs. ping - tells you general connnectivity to destination and amount of time. traceroute shows more info.

- eg. of command usage

	tracert google.com      # Windows cmd (also available in Linux) to show trace to google.com

		- sends 3 data packets to each router on its way to the destination
			- helps isolate false issues [instead of sending just 1 packet]
		- router sends back the 3 data packets with info about that router:
				- number of hops to router
				- time/duration of round trip
						- increases with each hop since each hop is further and further, towards destination
						- network slowness: if theres a big difference between hops, could indicate a problem or the distance between two particular hops/routers is very large
						- request timed out: indicates problem or
							- if the trace still goes all the way through and request timed out happens at one hop/router, it could be because that router wasnt configured to return info back to the source
				- IP of router

- time to live [TTL] - the max number of hops a trace will do [can be modified]
										 - prevents data packets from traveling endlessly around the internet
										 	- e.g. misconfigured routers tied together in a loop that receive a trace with no TTL might endlessly pass data packets and slow down a network

	tracert -h 4 google.com    # set max hops to 4 - if destination is further than 4 hops, destination will not be reached

- Linux/MacOS CLI equivalent is "traceroute"



---------------
troubleshooting
---------------

***Kubernetes deployments - general approach***

1. Deployments
2. Pods
For both check:
	a) status
	b) events

- Commands
	kubectl get deploy
	kubectl logs <pod_name>
	kubectl describe deploy <deployment_name>
- failure examples:
	- pod not able to pull image (e.g. its missing)
	- liveliness probe is failing (e.g. the URL doesnt exist)
- status examples and possible causes:			
	- status: ContainerCreating
		- could be stuck because config map missing
	- status: CrashLoopBack
		- pod attempts to continually restart but cannot, usually an app or image issue.
	- status: Pending
		- insufficient memory
		- image may not run (e.g. try docker run on it outside a pod)
	- pods running, but not reachable
		- check selector label in service def matches app selector label
			- kubectl get svc
	- connection refused
		- endpoint may have wrong targetPort (check yaml for port and targetPort)



***Generating Thread Dumps for troubleshooting performance issues***

1.	Identify the process ID (pid) of the tomcat process running JIRA by executing the following command: 
2.	ps aux | grep jira
3.	Execute the following command (be sure to replace both occurrences of $JIRA_PID with the pid you identified in the previous step): 
4.	for i in $(seq 6); do top -b -H -p $JIRA_PID -n 1 > jira_cpu_usage.`date +%s`.txt; kill -3 $JIRA_PID; sleep 10; done
5.	That script will run for one minute, during which it will generate top output of the process threads to six text files in the current directory.
6.	Attach the files generated by the script to this issue.
7.	Generate and send a complete support zip.



***Performance issues in a web app***

- in Chrome or whichever browser, open the console and check the performance tab, see if memory usage and see if it keeps climbing
- go to memory tab and take a snapshot of the memory values of each page or call



----
type
----

- (Linux/UNIX) - allows you to locate special commands such as shell built-ins


#### commands ####

type type             # show type for "type" command. Output: type is a shell builtin
type yum              # yum is /usr/bin/yum





------
ulimit
------

# Sources
# https://www.thegeekdiary.com/understanding-etc-security-limits-conf-file-to-set-ulimit/
# https://ss64.com/bash/ulimit.html


- UNIX/Linux
	- set or report the resource limit of the current user.
	- user limits - limit the use of system-wide resources.


- Syntax
      ulimit [-HS] -a
      ulimit [-HS] [-bcdefiklmnpqrstuvxPRT] [limit]


Key 	limits item 	description
-S   					Set a soft limit for the given resource.
-H   					Set a hard limit for the given resource.

-a   					All current limits are reported.
-b   	?				Maximum socket buffer size.
-c   	core 			Maximum size of core files created. 
-d   	data 			Maximum size of a process'' data segment.                  
-e   	priority		Maximum scheduling priority ("nice") 
-f   	fsize 			Maximum size of files created by the shell(default option).
-i   	sigpending 		Maximum number of pending signals.
-k   	?  				Maximum number of kqueues that may be allocated.
-l   	memlock 		Maximum size that can be locked into memory. 
-m      rss 			Maximum resident set size (ignored in Linux 2.4.30 and higher)
-n   	nofile 			Maximum number of open file descriptors. 
-p      ? 				Pipe buffer size.
-P 		maxlogins		Maximum number of pseudoterminals.
-q      msgqueue 		Maximum number of bytes in POSIX message queues.
-r   	rtprio 			Maximum real-time scheduling priority.
-R      ? 				Maximum time a real-time process can run before blocking, in microseconds.
-s      stack 			Maximum stack size. 
-t      cpu 			Maximum amount of cpu time in seconds.
-T      ? 				Maximum number of threads.
-u   	noproc 			Maximum number of processes available to a single user.
-v   	? 				Maximum amount of virtual memory available to the process.
-x   	locks			Maximum number of file locks.


# Permament ulimit settings - where default settings for ulimits are kept on the OS

- in "/etc/security/limits.conf"

#[domain]        [type]  [item]  [value]

*               -       core             [value]
*               -       data             [value]
*               -       priority         [value]
*               -       fsize            [value]
*               soft    sigpending       [value] eg:57344
*               hard    sigpending       [value] eg:57444
*               -       memlock          [value]
*               -       nofile           [value] eg:1024
*               -       msgqueue         [value] eg:819200
*               -       locks            [value]
*               soft    core             [value]
*               hard    nofile           [value]
@[group]        hard    nproc            [value]
[user]          soft    nproc            [value]
%[group]        hard    nproc            [value]
[user]          hard    nproc            [value]
@[group]        -       maxlogins        [value]
[user]          hard    cpu              [value]
[user]          soft    cpu              [value]
[user]          hard    locks            [value]
some_user       soft    nproc            16384          # eg of a max no. of processes setting for some user


- [domain] can be:
	- an user name
	- a group name, with @group syntax
	- the wildcard *, for default entry
	- the wildcard %, can be also used with %group syntax, for maxlogin limit

- [type] can have the two values:
	  “soft” for enforcing the soft limits
	  “hard” for enforcing hard limits

- [item] can be one of the following:
	  core – limits the core file size (KB)
	  data – max data size (KB)
	  fsize – maximum filesize (KB)
	  memlock – max locked-in-memory address space (KB)
	  nofile – max number of open files
	  rss – max resident set size (KB)
	  stack – max stack size (KB)
	  cpu – max CPU time (MIN)
	  nproc – max number of processes
	  as – address space limit (KB)
	  maxlogins – max number of logins for this user
	  maxsyslogins – max number of logins on the system
	  priority – the priority to run user process with
	  locks – max number of file locks the user can hold
	  sigpending – max number of pending signals
	  msgqueue – max memory used by POSIX message queues (bytes)
	  nice – max nice priority allowed to raise to values: [-20, 19]
	  rtprio – max realtime priority



# Temporary settings via command line - examples

ulimit -Sn [number] 		# set soft limit for number of processes for the logged in user
ulimit -Hn [number]         # set hard limit for number of processes for the logged in user
ulimit -Sn 					# verify soft limit for number of processes for the logged in user
ulimit -Hn 					# verify hard limit for number of processes for the logged in user



-----
uname
-----

#### General ####

- in Linux, this command shows information about the kernel: name, version, distribution, release date, etc.


#### Commands ####

uname         # show kernel name (-s option also does this, hence it is the default option)
uname -a      # shows all kernel info based on every option that uname has available to it
uname -i      # prints the hardware platform
uname -n      # prints system hostname
uname -o      # prints OS name 
uname -p      # prints processor info
uname -r      # prints kernel release
uname -v      # prints kernel release date

# Alternatives

cat /proc/version      # also prints kernel information
dmesg | grep Linux     # also prints kernel information



-----------
update-rc.d
-----------

- Linux Ubuntu - activate, deactivate or modify a service startup.
			   - replacement for "chkconfig" utility 


#### Commands ####

sudo update-rc.d -f apache2 remove          # Remove apache2 service
sudo update-rc.d apache2 defaults           # Add apache2 service to list of programs to startup on boot
sudo update-rc.d apache2 defaults 90 90     # Define the start and kill priority (respectively) of apache2 service




---------------
user management
---------------

####### Linux ########

# Users
useradd [user_id]         			# add user to OS
passwd [user_id]          			# set password for new user
usermod  -e 12/12/2020 [user_id] 	# set expiration date for the new user's password
usermod -L [user_id]                # lock user account
chage -d 0 [user_id]  				# force user's password into expired state
userdel [user_id]					# delete user from OS

# Groups
groupadd -g 10000 plabuser  		# add group with name "plabuser" with group ID 10000
usermod -aG plabuser [user_id]      # add user to plabuser group
id [user_id]  						# show user and its primary group
groupmod -n plab plabuser           # change the name of a group (in this case from plabuser to plab)
groupdel plab 						# delete group plab


# Password info
cat /etc/passwd 					# Show all users, their groups and their passwords (encrypted)
cat /etc/shadow  					# Show all users plaintext password (viewable ONLY by root user)
cat /etc/group 						# Show passwords for groups
ls -latr /etc/skel                  # See default profiles that all new users will inherit upon creation

# Lockdown
touch /etc/nologin 					# Run this and only root will be able to login to the system. Should be a 0 byte file (no content)



# last command

last               		# displays history of logins of all users on the system
last [user_id]     		# displays history of logins of target user on the system
last -F [user_id]  		# dispalys complete timing of login timing of target user


# w, who, whoami

who        		# displays the name of currently logged in user, the user's terminal, the login time of the user and host or IP from where the user is logged in
who -u     		# show list of all logged-in users
who -q -H  		# show the number of logged-in users
who -r    		# show runlevel of the system
who -a          # show complete detail of the logged-in user


w               # show complete detail of the logged-in user (alternative to who)
w -s            # supress output for login time, JCPU and PCPU columns
w [user_id] 	# show complete detail of the target user





-------
vagrant
-------

- open-source software product for building and maintaining portable virtual software development environments
	- grouped under "Virtual Machine Management"


# Vagrantfile example for provisioning three virtual machines for a K8s cluster

# START VAGRANTFILE
# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV['VAGRANT_NO_PARALLEL'] = 'yes'

Vagrant.configure(2) do |config|

  config.vm.provision "shell", path: "bootstrap.sh"

  # Kubernetes Master Server
  config.vm.define "kmaster" do |node|

    node.vm.box               = "generic/ubuntu2004"
    node.vm.box_check_update  = false
    node.vm.box_version       = "3.3.0"
    node.vm.hostname          = "kmaster.example.com"

    node.vm.network "private_network", ip: "172.16.16.100"

    node.vm.provider :virtualbox do |v|
      v.name    = "kmaster"
      v.memory  = 2048
      v.cpus    =  2
    end

    node.vm.provider :libvirt do |v|
      v.memory  = 2048
      v.nested  = true
      v.cpus    = 2
    end

    node.vm.provision "shell", path: "bootstrap_kmaster.sh"

  end


  # Kubernetes Worker Nodes
  NodeCount = 2

  (1..NodeCount).each do |i|

    config.vm.define "kworker#{i}" do |node|

      node.vm.box               = "generic/ubuntu2004"
      node.vm.box_check_update  = false
      node.vm.box_version       = "3.3.0"
      node.vm.hostname          = "kworker#{i}.example.com"

      node.vm.network "private_network", ip: "172.16.16.10#{i}"

      node.vm.provider :virtualbox do |v|
        v.name    = "kworker#{i}"
        v.memory  = 1024
        v.cpus    = 1
      end

      node.vm.provision "shell", path: "bootstrap_kworker.sh"

    end

  end

end

# END VAGRANTFILE


--
vi
--

- [Linux] a command line editor for files

To start VI:
$ vi <filename>
Vi has 2 modes: input mode (where you just get to type into your file), and command mode. Keep reading.


### Quick guide ###

:w    # To save work

:q    # To quit

:wq   # Save and quit
ZZ    # also save and quit
:x    # also save and quit (not available on some OS')

:e! somefile.txt     # close current file without saving and open new file called "somefile.txt"

# Moving around:
h     # left
j     # down
k     # up
l     # right
0     # beginning of line
$     # end of line (shift??)
^     # start of line (shift??)
H 	  # top of screen
L  	  # bottom of screen
G 	  # bottom of file
<n>G  # the n'th line of the file
3j    # Move down 3 lines
4k    # Move up 4 lines

1 then Shift+H    # Move to beginning of file
Shift+G           # Bottom of file


# Search (note: can use regex)
/tomorrow     # Search forwards for the word tomorrow
?tomorrow     # Search backwards for the word tomorrow

# Search and replace
:%s/left/LEFT/g    # replace all occurences of left with LEFT

w forward a word, counting punctuation
b backward a word, counting punctuation
W forward a word, not counting punctuation
B backward a word, not counting punctuation

^F <ctrl>F forward a screen
^B <ctrl>B backward a screen


:noh    # shuts off highlighting


- When invoked, VI is in command mode. You can return to command mode at any time by pressing the [ESC] key. The [ESC] key is also used to cancel a
  partially-entered command.

# To enter insert mode (actually enter text):
i inserts at (before) current cursor position
I inserts at beginning of current line
a appends following current cursor position
A appends to end of current line
o opens up a new line after the current line
O opens up a new line previous to the current line
^V <ctrl>V  To quote a character (insert a non-printing character)

To delete text:
x deletes a single character (at the cursor)
d<unit> deletes curent textual unit, taken from above. So,
dw deletes word, up to punctuation
dW deletes word, ignoring punctuation

D$           # deletes from cursor to end of line
dd or D      # delete a line
S            # delete a line and go into insert mode in the deleted line
dL deletes to bottom of screen
dG deletes from current cursor position to end of file
dd deletes current line (entirely)

# Copy and paste
y[unit]     # copy
yy          # copy (or yank) current line to buffer
yw          # copy (or yank) word
Y copy to end of line (or y$)

p paste, following the cursor.
P paste, preceding the cursor.

To change text:
c<unit> marks unit for overwrite, puts you in insert mode. So,
cw change current word
C change to end of line
cc change current line (entirely)
r replace current (single) character
~ changes case of current character
Other useful stuff:
J joins current line to line below
/<string> searches forward for <string>
n reexecutes last search
N same thing, but other direction
?<string> searches backward for <string>
^L <ctrl>L refreshes the screen


--------
webhooks
--------

Source: https://www.youtube.com/watch?v=rUaDIH5ZXB8&list=WL&index=2

Definition: http messages that are sent in response to an event to a third party service (e.g. you can configure a webhook in Github to send messages to Jenkins based on push events to a given repository)




-------
whereis
-------

- (Linux/UNIX) - allow you to locate more information than just the location of the command


#### commands ####

whereis ls            # shows location and more details of the "ls" command




-----
which
-----

- (Linux/UNIX) - shows the full path of shell commands
	- exception: shell builtins (see "type" section)
		- e.g. type - if you run "which type", it will not return the path since it is a shell builtin program


#### Commands ####

which yum 				 # show default path of yum
which -a yum             # show path(s) of yum  (e.g. you might see /usr/bin/yum and /bin/yum, two different locations)





-----
whois
-----

whois [domain_name]   # will return comprehensive information about that URL/domain (given that domain is publicly registered)



-------
Windows
-------

# Windows Spotlight images retrieval

- location: C:\Users\Buildmaster\AppData\Local\Packages\Microsoft.Windows.ContentDeliveryManager_cw5n1h2txyewy\LocalState\Assets>
	- from command prompt, cd to the above and then:

		cp * D:/new_folder     						# or some other drive/location
		for i in $( ls ); do mv $i $i.jpg; done     # UNIX sh: rename all the files to jpg

		OR

		ren * *.jpg  								# rename all files to jpg files with Powershell





----
yaml
----

- Yet Another Markup Language
	- is a data serialization language that is often used for writing configuration files.
	- has features that come from Perl, C, XML, HTML, and other programming languages. YAML is also a superset of JSON, so JSON files are valid in YAML.
	- uses Python-style indentation to indicate nesting
	- used for defining templates for different kubernetes resources (e.g. deployments, services, custom resource definitions)


# YAML mapping field definitions - taken from Marcel Dempers [That DevOps Guy]
# https://github.com/marcel-dempers/docker-development-youtube-series/blob/master/monitoring/prometheus/kubernetes/1.18.4/prometheus-operator/crd-thanosruler.yaml

# Below are definitions of different fields and what they mean when you look at a Kubernetes template.


- apiVersion - defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values.
						 - value eg. "apiextensions.k8s.io/v1"
							- More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

- kind - a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to.
			 - Cannot be updated
			 - value is written In CamelCase, egs. "ReplicationController, CustomResourceDefinition, Pod"
			 	- More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

- spec - Specification of the desired behavior of the object, contains a complete description of the desired state, including:
			 		- configuration settings provided by the user
			 		- default values expanded by the system
			 		- properties initialized or otherwise changed after creation by other ecosystem components (e.g., schedulers, auto-scalers), and is persisted in stable storage with the API object.
          - More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status


 ##### MORE DEFINITIONS TO ADD + NOTES ON NESTING #####




---
yum
---

- package manager for RHEL7 and below Linux OS'' (or CentOS 7 and below). Can still be used on CentOS8, RHEL8, Rocky Linux

# Install a package on RHEL-based Linux (e.g. httpd)

yum install httpd        # install via yum package manager (can use dnf on CentOS 8/RHEL 8 and up, Rocky Linux). Will also install its dependencies
yum list httpd           # check httpd is installed, shows a little detail about the package
yum update httpd         # check for and install updates to the http package
yum remove httpd         # remove httpd package and all its dependenies
yum info httpd           # show more details about httpd package
yum deplist httpd        # show dependencies list for http package
yum downloader squid     # download squid package RPM

reqoquery --list httpd   # list all files in httpd package (NOTE: need yum-utils package to be able to run) 


# yum configuration

cat /etc/yum.conf             # view current yum configurations
ls -latr /etc/yum.repos.d     # this is where yum repo information is kept, also referenced by yum.conf

